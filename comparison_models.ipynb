{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6193dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ad7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda | GPUs available: 4\n",
      "\n",
      "================================================================================\n",
      "RUNNING CRITEO  |  LR  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.6735 val_ll=0.5181 val_auc=0.7411 *BEST*\n",
      "Epoch 02 | train_ll=0.4976 val_ll=0.4871 val_auc=0.7638 *BEST*\n",
      "Epoch 03 | train_ll=0.4784 val_ll=0.4776 val_auc=0.7719 *BEST*\n",
      "Epoch 04 | train_ll=0.4707 val_ll=0.4734 val_auc=0.7759 *BEST*\n",
      "Epoch 05 | train_ll=0.4667 val_ll=0.4712 val_auc=0.7782 *BEST*\n",
      "Epoch 06 | train_ll=0.4643 val_ll=0.4700 val_auc=0.7795 *BEST*\n",
      "Epoch 07 | train_ll=0.4627 val_ll=0.4694 val_auc=0.7801 *BEST*\n",
      "Epoch 08 | train_ll=0.4617 val_ll=0.4691 val_auc=0.7807 *BEST*\n",
      "Epoch 09 | train_ll=0.4609 val_ll=0.4689 val_auc=0.7811 *BEST*\n",
      "Epoch 10 | train_ll=0.4604 val_ll=0.4689 val_auc=0.7812 *BEST*\n",
      "Epoch 11 | train_ll=0.4600 val_ll=0.4688 val_auc=0.7814 *BEST*\n",
      "Epoch 12 | train_ll=0.4596 val_ll=0.4689 val_auc=0.7815 *BEST*\n",
      "Epoch 13 | train_ll=0.4594 val_ll=0.4690 val_auc=0.7813 stale 1/5\n",
      "Epoch 14 | train_ll=0.4592 val_ll=0.4691 val_auc=0.7815 *BEST*\n",
      "Epoch 15 | train_ll=0.4590 val_ll=0.4692 val_auc=0.7814 stale 1/5\n",
      "Epoch 16 | train_ll=0.4589 val_ll=0.4693 val_auc=0.7813 stale 2/5\n",
      "Epoch 17 | train_ll=0.4587 val_ll=0.4694 val_auc=0.7814 stale 3/5\n",
      "Epoch 18 | train_ll=0.4586 val_ll=0.4695 val_auc=0.7815 stale 4/5\n",
      "Epoch 19 | train_ll=0.4586 val_ll=0.4696 val_auc=0.7814 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "CRITEO LR RESULTS  |  BASE=criteo_preprocessed\n",
      "================================================================================\n",
      "Test AUC:     0.7813\n",
      "Test LogLoss: 0.4693\n",
      "Saved to: runs_criteo_LR_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING CRITEO  |  FM  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "[warn] Could not init output bias; skipping.\n",
      "Epoch 01 | train_ll=0.5458 val_ll=0.4795 val_auc=0.7760 *BEST*\n",
      "Epoch 02 | train_ll=0.4651 val_ll=0.4740 val_auc=0.7811 *BEST*\n",
      "Epoch 03 | train_ll=0.4535 val_ll=0.4742 val_auc=0.7818 *BEST*\n",
      "Epoch 04 | train_ll=0.4457 val_ll=0.4759 val_auc=0.7818 stale 1/5\n",
      "Epoch 05 | train_ll=0.4399 val_ll=0.4780 val_auc=0.7806 stale 2/5\n",
      "Epoch 06 | train_ll=0.4353 val_ll=0.4808 val_auc=0.7797 stale 3/5\n",
      "Epoch 07 | train_ll=0.4314 val_ll=0.4834 val_auc=0.7788 stale 4/5\n",
      "Epoch 08 | train_ll=0.4282 val_ll=0.4858 val_auc=0.7776 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "CRITEO FM RESULTS  |  BASE=criteo_preprocessed\n",
      "================================================================================\n",
      "Test AUC:     0.7816\n",
      "Test LogLoss: 0.4744\n",
      "Saved to: runs_criteo_FM_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING CRITEO  |  DeepFM  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.4938 val_ll=0.4641 val_auc=0.7907 *BEST*\n",
      "Epoch 02 | train_ll=0.4527 val_ll=0.4618 val_auc=0.7930 *BEST*\n",
      "Epoch 03 | train_ll=0.4395 val_ll=0.4643 val_auc=0.7918 stale 1/5\n",
      "Epoch 04 | train_ll=0.4280 val_ll=0.4685 val_auc=0.7907 stale 2/5\n",
      "Epoch 05 | train_ll=0.4177 val_ll=0.4747 val_auc=0.7879 stale 3/5\n",
      "Epoch 06 | train_ll=0.4085 val_ll=0.4825 val_auc=0.7856 stale 4/5\n",
      "Epoch 07 | train_ll=0.4006 val_ll=0.4917 val_auc=0.7828 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "CRITEO DeepFM RESULTS  |  BASE=criteo_preprocessed\n",
      "================================================================================\n",
      "Test AUC:     0.7928\n",
      "Test LogLoss: 0.4618\n",
      "Saved to: runs_criteo_DeepFM_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING CRITEO  |  DCN  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.4970 val_ll=0.4651 val_auc=0.7871 *BEST*\n",
      "Epoch 02 | train_ll=0.4573 val_ll=0.4582 val_auc=0.7942 *BEST*\n",
      "Epoch 03 | train_ll=0.4472 val_ll=0.4560 val_auc=0.7976 *BEST*\n",
      "Epoch 04 | train_ll=0.4383 val_ll=0.4567 val_auc=0.7975 stale 1/5\n",
      "Epoch 05 | train_ll=0.4302 val_ll=0.4599 val_auc=0.7965 stale 2/5\n",
      "Epoch 06 | train_ll=0.4229 val_ll=0.4650 val_auc=0.7945 stale 3/5\n",
      "Epoch 07 | train_ll=0.4161 val_ll=0.4693 val_auc=0.7939 stale 4/5\n",
      "Epoch 08 | train_ll=0.4101 val_ll=0.4758 val_auc=0.7910 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "CRITEO DCN RESULTS  |  BASE=criteo_preprocessed\n",
      "================================================================================\n",
      "Test AUC:     0.7973\n",
      "Test LogLoss: 0.4563\n",
      "Saved to: runs_criteo_DCN_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING CRITEO  |  FinalMLP  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.4625 val_ll=0.4525 val_auc=0.7984 *BEST*\n",
      "Epoch 02 | train_ll=0.4491 val_ll=0.4480 val_auc=0.8031 *BEST*\n",
      "Epoch 03 | train_ll=0.4443 val_ll=0.4462 val_auc=0.8051 *BEST*\n",
      "Epoch 04 | train_ll=0.4411 val_ll=0.4455 val_auc=0.8059 *BEST*\n",
      "Epoch 05 | train_ll=0.4385 val_ll=0.4450 val_auc=0.8065 *BEST*\n",
      "Epoch 06 | train_ll=0.4363 val_ll=0.4455 val_auc=0.8065 stale 1/5\n",
      "Epoch 07 | train_ll=0.4343 val_ll=0.4451 val_auc=0.8065 stale 2/5\n",
      "Epoch 08 | train_ll=0.4323 val_ll=0.4452 val_auc=0.8065 stale 3/5\n",
      "Epoch 09 | train_ll=0.4304 val_ll=0.4458 val_auc=0.8060 stale 4/5\n",
      "Epoch 10 | train_ll=0.4285 val_ll=0.4460 val_auc=0.8058 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "CRITEO FinalMLP RESULTS  |  BASE=criteo_preprocessed\n",
      "================================================================================\n",
      "Test AUC:     0.8063\n",
      "Test LogLoss: 0.4452\n",
      "Saved to: runs_criteo_FinalMLP_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING CRITEO  |  FT-only  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_ll=0.4582 val_ll=0.4488 val_auc=0.8028 *BEST*\n",
      "Epoch 02 | train_ll=0.4462 val_ll=0.4465 val_auc=0.8060 *BEST*\n",
      "Epoch 03 | train_ll=0.4415 val_ll=0.4454 val_auc=0.8076 *BEST*\n",
      "Epoch 04 | train_ll=0.4376 val_ll=0.4438 val_auc=0.8080 *BEST*\n",
      "Epoch 05 | train_ll=0.4337 val_ll=0.4438 val_auc=0.8081 *BEST*\n",
      "Epoch 06 | train_ll=0.4293 val_ll=0.4486 val_auc=0.8067 stale 1/5\n",
      "Epoch 07 | train_ll=0.4242 val_ll=0.4511 val_auc=0.8048 stale 2/5\n",
      "Epoch 08 | train_ll=0.4182 val_ll=0.4589 val_auc=0.8013 stale 3/5\n",
      "Epoch 09 | train_ll=0.4114 val_ll=0.4610 val_auc=0.7973 stale 4/5\n",
      "Epoch 10 | train_ll=0.4040 val_ll=0.4690 val_auc=0.7950 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "CRITEO FT-only RESULTS  |  BASE=criteo_preprocessed\n",
      "================================================================================\n",
      "Test AUC:     0.8076\n",
      "Test LogLoss: 0.4442\n",
      "Saved to: runs_criteo_FT-only_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING OUTBRAIN  |  LR  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.5392 val_ll=0.4653 val_auc=0.6835 *BEST*\n",
      "Epoch 02 | train_ll=0.4547 val_ll=0.4535 val_auc=0.6976 *BEST*\n",
      "Epoch 03 | train_ll=0.4477 val_ll=0.4499 val_auc=0.7022 *BEST*\n",
      "Epoch 04 | train_ll=0.4448 val_ll=0.4483 val_auc=0.7048 *BEST*\n",
      "Epoch 05 | train_ll=0.4432 val_ll=0.4475 val_auc=0.7061 *BEST*\n",
      "Epoch 06 | train_ll=0.4421 val_ll=0.4471 val_auc=0.7069 *BEST*\n",
      "Epoch 07 | train_ll=0.4412 val_ll=0.4467 val_auc=0.7076 *BEST*\n",
      "Epoch 08 | train_ll=0.4406 val_ll=0.4466 val_auc=0.7078 *BEST*\n",
      "Epoch 09 | train_ll=0.4401 val_ll=0.4465 val_auc=0.7081 *BEST*\n",
      "Epoch 10 | train_ll=0.4396 val_ll=0.4465 val_auc=0.7082 *BEST*\n",
      "Epoch 11 | train_ll=0.4392 val_ll=0.4464 val_auc=0.7085 *BEST*\n",
      "Epoch 12 | train_ll=0.4389 val_ll=0.4464 val_auc=0.7086 *BEST*\n",
      "Epoch 13 | train_ll=0.4386 val_ll=0.4464 val_auc=0.7088 *BEST*\n",
      "Epoch 14 | train_ll=0.4384 val_ll=0.4466 val_auc=0.7088 stale 1/5\n",
      "Epoch 15 | train_ll=0.4381 val_ll=0.4466 val_auc=0.7087 stale 2/5\n",
      "Epoch 16 | train_ll=0.4379 val_ll=0.4467 val_auc=0.7088 *BEST*\n",
      "Epoch 17 | train_ll=0.4377 val_ll=0.4468 val_auc=0.7088 *BEST*\n",
      "Epoch 18 | train_ll=0.4376 val_ll=0.4467 val_auc=0.7089 *BEST*\n",
      "Epoch 19 | train_ll=0.4374 val_ll=0.4468 val_auc=0.7087 stale 1/5\n",
      "Epoch 20 | train_ll=0.4373 val_ll=0.4469 val_auc=0.7089 stale 2/5\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "OUTBRAIN LR RESULTS  |  BASE=outbrain_preprocessed_40m\n",
      "================================================================================\n",
      "Test AUC:     0.6985\n",
      "Test LogLoss: 0.4547\n",
      "Saved to: runs_outbrain_LR_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING OUTBRAIN  |  FM  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "[warn] Could not init output bias; skipping.\n",
      "Epoch 01 | train_ll=0.5179 val_ll=0.4546 val_auc=0.6992 *BEST*\n",
      "Epoch 02 | train_ll=0.4428 val_ll=0.4524 val_auc=0.7032 *BEST*\n",
      "Epoch 03 | train_ll=0.4382 val_ll=0.4516 val_auc=0.7040 *BEST*\n",
      "Epoch 04 | train_ll=0.4351 val_ll=0.4519 val_auc=0.7044 *BEST*\n",
      "Epoch 05 | train_ll=0.4328 val_ll=0.4527 val_auc=0.7042 stale 1/5\n",
      "Epoch 06 | train_ll=0.4308 val_ll=0.4528 val_auc=0.7035 stale 2/5\n",
      "Epoch 07 | train_ll=0.4291 val_ll=0.4539 val_auc=0.7032 stale 3/5\n",
      "Epoch 08 | train_ll=0.4276 val_ll=0.4543 val_auc=0.7032 stale 4/5\n",
      "Epoch 09 | train_ll=0.4263 val_ll=0.4553 val_auc=0.7016 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "OUTBRAIN FM RESULTS  |  BASE=outbrain_preprocessed_40m\n",
      "================================================================================\n",
      "Test AUC:     0.6894\n",
      "Test LogLoss: 0.4649\n",
      "Saved to: runs_outbrain_FM_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING OUTBRAIN  |  DeepFM  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.4575 val_ll=0.4505 val_auc=0.7032 *BEST*\n",
      "Epoch 02 | train_ll=0.4411 val_ll=0.4502 val_auc=0.7049 *BEST*\n",
      "Epoch 03 | train_ll=0.4351 val_ll=0.4499 val_auc=0.7051 *BEST*\n",
      "Epoch 04 | train_ll=0.4303 val_ll=0.4509 val_auc=0.7050 stale 1/5\n",
      "Epoch 05 | train_ll=0.4264 val_ll=0.4519 val_auc=0.7040 stale 2/5\n",
      "Epoch 06 | train_ll=0.4230 val_ll=0.4537 val_auc=0.7038 stale 3/5\n",
      "Epoch 07 | train_ll=0.4200 val_ll=0.4547 val_auc=0.7021 stale 4/5\n",
      "Epoch 08 | train_ll=0.4173 val_ll=0.4564 val_auc=0.7011 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "OUTBRAIN DeepFM RESULTS  |  BASE=outbrain_preprocessed_40m\n",
      "================================================================================\n",
      "Test AUC:     0.6901\n",
      "Test LogLoss: 0.4625\n",
      "Saved to: runs_outbrain_DeepFM_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING OUTBRAIN  |  DCN  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.4582 val_ll=0.4504 val_auc=0.7029 *BEST*\n",
      "Epoch 02 | train_ll=0.4421 val_ll=0.4482 val_auc=0.7055 *BEST*\n",
      "Epoch 03 | train_ll=0.4374 val_ll=0.4473 val_auc=0.7071 *BEST*\n",
      "Epoch 04 | train_ll=0.4337 val_ll=0.4475 val_auc=0.7086 *BEST*\n",
      "Epoch 05 | train_ll=0.4305 val_ll=0.4483 val_auc=0.7090 *BEST*\n",
      "Epoch 06 | train_ll=0.4275 val_ll=0.4490 val_auc=0.7067 stale 1/5\n",
      "Epoch 07 | train_ll=0.4248 val_ll=0.4500 val_auc=0.7078 stale 2/5\n",
      "Epoch 08 | train_ll=0.4222 val_ll=0.4509 val_auc=0.7062 stale 3/5\n",
      "Epoch 09 | train_ll=0.4199 val_ll=0.4524 val_auc=0.7074 stale 4/5\n",
      "Epoch 10 | train_ll=0.4178 val_ll=0.4529 val_auc=0.7066 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "OUTBRAIN DCN RESULTS  |  BASE=outbrain_preprocessed_40m\n",
      "================================================================================\n",
      "Test AUC:     0.6971\n",
      "Test LogLoss: 0.4593\n",
      "Saved to: runs_outbrain_DCN_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING OUTBRAIN  |  FinalMLP  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.4415 val_ll=0.4439 val_auc=0.7113 *BEST*\n",
      "Epoch 02 | train_ll=0.4396 val_ll=0.4437 val_auc=0.7124 *BEST*\n",
      "Epoch 03 | train_ll=0.4386 val_ll=0.4432 val_auc=0.7139 *BEST*\n",
      "Epoch 04 | train_ll=0.4378 val_ll=0.4426 val_auc=0.7145 *BEST*\n",
      "Epoch 05 | train_ll=0.4370 val_ll=0.4426 val_auc=0.7146 *BEST*\n",
      "Epoch 06 | train_ll=0.4361 val_ll=0.4430 val_auc=0.7140 stale 1/5\n",
      "Epoch 07 | train_ll=0.4352 val_ll=0.4431 val_auc=0.7141 stale 2/5\n",
      "Epoch 08 | train_ll=0.4343 val_ll=0.4430 val_auc=0.7144 stale 3/5\n",
      "Epoch 09 | train_ll=0.4333 val_ll=0.4433 val_auc=0.7147 *BEST*\n",
      "Epoch 10 | train_ll=0.4322 val_ll=0.4439 val_auc=0.7138 stale 1/5\n",
      "Epoch 11 | train_ll=0.4310 val_ll=0.4437 val_auc=0.7136 stale 2/5\n",
      "Epoch 12 | train_ll=0.4299 val_ll=0.4446 val_auc=0.7133 stale 3/5\n",
      "Epoch 13 | train_ll=0.4288 val_ll=0.4443 val_auc=0.7132 stale 4/5\n",
      "Epoch 14 | train_ll=0.4278 val_ll=0.4444 val_auc=0.7124 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "OUTBRAIN FinalMLP RESULTS  |  BASE=outbrain_preprocessed_40m\n",
      "================================================================================\n",
      "Test AUC:     0.7013\n",
      "Test LogLoss: 0.4539\n",
      "Saved to: runs_outbrain_FinalMLP_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING OUTBRAIN  |  FT-only  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_ll=0.4421 val_ll=0.4444 val_auc=0.7112 *BEST*\n",
      "Epoch 02 | train_ll=0.4400 val_ll=0.4431 val_auc=0.7140 *BEST*\n",
      "Epoch 03 | train_ll=0.4387 val_ll=0.4421 val_auc=0.7158 *BEST*\n",
      "Epoch 04 | train_ll=0.4376 val_ll=0.4428 val_auc=0.7151 stale 1/5\n",
      "Epoch 05 | train_ll=0.4361 val_ll=0.4426 val_auc=0.7150 stale 2/5\n",
      "Epoch 06 | train_ll=0.4345 val_ll=0.4429 val_auc=0.7150 stale 3/5\n",
      "Epoch 07 | train_ll=0.4327 val_ll=0.4437 val_auc=0.7136 stale 4/5\n",
      "Epoch 08 | train_ll=0.4307 val_ll=0.4447 val_auc=0.7113 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "OUTBRAIN FT-only RESULTS  |  BASE=outbrain_preprocessed_40m\n",
      "================================================================================\n",
      "Test AUC:     0.7037\n",
      "Test LogLoss: 0.4499\n",
      "Saved to: runs_outbrain_FT-only_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING AVAZU  |  LR  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.5831 val_ll=0.4479 val_auc=0.6924 *BEST*\n",
      "Epoch 02 | train_ll=0.4171 val_ll=0.4177 val_auc=0.7123 *BEST*\n",
      "Epoch 03 | train_ll=0.4054 val_ll=0.4080 val_auc=0.7219 *BEST*\n",
      "Epoch 04 | train_ll=0.3990 val_ll=0.4038 val_auc=0.7265 *BEST*\n",
      "Epoch 05 | train_ll=0.3946 val_ll=0.4031 val_auc=0.7286 *BEST*\n",
      "Epoch 06 | train_ll=0.3913 val_ll=0.4019 val_auc=0.7306 *BEST*\n",
      "Epoch 07 | train_ll=0.3889 val_ll=0.4024 val_auc=0.7304 stale 1/5\n",
      "Epoch 08 | train_ll=0.3869 val_ll=0.4027 val_auc=0.7310 *BEST*\n",
      "Epoch 09 | train_ll=0.3853 val_ll=0.4025 val_auc=0.7309 stale 1/5\n",
      "Epoch 10 | train_ll=0.3841 val_ll=0.4022 val_auc=0.7313 *BEST*\n",
      "Epoch 11 | train_ll=0.3830 val_ll=0.4041 val_auc=0.7303 stale 1/5\n",
      "Epoch 12 | train_ll=0.3821 val_ll=0.4027 val_auc=0.7311 stale 2/5\n",
      "Epoch 13 | train_ll=0.3814 val_ll=0.4040 val_auc=0.7302 stale 3/5\n",
      "Epoch 14 | train_ll=0.3808 val_ll=0.4047 val_auc=0.7305 stale 4/5\n",
      "Epoch 15 | train_ll=0.3803 val_ll=0.4039 val_auc=0.7305 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "AVAZU LR RESULTS  |  BASE=runs_avazu_40m_improved_ft_afm\n",
      "================================================================================\n",
      "Test AUC:     0.7087\n",
      "Test LogLoss: 0.4893\n",
      "Saved to: runs_avazu_LR_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING AVAZU  |  FM  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "[warn] Could not init output bias; skipping.\n",
      "Epoch 01 | train_ll=0.4124 val_ll=0.4136 val_auc=0.7152 *BEST*\n",
      "Epoch 02 | train_ll=0.3798 val_ll=0.4157 val_auc=0.7190 *BEST*\n",
      "Epoch 03 | train_ll=0.3725 val_ll=0.4226 val_auc=0.7088 stale 1/5\n",
      "Epoch 04 | train_ll=0.3676 val_ll=0.4412 val_auc=0.6998 stale 2/5\n",
      "Epoch 05 | train_ll=0.3638 val_ll=0.4385 val_auc=0.6992 stale 3/5\n",
      "Epoch 06 | train_ll=0.3609 val_ll=0.4462 val_auc=0.6967 stale 4/5\n",
      "Epoch 07 | train_ll=0.3584 val_ll=0.4595 val_auc=0.6926 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "AVAZU FM RESULTS  |  BASE=runs_avazu_40m_improved_ft_afm\n",
      "================================================================================\n",
      "Test AUC:     0.7022\n",
      "Test LogLoss: 0.4614\n",
      "Saved to: runs_avazu_FM_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING AVAZU  |  DeepFM  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.4045 val_ll=0.4013 val_auc=0.7175 *BEST*\n",
      "Epoch 02 | train_ll=0.3780 val_ll=0.4124 val_auc=0.7144 stale 1/5\n",
      "Epoch 03 | train_ll=0.3691 val_ll=0.4153 val_auc=0.7103 stale 2/5\n",
      "Epoch 04 | train_ll=0.3631 val_ll=0.4288 val_auc=0.7046 stale 3/5\n",
      "Epoch 05 | train_ll=0.3583 val_ll=0.4308 val_auc=0.7050 stale 4/5\n",
      "Epoch 06 | train_ll=0.3540 val_ll=0.4411 val_auc=0.6975 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "AVAZU DeepFM RESULTS  |  BASE=runs_avazu_40m_improved_ft_afm\n",
      "================================================================================\n",
      "Test AUC:     0.6575\n",
      "Test LogLoss: 0.4560\n",
      "Saved to: runs_avazu_DeepFM_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING AVAZU  |  DCN  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.4074 val_ll=0.3989 val_auc=0.7226 *BEST*\n",
      "Epoch 02 | train_ll=0.3829 val_ll=0.3990 val_auc=0.7261 *BEST*\n",
      "Epoch 03 | train_ll=0.3759 val_ll=0.4082 val_auc=0.7239 stale 1/5\n",
      "Epoch 04 | train_ll=0.3706 val_ll=0.4048 val_auc=0.7217 stale 2/5\n",
      "Epoch 05 | train_ll=0.3663 val_ll=0.4087 val_auc=0.7236 stale 3/5\n",
      "Epoch 06 | train_ll=0.3625 val_ll=0.4116 val_auc=0.7233 stale 4/5\n",
      "Epoch 07 | train_ll=0.3593 val_ll=0.4223 val_auc=0.7168 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "AVAZU DCN RESULTS  |  BASE=runs_avazu_40m_improved_ft_afm\n",
      "================================================================================\n",
      "Test AUC:     0.7000\n",
      "Test LogLoss: 0.4743\n",
      "Saved to: runs_avazu_DCN_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING AVAZU  |  FinalMLP  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n",
      "Epoch 01 | train_ll=0.3934 val_ll=0.3822 val_auc=0.7474 *BEST*\n",
      "Epoch 02 | train_ll=0.3871 val_ll=0.3824 val_auc=0.7479 *BEST*\n",
      "Epoch 03 | train_ll=0.3826 val_ll=0.3825 val_auc=0.7479 stale 1/5\n",
      "Epoch 04 | train_ll=0.3787 val_ll=0.3831 val_auc=0.7468 stale 2/5\n",
      "Epoch 05 | train_ll=0.3757 val_ll=0.3833 val_auc=0.7474 stale 3/5\n",
      "Epoch 06 | train_ll=0.3733 val_ll=0.3842 val_auc=0.7467 stale 4/5\n",
      "Epoch 07 | train_ll=0.3715 val_ll=0.3847 val_auc=0.7460 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "AVAZU FinalMLP RESULTS  |  BASE=runs_avazu_40m_improved_ft_afm\n",
      "================================================================================\n",
      "Test AUC:     0.7371\n",
      "Test LogLoss: 0.4008\n",
      "Saved to: runs_avazu_FinalMLP_seed42/\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "RUNNING AVAZU  |  FT-only  |  batch=8192  |  GPUs=4\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_ll=0.3938 val_ll=0.3826 val_auc=0.7504 *BEST*\n",
      "Epoch 02 | train_ll=0.3853 val_ll=0.3811 val_auc=0.7507 *BEST*\n",
      "Epoch 03 | train_ll=0.3797 val_ll=0.3814 val_auc=0.7526 *BEST*\n",
      "Epoch 04 | train_ll=0.3759 val_ll=0.3828 val_auc=0.7501 stale 1/5\n",
      "Epoch 05 | train_ll=0.3731 val_ll=0.3839 val_auc=0.7498 stale 2/5\n",
      "Epoch 06 | train_ll=0.3706 val_ll=0.3848 val_auc=0.7486 stale 3/5\n",
      "Epoch 07 | train_ll=0.3683 val_ll=0.3913 val_auc=0.7445 stale 4/5\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, time, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "# -----------------------------\n",
    "# Repro / device\n",
    "# -----------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "print(\"DEVICE:\", DEVICE, \"| GPUs available:\", NUM_GPUS)\n",
    "\n",
    "def unwrap(m):  # for DataParallel\n",
    "    return m.module if isinstance(m, nn.DataParallel) else m\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset (memmap safe)\n",
    "# -----------------------------\n",
    "class NPYDataset(Dataset):\n",
    "    def __init__(self, Xc, Xn, y):\n",
    "        self.Xc = Xc; self.Xn = Xn; self.y = y\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.from_numpy(self.Xc[idx].astype(np.int64)),\n",
    "                torch.from_numpy(self.Xn[idx].astype(np.float32)),\n",
    "                torch.tensor(float(self.y[idx]), dtype=torch.float32))\n",
    "\n",
    "def make_loader(Xc, Xn, y, batch_size, shuffle, workers=4):\n",
    "    return DataLoader(NPYDataset(Xc, Xn, y),\n",
    "                      batch_size=batch_size, shuffle=shuffle,\n",
    "                      num_workers=workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Pretty logger\n",
    "# -----------------------------\n",
    "class PrettyLogger:\n",
    "    def __init__(self, patience=5):\n",
    "        self.best_auc = -1.0\n",
    "        self.stale = 0\n",
    "        self.patience = patience\n",
    "    def log(self, epoch, train_ll, val_ll, val_auc):\n",
    "        if val_auc > self.best_auc + 1e-12:\n",
    "            self.best_auc = val_auc\n",
    "            self.stale = 0\n",
    "            tag = \" *BEST*\"\n",
    "        else:\n",
    "            self.stale += 1\n",
    "            tag = f\" stale {self.stale}/{self.patience}\"\n",
    "        print(f\"Epoch {epoch:02d} | train_ll={train_ll:.4f} val_ll={val_ll:.4f} val_auc={val_auc:.4f}{tag}\")\n",
    "    def should_stop(self): return self.stale >= self.patience\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "class WideLR(nn.Module):\n",
    "    \"\"\"Paper-standard 'wide' LR: linear numeric + per-field embedding to scalar.\"\"\"\n",
    "    def __init__(self, cat_cards, n_num):\n",
    "        super().__init__()\n",
    "        self.cat_w = nn.ModuleList([nn.Embedding(c, 1) for c in cat_cards])\n",
    "        self.num_w = nn.Linear(n_num, 1)\n",
    "    def forward(self, x_cat, x_num):\n",
    "        s = self.num_w(x_num).squeeze(-1)\n",
    "        for i, emb in enumerate(self.cat_w):\n",
    "            s = s + emb(x_cat[:, i]).squeeze(-1)\n",
    "        return s\n",
    "\n",
    "class CatEmbeddings(nn.Module):\n",
    "    def __init__(self, cat_cards, d):\n",
    "        super().__init__()\n",
    "        self.embs = nn.ModuleList([nn.Embedding(c, d) for c in cat_cards])\n",
    "        for e in self.embs:\n",
    "            nn.init.normal_(e.weight, std=0.01)\n",
    "    def forward(self, x_cat):\n",
    "        outs = [emb(x_cat[:, i]) for i, emb in enumerate(self.embs)]\n",
    "        return torch.stack(outs, dim=1)  # [B,F,d]\n",
    "\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, cat_cards, n_num, d=16):\n",
    "        super().__init__()\n",
    "        self.lr = WideLR(cat_cards, n_num)\n",
    "        self.cat_emb = CatEmbeddings(cat_cards, d)\n",
    "        self.num_proj = nn.Linear(n_num, d, bias=False)\n",
    "    def forward(self, x_cat, x_num):\n",
    "        lin = self.lr(x_cat, x_num)\n",
    "        Ec = self.cat_emb(x_cat)                          # [B,F,d]\n",
    "        En = self.num_proj(x_num).unsqueeze(1)            # [B,1,d]\n",
    "        E = torch.cat([Ec, En], dim=1)                    # [B,F+1,d]\n",
    "        s = E.sum(dim=1)\n",
    "        fm = 0.5 * (s*s - (E*E).sum(dim=1)).sum(dim=1)\n",
    "        return lin + fm\n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, cat_cards, n_num, d=16, hidden=(400,400,400), dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fm = FM(cat_cards, n_num, d)\n",
    "        self.cat_emb = CatEmbeddings(cat_cards, d)\n",
    "        self.num_emb = nn.Linear(n_num, d)\n",
    "        in_dim = (len(cat_cards)+1) * d\n",
    "        layers = []\n",
    "        cur = in_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(cur, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            cur = h\n",
    "        layers += [nn.Linear(cur, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    def forward(self, x_cat, x_num):\n",
    "        y_fm = self.fm(x_cat, x_num)\n",
    "        Ec = self.cat_emb(x_cat)\n",
    "        En = self.num_emb(x_num).unsqueeze(1)\n",
    "        x = torch.cat([Ec, En], dim=1).flatten(1)\n",
    "        y_dnn = self.mlp(x).squeeze(-1)\n",
    "        return y_fm + y_dnn\n",
    "\n",
    "class DCN(nn.Module):\n",
    "    def __init__(self, cat_cards, n_num, d=16, cross_layers=3, hidden=(400,400), dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lr = WideLR(cat_cards, n_num)\n",
    "        self.cat_emb = CatEmbeddings(cat_cards, d)\n",
    "        self.num_emb = nn.Linear(n_num, d)\n",
    "        self.D = (len(cat_cards)+1) * d\n",
    "\n",
    "        self.cross_w = nn.ParameterList([nn.Parameter(torch.randn(self.D, 1)*0.01) for _ in range(cross_layers)])\n",
    "        self.cross_b = nn.ParameterList([nn.Parameter(torch.zeros(self.D)) for _ in range(cross_layers)])\n",
    "\n",
    "        layers = []\n",
    "        cur = self.D\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(cur, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            cur = h\n",
    "        self.deep = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(cur + self.D, 1)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        Ec = self.cat_emb(x_cat)\n",
    "        En = self.num_emb(x_num).unsqueeze(1)\n",
    "        x0 = torch.cat([Ec, En], dim=1).flatten(1)  # [B,D]\n",
    "        x = x0\n",
    "        for w, b in zip(self.cross_w, self.cross_b):\n",
    "            xlw = (x @ w).squeeze(1)               # [B]\n",
    "            x = x0 * xlw.unsqueeze(1) + b + x\n",
    "        deep_out = self.deep(x0)\n",
    "        return self.out(torch.cat([x, deep_out], dim=1)).squeeze(-1) + self.lr(x_cat, x_num)\n",
    "\n",
    "class FinalMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper-style FinalMLP core:\n",
    "    - Dual-stream MLP\n",
    "    - Stream-specific feature selection (bit-wise gates)\n",
    "    - Multi-head bilinear fusion of stream representations\n",
    "    \"\"\"\n",
    "    def __init__(self, cat_cards, n_num, d=16, hidden=(512,256,128), dropout=0.2, n_heads=4, bilinear_dim=128):\n",
    "        super().__init__()\n",
    "        self.F = len(cat_cards); self.d = d\n",
    "        self.cat_emb = nn.ModuleList([nn.Embedding(c, d) for c in cat_cards])\n",
    "        self.num_emb = nn.Linear(n_num, d)\n",
    "\n",
    "        self.gate_a = nn.ModuleList([nn.Linear(d, d) for _ in range(self.F + 1)])\n",
    "        self.gate_b = nn.ModuleList([nn.Linear(d, d) for _ in range(self.F + 1)])\n",
    "\n",
    "        in_dim = (self.F + 1) * d\n",
    "        self.mlp_a, last_dim = self._mlp(in_dim, hidden, dropout)\n",
    "        self.mlp_b, _        = self._mlp(in_dim, hidden, dropout)\n",
    "\n",
    "        assert last_dim % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = last_dim // n_heads\n",
    "        self.W = nn.Parameter(torch.randn(n_heads, self.head_dim, self.head_dim) * 0.01)\n",
    "        self.bilinear_out = nn.Linear(n_heads * self.head_dim, bilinear_dim)\n",
    "\n",
    "        self.out = nn.Linear(last_dim*2 + bilinear_dim, 1)\n",
    "\n",
    "    def _mlp(self, in_dim, hidden, dropout):\n",
    "        layers = []\n",
    "        cur = in_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(cur, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            cur = h\n",
    "        return nn.Sequential(*layers), cur\n",
    "\n",
    "    def _embed(self, x_cat, x_num):\n",
    "        B = x_cat.size(0)\n",
    "        Ec = torch.stack([emb(x_cat[:, i]) for i, emb in enumerate(self.cat_emb)], dim=1)  # [B,F,d]\n",
    "        En = self.num_emb(x_num).view(B,1,self.d)                                          # [B,1,d]\n",
    "        return torch.cat([Ec, En], dim=1)                                                  # [B,F+1,d]\n",
    "\n",
    "    def _gate(self, E, gates):\n",
    "        outs = []\n",
    "        for i in range(E.size(1)):\n",
    "            g = torch.sigmoid(gates[i](E[:, i, :]))\n",
    "            outs.append(E[:, i, :] * g)\n",
    "        return torch.stack(outs, dim=1)\n",
    "\n",
    "    def _mh_bilinear(self, ha, hb):\n",
    "        B = ha.size(0)\n",
    "        ha = ha.view(B, self.n_heads, self.head_dim)\n",
    "        hb = hb.view(B, self.n_heads, self.head_dim)\n",
    "        hw = torch.einsum(\"bhd,hde->bhe\", ha, self.W)          # [B,H,Dh]\n",
    "        z  = (hw * hb).reshape(B, self.n_heads * self.head_dim)\n",
    "        return self.bilinear_out(z)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        E  = self._embed(x_cat, x_num)\n",
    "        Ea = self._gate(E, self.gate_a).flatten(1)\n",
    "        Eb = self._gate(E, self.gate_b).flatten(1)\n",
    "        ha = self.mlp_a(Ea)\n",
    "        hb = self.mlp_b(Eb)\n",
    "        hib = self._mh_bilinear(ha, hb)\n",
    "        return self.out(torch.cat([ha, hb, hib], dim=1)).squeeze(-1)\n",
    "    \n",
    "class FeatureTokenizer(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, n_num, d_model):\n",
    "        super().__init__()\n",
    "        self.cat_embs = nn.ModuleList([nn.Embedding(card, d_model) for card in cat_cardinalities])\n",
    "        self.num_proj = nn.ModuleList([nn.Linear(1, d_model) for _ in range(n_num)])\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        B = x_cat.size(0)\n",
    "        cat_tokens = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embs)]\n",
    "        num_tokens = [proj(x_num[:, i:i+1]) for i, proj in enumerate(self.num_proj)]\n",
    "        field_embs = torch.stack(cat_tokens + num_tokens, dim=1)   # [B, F, d]\n",
    "        cls = self.cls.expand(B, -1, -1)                           # [B, 1, d]\n",
    "        tokens = torch.cat([cls, field_embs], dim=1)               # [B, 1+F, d]\n",
    "        return tokens, field_embs\n",
    "\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15):\n",
    "        super().__init__()\n",
    "        enc = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    \n",
    "class FTOnly(nn.Module):\n",
    "    def __init__(self, cat_cards, n_num,\n",
    "                 d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.tok = FeatureTokenizer(cat_cards, n_num, d_model)\n",
    "        self.backbone = FTTransformer(d_model, nhead, ff, n_layers, dropout)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, 256), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        tokens, _ = self.tok(x_cat, x_num)\n",
    "        H = self.backbone(tokens)\n",
    "        h_cls = H[:, 0, :]\n",
    "        return self.head(h_cls).squeeze(1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train/Eval\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    total_loss, n = 0.0, 0\n",
    "    for x_cat, x_num, y in loader:\n",
    "        x_cat = x_cat.to(DEVICE, non_blocking=True)\n",
    "        x_num = x_num.to(DEVICE, non_blocking=True)\n",
    "        y     = y.to(DEVICE, non_blocking=True)\n",
    "        logits = model(x_cat, x_num)\n",
    "        loss = bce(logits, y)\n",
    "        total_loss += float(loss) * len(y)\n",
    "        n += len(y)\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "        ps.append(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_pred = np.concatenate(ps)\n",
    "    return total_loss / n, roc_auc_score(y_true, y_pred)\n",
    "\n",
    "def init_output_bias_for_ctr(model, base_ctr):\n",
    "    m = unwrap(model)\n",
    "    b = math.log(base_ctr/(1.0-base_ctr))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # common \"out\" (DCN / FinalMLP in our code)\n",
    "        if hasattr(m, \"out\") and isinstance(m.out, nn.Linear) and m.out.bias is not None:\n",
    "            m.out.bias.fill_(b); return\n",
    "\n",
    "        # your FT-AFM style \"head\"\n",
    "        if hasattr(m, \"head\") and isinstance(m.head, nn.Sequential):\n",
    "            last = m.head[-1]\n",
    "            if isinstance(last, nn.Linear) and last.bias is not None:\n",
    "                last.bias.fill_(b); return\n",
    "\n",
    "        # WideLR\n",
    "        if hasattr(m, \"num_w\") and isinstance(m.num_w, nn.Linear) and m.num_w.bias is not None:\n",
    "            m.num_w.bias.fill_(b); return\n",
    "\n",
    "        # DeepFM: last layer inside mlp\n",
    "        if hasattr(m, \"mlp\") and isinstance(m.mlp, nn.Sequential):\n",
    "            for layer in reversed(m.mlp):\n",
    "                if isinstance(layer, nn.Linear) and layer.bias is not None:\n",
    "                    layer.bias.fill_(b); return\n",
    "\n",
    "        print(\"[warn] Could not init output bias; skipping.\")\n",
    "\n",
    "\n",
    "def train_one(name, build_model_fn, BASE, out_dir,\n",
    "              batch_size=8192, max_epochs=20, patience=5, lr=1e-3,\n",
    "              sample_size=None, val_size=None, test_size=None):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    Xc_tr = np.load(os.path.join(BASE, \"Xc_train.npy\"), mmap_mode=\"r\")\n",
    "    Xc_va = np.load(os.path.join(BASE, \"Xc_val.npy\"),   mmap_mode=\"r\")\n",
    "    Xc_te = np.load(os.path.join(BASE, \"Xc_test.npy\"),  mmap_mode=\"r\")\n",
    "    Xn_tr = np.load(os.path.join(BASE, \"Xn_train.npy\"), mmap_mode=\"r\")\n",
    "    Xn_va = np.load(os.path.join(BASE, \"Xn_val.npy\"),   mmap_mode=\"r\")\n",
    "    Xn_te = np.load(os.path.join(BASE, \"Xn_test.npy\"),  mmap_mode=\"r\")\n",
    "    y_tr  = np.load(os.path.join(BASE, \"y_train.npy\"),  mmap_mode=\"r\")\n",
    "    y_va  = np.load(os.path.join(BASE, \"y_val.npy\"),    mmap_mode=\"r\")\n",
    "    y_te  = np.load(os.path.join(BASE, \"y_test.npy\"),   mmap_mode=\"r\")\n",
    "\n",
    "    schema = json.load(open(os.path.join(BASE, \"schema.json\"), \"r\"))\n",
    "    cat_cards = schema[\"cat_cards\"]\n",
    "    n_num = len(schema[\"num_cols\"])\n",
    "\n",
    "    # ----------------------------\n",
    "    # SMALL-SAMPLE SMOKE TEST\n",
    "    # ----------------------------\n",
    "    if sample_size is not None:\n",
    "        ntr = min(int(sample_size), len(y_tr))\n",
    "        if val_size is None:\n",
    "            val_size = max(200, ntr // 5)\n",
    "        if test_size is None:\n",
    "            test_size = max(200, ntr // 5)\n",
    "\n",
    "        nva = min(int(val_size), len(y_va))\n",
    "        nte = min(int(test_size), len(y_te))\n",
    "\n",
    "        Xc_tr, Xn_tr, y_tr = Xc_tr[:ntr], Xn_tr[:ntr], y_tr[:ntr]\n",
    "        Xc_va, Xn_va, y_va = Xc_va[:nva], Xn_va[:nva], y_va[:nva]\n",
    "        Xc_te, Xn_te, y_te = Xc_te[:nte], Xn_te[:nte], y_te[:nte]\n",
    "\n",
    "        print(f\"[SMOKE] train={len(y_tr)} val={len(y_va)} test={len(y_te)}\")\n",
    "\n",
    "    tr_loader = make_loader(Xc_tr, Xn_tr, y_tr, batch_size=batch_size, shuffle=True)\n",
    "    va_loader = make_loader(Xc_va, Xn_va, y_va, batch_size=batch_size, shuffle=False)\n",
    "    te_loader = make_loader(Xc_te, Xn_te, y_te, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = build_model_fn(cat_cards, n_num).to(DEVICE)\n",
    "    if NUM_GPUS > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    init_output_bias_for_ctr(model, float(np.mean(y_tr)))\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    logger = PrettyLogger(patience=patience)\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss, n = 0.0, 0\n",
    "        for x_cat, x_num, y in tr_loader:\n",
    "            x_cat = x_cat.to(DEVICE, non_blocking=True)\n",
    "            x_num = x_num.to(DEVICE, non_blocking=True)\n",
    "            y     = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x_cat, x_num)\n",
    "            loss = bce(logits, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += float(loss) * len(y)\n",
    "            n += len(y)\n",
    "\n",
    "        train_ll = total_loss / n\n",
    "        val_ll, val_auc = evaluate(model, va_loader)\n",
    "        logger.log(epoch, train_ll, val_ll, val_auc)\n",
    "\n",
    "        if val_auc > best_auc + 1e-12:\n",
    "            best_auc = val_auc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in unwrap(model).state_dict().items()}\n",
    "\n",
    "        if logger.should_stop():\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    if best_state is not None:\n",
    "        unwrap(model).load_state_dict(best_state)\n",
    "\n",
    "    test_ll, test_auc = evaluate(model, te_loader)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{name} RESULTS  |  BASE={BASE}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Test AUC:     {test_auc:.4f}\")\n",
    "    print(f\"Test LogLoss: {test_ll:.4f}\")\n",
    "    print(f\"Saved to: {out_dir}/\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"results.json\"), \"w\") as f:\n",
    "        json.dump({\"test_auc\": float(test_auc), \"test_logloss\": float(test_ll)}, f, indent=2)\n",
    "\n",
    "    return float(test_auc), float(test_ll)\n",
    "\n",
    "def run_all_baselines(BASE, prefix,\n",
    "                      batch_size=8192, max_epochs=20, patience=5, lr=1e-3):\n",
    "    results = {}\n",
    "\n",
    "    def lr_fn(cat_cards, n_num):      return WideLR(cat_cards, n_num)\n",
    "    def fm_fn(cat_cards, n_num):      return FM(cat_cards, n_num, d=16)\n",
    "    def deepfm_fn(cat_cards, n_num):  return DeepFM(cat_cards, n_num, d=16)\n",
    "    def dcn_fn(cat_cards, n_num):     return DCN(cat_cards, n_num, d=16, cross_layers=3)\n",
    "    def finalmlp_fn(cat_cards, n_num):return FinalMLP(cat_cards, n_num, d=16, n_heads=4, bilinear_dim=128)\n",
    "    def ftonly_fn(cat_cards, n_num):  return FTOnly(cat_cards, n_num, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15)\n",
    "\n",
    "\n",
    "    models = [\n",
    "        (\"LR\", lr_fn),\n",
    "        (\"FM\", fm_fn),\n",
    "        (\"DeepFM\", deepfm_fn),\n",
    "        (\"DCN\", dcn_fn),\n",
    "        (\"FinalMLP\", finalmlp_fn),\n",
    "        (\"FT-only\", ftonly_fn),\n",
    "    ]\n",
    "\n",
    "    for name, fn in models:\n",
    "        out_dir = f\"runs_{prefix}_{name}_seed42\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"RUNNING {prefix.upper()}  |  {name}  |  batch={batch_size}  |  GPUs={NUM_GPUS}\")\n",
    "        print(\"=\"*80)\n",
    "        results[name] = train_one(name=f\"{prefix.upper()} {name}\",\n",
    "                                  build_model_fn=fn,\n",
    "                                  BASE=BASE,\n",
    "                                  out_dir=out_dir,\n",
    "                                  batch_size=batch_size,\n",
    "                                  max_epochs=max_epochs,\n",
    "                                  patience=patience,\n",
    "                                  lr=lr)\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# CALLS (edit BASE paths if needed)\n",
    "# -----------------------------\n",
    "# Use the folder names exactly as they appear in your file browser\n",
    "results_criteo = run_all_baselines(BASE=\"criteo_preprocessed\", prefix=\"criteo\")\n",
    "results_outbrain = run_all_baselines(BASE=\"outbrain_preprocessed_40m\", prefix=\"outbrain\")  # change if your folder differs\n",
    "results_avazu = run_all_baselines(BASE=\"runs_avazu_40m_improved_ft_afm\", prefix=\"avazu\")\n",
    "\n",
    "print(\"Ready. Uncomment the dataset runs at the bottom.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47391c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pandas as pd\n",
    "\n",
    "def collect_results(prefix):\n",
    "    rows = []\n",
    "    for d in sorted(os.listdir(\".\")):\n",
    "        if d.startswith(f\"runs_{prefix}_\") and os.path.isdir(d):\n",
    "            res_path = os.path.join(d, \"results.json\")\n",
    "            if os.path.exists(res_path):\n",
    "                with open(res_path, \"r\") as f:\n",
    "                    r = json.load(f)\n",
    "                model = d.replace(f\"runs_{prefix}_\", \"\").replace(\"_seed42\", \"\")\n",
    "                rows.append({\n",
    "                    \"Model\": model,\n",
    "                    \"Test AUC\": r[\"test_auc\"],\n",
    "                    \"Test LogLoss\": r[\"test_logloss\"]\n",
    "                })\n",
    "    df = pd.DataFrame(rows).sort_values(\"Test AUC\", ascending=False)\n",
    "    return df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949f17cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test AUC</th>\n",
       "      <th>Test LogLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FT-only</td>\n",
       "      <td>0.807624</td>\n",
       "      <td>0.444239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FinalMLP</td>\n",
       "      <td>0.806275</td>\n",
       "      <td>0.445242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DCN</td>\n",
       "      <td>0.797264</td>\n",
       "      <td>0.456308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepFM</td>\n",
       "      <td>0.792818</td>\n",
       "      <td>0.461807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FM</td>\n",
       "      <td>0.781562</td>\n",
       "      <td>0.474400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.781301</td>\n",
       "      <td>0.469319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Test AUC  Test LogLoss\n",
       "0   FT-only  0.807624      0.444239\n",
       "1  FinalMLP  0.806275      0.445242\n",
       "2       DCN  0.797264      0.456308\n",
       "3    DeepFM  0.792818      0.461807\n",
       "4        FM  0.781562      0.474400\n",
       "5        LR  0.781301      0.469319"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criteo_df   = collect_results(\"criteo\")\n",
    "outbrain_df = collect_results(\"outbrain\")\n",
    "avazu_df    = collect_results(\"avazu\")\n",
    "\n",
    "criteo_df\n",
    "#outbrain_df\n",
    "#avazu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4491f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
