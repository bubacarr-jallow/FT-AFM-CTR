{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da2feb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 4\n",
      "Using DataParallel with 4 GPUs\n",
      "\n",
      "================================================================================\n",
      "Starting FT-AFM training on Avazu 40M dataset\n",
      "================================================================================\n",
      "\n",
      "================= FT-AFM on 40M Samples =================\n",
      "Loading data...\n",
      "Loaded 40,428,967 rows\n",
      "Splitting data (8:1:1)...\n",
      "Train: 32,343,173, Val: 4,042,896, Test: 4,042,898\n",
      "Encoding categoricals...\n",
      "Engineering frequency features...\n",
      "Target encoding...\n",
      "Standardizing numerics...\n",
      "Preparing data loaders...\n",
      "Train batch size: 16384 (across 4 GPUs)\n",
      "Eval batch size: 32768\n",
      "Building FT-AFM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FT-AFM on 32,343,173 samples...\n",
      "Wrapping model with DataParallel across 4 GPUs\n",
      "  Epoch 1, Batch 100/1975, Loss: 0.4097\n",
      "  Epoch 1, Batch 200/1975, Loss: 0.4107\n",
      "  Epoch 1, Batch 300/1975, Loss: 0.4027\n",
      "  Epoch 1, Batch 400/1975, Loss: 0.4027\n",
      "  Epoch 1, Batch 500/1975, Loss: 0.4064\n",
      "  Epoch 1, Batch 600/1975, Loss: 0.4026\n",
      "  Epoch 1, Batch 700/1975, Loss: 0.3987\n",
      "  Epoch 1, Batch 800/1975, Loss: 0.3978\n",
      "  Epoch 1, Batch 900/1975, Loss: 0.3984\n",
      "  Epoch 1, Batch 1000/1975, Loss: 0.3970\n",
      "  Epoch 1, Batch 1100/1975, Loss: 0.3975\n",
      "  Epoch 1, Batch 1200/1975, Loss: 0.3967\n",
      "  Epoch 1, Batch 1300/1975, Loss: 0.3980\n",
      "  Epoch 1, Batch 1400/1975, Loss: 0.3978\n",
      "  Epoch 1, Batch 1500/1975, Loss: 0.4044\n",
      "  Epoch 1, Batch 1600/1975, Loss: 0.3968\n",
      "  Epoch 1, Batch 1700/1975, Loss: 0.3980\n",
      "  Epoch 1, Batch 1800/1975, Loss: 0.3982\n",
      "  Epoch 1, Batch 1900/1975, Loss: 0.3987\n",
      "[FT-AFM] ep01 train_ll=0.4009 val_ll=0.3874 val_auc=0.7422 *BEST*\n",
      "  Epoch 2, Batch 100/1975, Loss: 0.3991\n",
      "  Epoch 2, Batch 200/1975, Loss: 0.3955\n",
      "  Epoch 2, Batch 300/1975, Loss: 0.3906\n",
      "  Epoch 2, Batch 400/1975, Loss: 0.4010\n",
      "  Epoch 2, Batch 500/1975, Loss: 0.4024\n",
      "  Epoch 2, Batch 600/1975, Loss: 0.3991\n",
      "  Epoch 2, Batch 700/1975, Loss: 0.3991\n",
      "  Epoch 2, Batch 800/1975, Loss: 0.3963\n",
      "  Epoch 2, Batch 900/1975, Loss: 0.3927\n",
      "  Epoch 2, Batch 1000/1975, Loss: 0.3935\n",
      "  Epoch 2, Batch 1100/1975, Loss: 0.3966\n",
      "  Epoch 2, Batch 1200/1975, Loss: 0.3910\n",
      "  Epoch 2, Batch 1300/1975, Loss: 0.4040\n",
      "  Epoch 2, Batch 1400/1975, Loss: 0.3974\n",
      "  Epoch 2, Batch 1500/1975, Loss: 0.3985\n",
      "  Epoch 2, Batch 1600/1975, Loss: 0.3978\n",
      "  Epoch 2, Batch 1700/1975, Loss: 0.3861\n",
      "  Epoch 2, Batch 1800/1975, Loss: 0.3903\n",
      "  Epoch 2, Batch 1900/1975, Loss: 0.3867\n",
      "[FT-AFM] ep02 train_ll=0.3963 val_ll=0.3858 val_auc=0.7460 *BEST*\n",
      "  Epoch 3, Batch 100/1975, Loss: 0.3972\n",
      "  Epoch 3, Batch 200/1975, Loss: 0.3938\n",
      "  Epoch 3, Batch 300/1975, Loss: 0.3978\n",
      "  Epoch 3, Batch 400/1975, Loss: 0.3994\n",
      "  Epoch 3, Batch 500/1975, Loss: 0.3961\n",
      "  Epoch 3, Batch 600/1975, Loss: 0.3937\n",
      "  Epoch 3, Batch 700/1975, Loss: 0.3955\n",
      "  Epoch 3, Batch 800/1975, Loss: 0.3953\n",
      "  Epoch 3, Batch 900/1975, Loss: 0.4019\n",
      "  Epoch 3, Batch 1000/1975, Loss: 0.3899\n",
      "  Epoch 3, Batch 1100/1975, Loss: 0.3971\n",
      "  Epoch 3, Batch 1200/1975, Loss: 0.3956\n",
      "  Epoch 3, Batch 1300/1975, Loss: 0.3892\n",
      "  Epoch 3, Batch 1400/1975, Loss: 0.3942\n",
      "  Epoch 3, Batch 1500/1975, Loss: 0.3923\n",
      "  Epoch 3, Batch 1600/1975, Loss: 0.3898\n",
      "  Epoch 3, Batch 1700/1975, Loss: 0.4033\n",
      "  Epoch 3, Batch 1800/1975, Loss: 0.3910\n",
      "  Epoch 3, Batch 1900/1975, Loss: 0.3990\n",
      "[FT-AFM] ep03 train_ll=0.3950 val_ll=0.3851 val_auc=0.7456 *BEST*\n",
      "  Epoch 4, Batch 100/1975, Loss: 0.3909\n",
      "  Epoch 4, Batch 200/1975, Loss: 0.3969\n",
      "  Epoch 4, Batch 300/1975, Loss: 0.3889\n",
      "  Epoch 4, Batch 400/1975, Loss: 0.3950\n",
      "  Epoch 4, Batch 500/1975, Loss: 0.3862\n",
      "  Epoch 4, Batch 600/1975, Loss: 0.3935\n",
      "  Epoch 4, Batch 700/1975, Loss: 0.3982\n",
      "  Epoch 4, Batch 800/1975, Loss: 0.3930\n",
      "  Epoch 4, Batch 900/1975, Loss: 0.3894\n",
      "  Epoch 4, Batch 1000/1975, Loss: 0.3929\n",
      "  Epoch 4, Batch 1100/1975, Loss: 0.3920\n",
      "  Epoch 4, Batch 1200/1975, Loss: 0.3951\n",
      "  Epoch 4, Batch 1300/1975, Loss: 0.3972\n",
      "  Epoch 4, Batch 1400/1975, Loss: 0.3970\n",
      "  Epoch 4, Batch 1500/1975, Loss: 0.3876\n",
      "  Epoch 4, Batch 1600/1975, Loss: 0.3949\n",
      "  Epoch 4, Batch 1700/1975, Loss: 0.3972\n",
      "  Epoch 4, Batch 1800/1975, Loss: 0.3976\n",
      "  Epoch 4, Batch 1900/1975, Loss: 0.3904\n",
      "[FT-AFM] ep04 train_ll=0.3943 val_ll=0.3859 val_auc=0.7456 stale 1/3\n",
      "  Epoch 5, Batch 100/1975, Loss: 0.3960\n",
      "  Epoch 5, Batch 200/1975, Loss: 0.3886\n",
      "  Epoch 5, Batch 300/1975, Loss: 0.3903\n",
      "  Epoch 5, Batch 400/1975, Loss: 0.3931\n",
      "  Epoch 5, Batch 500/1975, Loss: 0.3957\n",
      "  Epoch 5, Batch 600/1975, Loss: 0.3933\n",
      "  Epoch 5, Batch 700/1975, Loss: 0.3960\n",
      "  Epoch 5, Batch 800/1975, Loss: 0.3922\n",
      "  Epoch 5, Batch 900/1975, Loss: 0.3967\n",
      "  Epoch 5, Batch 1000/1975, Loss: 0.3943\n",
      "  Epoch 5, Batch 1100/1975, Loss: 0.3951\n",
      "  Epoch 5, Batch 1200/1975, Loss: 0.3997\n",
      "  Epoch 5, Batch 1300/1975, Loss: 0.4019\n",
      "  Epoch 5, Batch 1400/1975, Loss: 0.3998\n",
      "  Epoch 5, Batch 1500/1975, Loss: 0.3855\n",
      "  Epoch 5, Batch 1600/1975, Loss: 0.3984\n",
      "  Epoch 5, Batch 1700/1975, Loss: 0.3904\n",
      "  Epoch 5, Batch 1800/1975, Loss: 0.3933\n",
      "  Epoch 5, Batch 1900/1975, Loss: 0.3960\n",
      "[FT-AFM] ep05 train_ll=0.3938 val_ll=0.3852 val_auc=0.7464 stale 2/3\n",
      "  Epoch 6, Batch 100/1975, Loss: 0.3972\n",
      "  Epoch 6, Batch 200/1975, Loss: 0.3935\n",
      "  Epoch 6, Batch 300/1975, Loss: 0.3953\n",
      "  Epoch 6, Batch 400/1975, Loss: 0.3841\n",
      "  Epoch 6, Batch 500/1975, Loss: 0.3962\n",
      "  Epoch 6, Batch 600/1975, Loss: 0.3899\n",
      "  Epoch 6, Batch 700/1975, Loss: 0.3948\n",
      "  Epoch 6, Batch 800/1975, Loss: 0.3947\n",
      "  Epoch 6, Batch 900/1975, Loss: 0.3915\n",
      "  Epoch 6, Batch 1000/1975, Loss: 0.3886\n",
      "  Epoch 6, Batch 1100/1975, Loss: 0.3984\n",
      "  Epoch 6, Batch 1200/1975, Loss: 0.4015\n",
      "  Epoch 6, Batch 1300/1975, Loss: 0.3895\n",
      "  Epoch 6, Batch 1400/1975, Loss: 0.3897\n",
      "  Epoch 6, Batch 1500/1975, Loss: 0.3875\n",
      "  Epoch 6, Batch 1600/1975, Loss: 0.3961\n",
      "  Epoch 6, Batch 1700/1975, Loss: 0.3969\n",
      "  Epoch 6, Batch 1800/1975, Loss: 0.3967\n",
      "  Epoch 6, Batch 1900/1975, Loss: 0.3874\n",
      "[FT-AFM] ep06 train_ll=0.3935 val_ll=0.3842 val_auc=0.7465 *BEST*\n",
      "  Epoch 7, Batch 100/1975, Loss: 0.3913\n",
      "  Epoch 7, Batch 200/1975, Loss: 0.3907\n",
      "  Epoch 7, Batch 300/1975, Loss: 0.3885\n",
      "  Epoch 7, Batch 400/1975, Loss: 0.3909\n",
      "  Epoch 7, Batch 500/1975, Loss: 0.3917\n",
      "  Epoch 7, Batch 600/1975, Loss: 0.3925\n",
      "  Epoch 7, Batch 700/1975, Loss: 0.3908\n",
      "  Epoch 7, Batch 800/1975, Loss: 0.3870\n",
      "  Epoch 7, Batch 900/1975, Loss: 0.3972\n",
      "  Epoch 7, Batch 1000/1975, Loss: 0.3901\n",
      "  Epoch 7, Batch 1100/1975, Loss: 0.3871\n",
      "  Epoch 7, Batch 1200/1975, Loss: 0.3989\n",
      "  Epoch 7, Batch 1300/1975, Loss: 0.3957\n",
      "  Epoch 7, Batch 1400/1975, Loss: 0.3954\n",
      "  Epoch 7, Batch 1500/1975, Loss: 0.3955\n",
      "  Epoch 7, Batch 1600/1975, Loss: 0.3942\n",
      "  Epoch 7, Batch 1700/1975, Loss: 0.3965\n",
      "  Epoch 7, Batch 1800/1975, Loss: 0.3941\n",
      "  Epoch 7, Batch 1900/1975, Loss: 0.3894\n",
      "[FT-AFM] ep07 train_ll=0.3933 val_ll=0.3845 val_auc=0.7464 stale 1/3\n",
      "  Epoch 8, Batch 100/1975, Loss: 0.3914\n",
      "  Epoch 8, Batch 200/1975, Loss: 0.3976\n",
      "  Epoch 8, Batch 300/1975, Loss: 0.3931\n",
      "  Epoch 8, Batch 400/1975, Loss: 0.3956\n",
      "  Epoch 8, Batch 500/1975, Loss: 0.3994\n",
      "  Epoch 8, Batch 600/1975, Loss: 0.3924\n",
      "  Epoch 8, Batch 700/1975, Loss: 0.3904\n",
      "  Epoch 8, Batch 800/1975, Loss: 0.3818\n",
      "  Epoch 8, Batch 900/1975, Loss: 0.3897\n",
      "  Epoch 8, Batch 1000/1975, Loss: 0.3910\n",
      "  Epoch 8, Batch 1100/1975, Loss: 0.3918\n",
      "  Epoch 8, Batch 1200/1975, Loss: 0.3984\n",
      "  Epoch 8, Batch 1300/1975, Loss: 0.4014\n",
      "  Epoch 8, Batch 1400/1975, Loss: 0.3957\n",
      "  Epoch 8, Batch 1500/1975, Loss: 0.3910\n",
      "  Epoch 8, Batch 1600/1975, Loss: 0.3886\n",
      "  Epoch 8, Batch 1700/1975, Loss: 0.4003\n",
      "  Epoch 8, Batch 1800/1975, Loss: 0.3936\n",
      "  Epoch 8, Batch 1900/1975, Loss: 0.3929\n",
      "[FT-AFM] ep08 train_ll=0.3932 val_ll=0.3844 val_auc=0.7464 stale 2/3\n",
      "  Epoch 9, Batch 100/1975, Loss: 0.3916\n",
      "  Epoch 9, Batch 200/1975, Loss: 0.3975\n",
      "  Epoch 9, Batch 300/1975, Loss: 0.3954\n",
      "  Epoch 9, Batch 400/1975, Loss: 0.3932\n",
      "  Epoch 9, Batch 500/1975, Loss: 0.3875\n",
      "  Epoch 9, Batch 600/1975, Loss: 0.3953\n",
      "  Epoch 9, Batch 700/1975, Loss: 0.3940\n",
      "  Epoch 9, Batch 800/1975, Loss: 0.3957\n",
      "  Epoch 9, Batch 900/1975, Loss: 0.4001\n",
      "  Epoch 9, Batch 1000/1975, Loss: 0.3901\n",
      "  Epoch 9, Batch 1100/1975, Loss: 0.4004\n",
      "  Epoch 9, Batch 1200/1975, Loss: 0.4016\n",
      "  Epoch 9, Batch 1300/1975, Loss: 0.3865\n",
      "  Epoch 9, Batch 1400/1975, Loss: 0.3918\n",
      "  Epoch 9, Batch 1500/1975, Loss: 0.3918\n",
      "  Epoch 9, Batch 1600/1975, Loss: 0.3900\n",
      "  Epoch 9, Batch 1700/1975, Loss: 0.3955\n",
      "  Epoch 9, Batch 1800/1975, Loss: 0.3919\n",
      "  Epoch 9, Batch 1900/1975, Loss: 0.3899\n",
      "[FT-AFM] ep09 train_ll=0.3932 val_ll=0.3850 val_auc=0.7463 stale 3/3\n",
      "[FT-AFM] early-stopped.\n",
      "Evaluating on test set...\n",
      "\n",
      "============================================================\n",
      "FT-AFM Results on 40M Samples:\n",
      "Test AUC: 0.7327\n",
      "Test LogLoss: 0.4054\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS - Avazu 40M FT-AFM\n",
      "================================================================================\n",
      "Test AUC:     0.732672\n",
      "Test LogLoss: 0.405355\n",
      "================================================================================\n",
      "\n",
      "Results saved to: runs_avazu_40m_ft_afm/\n",
      "✅ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FT-AFM Training on 40M Samples (8:1:1 split)\n",
    "# Based on original pipeline parameters\n",
    "# ============================================================\n",
    "import os, json, math, random, numpy as np, pandas as pd, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# ============== Repro/Device ==============\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "USE_DDP = NUM_GPUS > 1\n",
    "\n",
    "print(f\"Available GPUs: {NUM_GPUS}\")\n",
    "if USE_DDP:\n",
    "    print(f\"Using DataParallel with {NUM_GPUS} GPUs\")\n",
    "else:\n",
    "    print(f\"Using single device: {DEVICE}\")\n",
    "\n",
    "# ============== Utils ==============\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def kfold_target_encode(df_tr, df_va, col, yname, n_splits=5, min_samples=50, prior=None, seed=42):\n",
    "    if prior is None: prior = float(df_tr[yname].mean())\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    te_tr = pd.Series(np.zeros(len(df_tr), dtype=\"float32\"), index=df_tr.index)\n",
    "    for tr_idx, hold_idx in kf.split(df_tr):\n",
    "        cur = df_tr.iloc[tr_idx]\n",
    "        means = cur.groupby(col)[yname].mean()\n",
    "        cnts  = cur.groupby(col)[yname].size()\n",
    "        m = (means*cnts + prior*min_samples) / (cnts + min_samples)\n",
    "        te_tr.iloc[hold_idx] = df_tr.iloc[hold_idx][col].map(m).fillna(prior).astype(\"float32\")\n",
    "    means = df_tr.groupby(col)[yname].mean()\n",
    "    cnts  = df_tr.groupby(col)[yname].size()\n",
    "    mfull = (means*cnts + prior*min_samples) / (cnts + min_samples)\n",
    "    te_va = df_va[col].map(mfull).fillna(prior).astype(\"float32\")\n",
    "    return te_tr, te_va\n",
    "\n",
    "def temporal_or_stratified_split(df, label, time_cols=None, order_key=None, train=0.8, val=0.1, test=0.1):\n",
    "    assert abs(train + val + test - 1.0) < 1e-8\n",
    "    if time_cols and all(c in df.columns for c in time_cols):\n",
    "        key = order_key if order_key is not None else time_cols\n",
    "        df_sorted = df.sort_values(key).reset_index(drop=True)\n",
    "        n = len(df_sorted); n_tr = int(train*n); n_va = int(val*n)\n",
    "        df_tr = df_sorted.iloc[:n_tr].copy()\n",
    "        df_va = df_sorted.iloc[n_tr:n_tr+n_va].copy()\n",
    "        df_te = df_sorted.iloc[n_tr+n_va:].copy()\n",
    "    else:\n",
    "        df_tr, df_tmp = train_test_split(df, test_size=(1-train), stratify=df[label], random_state=42)\n",
    "        df_va, df_te = train_test_split(df_tmp, test_size=(test/(test+val)), stratify=df_tmp[label], random_state=42)\n",
    "        df_tr, df_va, df_te = df_tr.copy(), df_va.copy(), df_te.copy()\n",
    "    return df_tr, df_va, df_te\n",
    "\n",
    "# ============== Models ==============\n",
    "class AFM(nn.Module):\n",
    "    def __init__(self, d, attn_dim=32):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(d, attn_dim, bias=False)\n",
    "        self.h = nn.Linear(attn_dim, 1, bias=False)\n",
    "    def forward(self, E):\n",
    "        B,F,d = E.shape; pairs=[]\n",
    "        for i in range(F):\n",
    "            for j in range(i+1, F):\n",
    "                pairs.append(E[:,i]*E[:,j])\n",
    "        P = torch.stack(pairs, dim=1)\n",
    "        A = torch.softmax(self.h(torch.tanh(self.W(P))), dim=1)\n",
    "        return (A * P).sum(dim=1)\n",
    "\n",
    "class FeatureTokenizer(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, n_num, d_model):\n",
    "        super().__init__()\n",
    "        self.cat_embs = nn.ModuleList([nn.Embedding(card, d_model) for card in cat_cardinalities])\n",
    "        self.num_proj = nn.ModuleList([nn.Linear(1, d_model) for _ in range(n_num)])\n",
    "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)); nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "    def forward(self, x_cat, x_num):\n",
    "        B = x_cat.size(0)\n",
    "        cat_tokens = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embs)]\n",
    "        num_tokens = [proj(x_num[:, i:i+1]) for i, proj in enumerate(self.num_proj)]\n",
    "        field_embs = torch.stack(cat_tokens + num_tokens, dim=1)\n",
    "        cls = self.cls.expand(B, -1, -1)\n",
    "        tokens = torch.cat([cls, field_embs], dim=1)\n",
    "        return tokens, field_embs\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, d_model=128, nhead=8, ff=256, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=ff,\n",
    "                                         dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "    def forward(self, tokens): return self.encoder(tokens)\n",
    "\n",
    "class FTWithAFM(nn.Module):\n",
    "    def __init__(self, cat_cards, n_num, d_model=128, nhead=8, ff=256, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tok = FeatureTokenizer(cat_cards, n_num, d_model)\n",
    "        self.backbone = FTTransformer(d_model, nhead, ff, n_layers, dropout)\n",
    "        self.afm = AFM(d_model, attn_dim=32)\n",
    "        fusion_in = d_model + d_model\n",
    "        self.head = nn.Sequential(nn.Linear(fusion_in, 128), nn.ReLU(), nn.Dropout(dropout), nn.Linear(128, 1))\n",
    "    def forward(self, x_cat, x_num):\n",
    "        tokens, field_embs = self.tok(x_cat, x_num)\n",
    "        H = self.backbone(tokens); cls = H[:,0,:]\n",
    "        afm_out = self.afm(field_embs)\n",
    "        z = torch.cat([cls, afm_out], dim=1)\n",
    "        return self.head(z).squeeze(1)\n",
    "\n",
    "# ============== Train/Eval ==============\n",
    "def init_final_bias_to_ctr(module_last_linear, base_ctr):\n",
    "    with torch.no_grad():\n",
    "        module_last_linear.bias.fill_(math.log(base_ctr/(1.0-base_ctr)))\n",
    "\n",
    "def evaluate_model(model, dl):\n",
    "    model.eval(); ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xc, Xn, yb in dl:\n",
    "            Xc, Xn, yb = Xc.to(DEVICE), Xn.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(Xc, Xn)\n",
    "            if logits.dim()==1: logits = logits.unsqueeze(1)\n",
    "            probs = torch.sigmoid(logits).squeeze(-1)\n",
    "            ys.append(yb.squeeze(-1).cpu().numpy()); ps.append(probs.cpu().numpy())\n",
    "    y_true = np.concatenate(ys); y_prob = np.clip(np.concatenate(ps), 1e-7, 1-1e-7)\n",
    "    return roc_auc_score(y_true, y_prob), log_loss(y_true, y_prob)\n",
    "\n",
    "class TrainCfg:\n",
    "    def __init__(self, d_model=128, nhead=8, ff=256, n_layers=2, dropout=0.10,\n",
    "                 lr=1e-3, weight_decay=1e-4, max_epochs=12, patience=3, clip=1.0):\n",
    "        self.d_model=d_model; self.nhead=nhead; self.ff=ff; self.n_layers=n_layers\n",
    "        self.dropout=dropout; self.lr=lr; self.weight_decay=weight_decay\n",
    "        self.max_epochs=max_epochs; self.patience=patience; self.clip=clip\n",
    "        self.use_amp=torch.cuda.is_available()\n",
    "\n",
    "def train_ft_afm(model, tr_dl, va_dl, cfg, outdir):\n",
    "    ensure_dir(outdir)\n",
    "    \n",
    "    # Wrap model with DataParallel if multiple GPUs available\n",
    "    if USE_DDP and NUM_GPUS > 1:\n",
    "        print(f\"Wrapping model with DataParallel across {NUM_GPUS} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scaler = GradScaler(device='cuda', enabled=cfg.use_amp)\n",
    "    best_ll, best_state, stale = float(\"inf\"), None, 0\n",
    "    history = []\n",
    "    \n",
    "    for ep in range(1, cfg.max_epochs+1):\n",
    "        model.train(); run = 0.0\n",
    "        batch_count = 0\n",
    "        for Xc, Xn, yb in tr_dl:\n",
    "            Xc, Xn, yb = Xc.to(DEVICE), Xn.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type='cuda', enabled=cfg.use_amp):\n",
    "                logits = model(Xc, Xn)\n",
    "                if logits.dim()==1: logits = logits.unsqueeze(1)\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Get actual model for gradient clipping (unwrap DataParallel if needed)\n",
    "            model_to_clip = model.module if isinstance(model, nn.DataParallel) else model\n",
    "            torch.nn.utils.clip_grad_norm_(model_to_clip.parameters(), cfg.clip)\n",
    "            \n",
    "            scaler.step(opt); scaler.update()\n",
    "            run += loss.item() * yb.size(0)\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Progress indicator every 100 batches\n",
    "            if batch_count % 100 == 0:\n",
    "                print(f\"  Epoch {ep}, Batch {batch_count}/{len(tr_dl)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        val_auc, val_ll = evaluate_model(model, va_dl)\n",
    "        train_ll = run / len(tr_dl.dataset)\n",
    "        improved = val_ll + 1e-9 < best_ll\n",
    "        if improved:\n",
    "            best_ll = val_ll; stale = 0\n",
    "            # Save underlying model state (unwrap DataParallel)\n",
    "            model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n",
    "            best_state = {k: v.detach().cpu().clone() for k,v in model_to_save.state_dict().items()}\n",
    "        else:\n",
    "            stale += 1\n",
    "\n",
    "        history.append({\"epoch\": ep, \"train_ll\": float(train_ll), \"val_ll\": float(val_ll), \"val_auc\": float(val_auc)})\n",
    "        print(f\"[FT-AFM] ep{ep:02d} train_ll={train_ll:.4f} val_ll={val_ll:.4f} val_auc={val_auc:.4f} \"\n",
    "              f\"{'*BEST*' if improved else f'stale {stale}/{cfg.patience}'}\")\n",
    "        if stale >= cfg.patience:\n",
    "            print(f\"[FT-AFM] early-stopped.\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        # Load back to the underlying model\n",
    "        model_to_load = model.module if isinstance(model, nn.DataParallel) else model\n",
    "        model_to_load.load_state_dict(best_state)\n",
    "        torch.save(best_state, os.path.join(outdir, \"ft_afm_40m_best.pth\"))\n",
    "    pd.DataFrame(history).to_csv(os.path.join(outdir, \"ft_afm_40m_history.csv\"), index=False)\n",
    "    \n",
    "    val_auc, val_ll = evaluate_model(model, va_dl)\n",
    "    save_json({\"val_auc\": float(val_auc), \"val_logloss\": float(val_ll)}, \n",
    "              os.path.join(outdir, \"ft_afm_40m_val_metrics.json\"))\n",
    "    \n",
    "    # Return the unwrapped model\n",
    "    return model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "# ============== Dataset ==============\n",
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, Xc, Xn, y):\n",
    "        self.Xc = torch.as_tensor(Xc, dtype=torch.long)\n",
    "        self.Xn = torch.as_tensor(Xn, dtype=torch.float32)\n",
    "        self.y  = torch.as_tensor(y,  dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.Xc[i], self.Xn[i], self.y[i].unsqueeze(-1)\n",
    "\n",
    "# ============== Main Runner ==============\n",
    "def run_ft_afm_40m(csv_path, outdir, label_col, time_info, base_num_cols, drop_cols, \n",
    "                   single_freq_cats, pair_freq_cats, te_targets):\n",
    "    print(f\"\\n================= FT-AFM on 40M Samples =================\")\n",
    "    ensure_dir(outdir)\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    if csv_path.endswith('.parquet'):\n",
    "        df = pd.read_parquet(csv_path)\n",
    "    else:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {len(df):,} rows\")\n",
    "    assert label_col in df.columns\n",
    "\n",
    "    # Save schema before\n",
    "    schema_before = {\n",
    "        \"n_rows\": int(len(df)),\n",
    "        \"n_cols\": int(len(df.columns)),\n",
    "        \"positive_rate\": float(df[label_col].mean())\n",
    "    }\n",
    "    save_json(schema_before, os.path.join(outdir, \"schema_before.json\"))\n",
    "\n",
    "    # Cast base numeric columns\n",
    "    for c in base_num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # 8:1:1 split\n",
    "    print(\"Splitting data (8:1:1)...\")\n",
    "    if time_info is not None:\n",
    "        df_tr, df_va, df_te = temporal_or_stratified_split(\n",
    "            df, label_col, time_cols=time_info.get(\"time_cols\"), \n",
    "            order_key=time_info.get(\"order_key\"), train=0.8, val=0.1, test=0.1\n",
    "        )\n",
    "    else:\n",
    "        df_tr, df_va, df_te = temporal_or_stratified_split(\n",
    "            df, label_col, time_cols=None, order_key=None, train=0.8, val=0.1, test=0.1\n",
    "        )\n",
    "    print(f\"Train: {len(df_tr):,}, Val: {len(df_va):,}, Test: {len(df_te):,}\")\n",
    "\n",
    "    # Feature engineering\n",
    "    all_cols_tr = df_tr.columns.tolist()\n",
    "    drop_cols_eff = [c for c in drop_cols if c in all_cols_tr]\n",
    "    base_num_eff = [c for c in base_num_cols if c in all_cols_tr]\n",
    "    cat_cols = [c for c in all_cols_tr if c not in ([label_col] + drop_cols_eff + base_num_eff)]\n",
    "    num_cols = base_num_eff[:]\n",
    "\n",
    "    # Categorical encoding with UNK\n",
    "    print(\"Encoding categoricals...\")\n",
    "    cat_cards, cat_maps = [], {}\n",
    "    for c in cat_cols:\n",
    "        uniq = pd.Index(df_tr[c].astype(\"object\").unique())\n",
    "        mapping = {v:i for i,v in enumerate(uniq)}\n",
    "        unk_id = len(mapping)\n",
    "        cat_maps[c] = (mapping, unk_id)\n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            d[c] = d[c].map(mapping).fillna(unk_id).astype(\"int64\")\n",
    "        cat_cards.append(unk_id + 1)\n",
    "\n",
    "    # Numeric base features\n",
    "    for c in num_cols:\n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\").fillna(0).astype(\"float32\")\n",
    "\n",
    "    # Single frequency features\n",
    "    print(\"Engineering frequency features...\")\n",
    "    cand_freq = [c for c in single_freq_cats if c in df_tr.columns]\n",
    "    for c in cand_freq:\n",
    "        vc = df_tr[c].value_counts()\n",
    "        df_tr[f\"{c}_freq\"] = df_tr[c].map(vc).astype(\"float32\")\n",
    "        df_va[f\"{c}_freq\"] = df_va[c].map(vc).fillna(0).astype(\"float32\")\n",
    "        df_te[f\"{c}_freq\"] = df_te[c].map(vc).fillna(0).astype(\"float32\")\n",
    "\n",
    "    # Pairwise frequency features\n",
    "    pairs_eff = [(a,b) for (a,b) in pair_freq_cats if set([a,b]).issubset(df_tr.columns)]\n",
    "    for a,b in pairs_eff:\n",
    "        key_tr = df_tr[a].astype(\"int64\")*10_000_000 + df_tr[b].astype(\"int64\")\n",
    "        key_va = df_va[a].astype(\"int64\")*10_000_000 + df_va[b].astype(\"int64\")\n",
    "        key_te = df_te[a].astype(\"int64\")*10_000_000 + df_te[b].astype(\"int64\")\n",
    "        vc = key_tr.value_counts()\n",
    "        name = f\"{a}__{b}__freq\"\n",
    "        df_tr[name] = key_tr.map(vc).astype(\"float32\")\n",
    "        df_va[name] = key_va.map(vc).fillna(0).astype(\"float32\")\n",
    "        df_te[name] = key_te.map(vc).fillna(0).astype(\"float32\")\n",
    "\n",
    "    # Target encoding\n",
    "    print(\"Target encoding...\")\n",
    "    te_eff = [c for c in te_targets if c in df_tr.columns]\n",
    "    for c in te_eff:\n",
    "        te_tr, te_va = kfold_target_encode(df_tr, df_va, c, yname=label_col, n_splits=5, min_samples=50)\n",
    "        prior = float(df_tr[label_col].mean())\n",
    "        means = df_tr.groupby(c)[label_col].mean()\n",
    "        cnts  = df_tr.groupby(c)[label_col].size()\n",
    "        mfull = (means*cnts + prior*50) / (cnts + 50)\n",
    "        te_te = df_te[c].map(mfull).fillna(prior).astype(\"float32\")\n",
    "        df_tr[f\"{c}_te\"] = te_tr.astype(\"float32\")\n",
    "        df_va[f\"{c}_te\"] = te_va.astype(\"float32\")\n",
    "        df_te[f\"{c}_te\"] = te_te.astype(\"float32\")\n",
    "\n",
    "    # Register new numeric columns\n",
    "    new_num = [f\"{c}_freq\" for c in cand_freq] + [f\"{a}__{b}__freq\" for (a,b) in pairs_eff] + [f\"{c}_te\" for c in te_eff]\n",
    "    for c in new_num:\n",
    "        if c not in num_cols: num_cols.append(c)\n",
    "\n",
    "    # Log1p freq features\n",
    "    freq_like = [c for c in num_cols if c.endswith(\"_freq\")]\n",
    "    for c in freq_like:\n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            d[c] = np.log1p(pd.to_numeric(d[c], errors=\"coerce\").fillna(0).clip(lower=0).astype(\"float64\"))\n",
    "\n",
    "    # Standardize all numerics\n",
    "    print(\"Standardizing numerics...\")\n",
    "    num_means = {c: float(pd.to_numeric(df_tr[c], errors=\"coerce\").fillna(0).mean()) for c in num_cols}\n",
    "    num_stds  = {c: float(pd.to_numeric(df_tr[c], errors=\"coerce\").fillna(0).std(ddof=0)) for c in num_cols}\n",
    "    for c in num_cols:\n",
    "        mu, sd = num_means[c], (num_stds[c] if num_stds[c] > 1e-8 else 1.0)\n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            v = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "            d[c] = ((v - mu)/sd).replace([np.inf,-np.inf], np.nan).fillna(0).astype(\"float32\")\n",
    "\n",
    "    # Save schema after\n",
    "    schema_after = {\n",
    "        \"cat_cols\": cat_cols,\n",
    "        \"num_cols\": num_cols,\n",
    "        \"cat_cards\": cat_cards,\n",
    "        \"splits\": {\"train\": len(df_tr), \"val\": len(df_va), \"test\": len(df_te)},\n",
    "        \"train_ctr\": float(df_tr[label_col].mean()),\n",
    "        \"val_ctr\": float(df_va[label_col].mean()),\n",
    "        \"test_ctr\": float(df_te[label_col].mean())\n",
    "    }\n",
    "    save_json(schema_after, os.path.join(outdir, \"schema_after.json\"))\n",
    "\n",
    "    # Prepare tensors\n",
    "    print(\"Preparing data loaders...\")\n",
    "    Xc_tr = df_tr[cat_cols].to_numpy()\n",
    "    Xn_tr = df_tr[num_cols].to_numpy().astype(\"float32\")\n",
    "    y_tr = df_tr[label_col].to_numpy().astype(\"float32\")\n",
    "    \n",
    "    Xc_va = df_va[cat_cols].to_numpy()\n",
    "    Xn_va = df_va[num_cols].to_numpy().astype(\"float32\")\n",
    "    y_va = df_va[label_col].to_numpy().astype(\"float32\")\n",
    "    \n",
    "    Xc_te = df_te[cat_cols].to_numpy()\n",
    "    Xn_te = df_te[num_cols].to_numpy().astype(\"float32\")\n",
    "    y_te = df_te[label_col].to_numpy().astype(\"float32\")\n",
    "\n",
    "    # Increase batch size for multi-GPU training\n",
    "    batch_size_train = 4096 * NUM_GPUS if USE_DDP else 4096\n",
    "    batch_size_eval = 8192 * NUM_GPUS if USE_DDP else 8192\n",
    "    \n",
    "    print(f\"Train batch size: {batch_size_train} (across {NUM_GPUS} GPUs)\")\n",
    "    print(f\"Eval batch size: {batch_size_eval}\")\n",
    "    \n",
    "    tr_dl = DataLoader(CTRDataset(Xc_tr, Xn_tr, y_tr), batch_size=batch_size_train, \n",
    "                       shuffle=True, num_workers=4, pin_memory=True)\n",
    "    va_dl = DataLoader(CTRDataset(Xc_va, Xn_va, y_va), batch_size=batch_size_eval, \n",
    "                       shuffle=False, num_workers=4, pin_memory=True)\n",
    "    te_dl = DataLoader(CTRDataset(Xc_te, Xn_te, y_te), batch_size=batch_size_eval, \n",
    "                       shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Build and train FT-AFM\n",
    "    base_ctr = float(df_tr[label_col].mean())\n",
    "    cfg = TrainCfg(d_model=128, nhead=8, ff=256, n_layers=2, dropout=0.10, \n",
    "                   lr=1e-3, weight_decay=1e-4, max_epochs=12, patience=3)\n",
    "    \n",
    "    print(\"Building FT-AFM model...\")\n",
    "    model = FTWithAFM(cat_cards, len(num_cols), cfg.d_model, cfg.nhead, \n",
    "                      cfg.ff, cfg.n_layers, cfg.dropout).to(DEVICE)\n",
    "    init_final_bias_to_ctr(model.head[-1], base_ctr)\n",
    "    \n",
    "    print(f\"Training FT-AFM on {len(df_tr):,} samples...\")\n",
    "    model = train_ft_afm(model, tr_dl, va_dl, cfg, outdir)\n",
    "    \n",
    "    # Final test evaluation\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_auc, test_ll = evaluate_model(model, te_dl)\n",
    "    save_json({\"test_auc\": float(test_auc), \"test_logloss\": float(test_ll)},\n",
    "              os.path.join(outdir, \"ft_afm_40m_test_metrics.json\"))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FT-AFM Results on 40M Samples:\")\n",
    "    print(f\"Test AUC: {test_auc:.4f}\")\n",
    "    print(f\"Test LogLoss: {test_ll:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return test_auc, test_ll\n",
    "\n",
    "# ================== RUN: AVAZU 40M ==================\n",
    "AVAZU_40M_PATH = \"/home/elicer/ctr_project/data/avazu_full/avazu_full.parquet\"\n",
    "OUT_AVAZU_40M = \"runs_avazu_40m_ft_afm\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting FT-AFM training on Avazu 40M dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_auc, test_ll = run_ft_afm_40m(\n",
    "    csv_path=AVAZU_40M_PATH,\n",
    "    outdir=OUT_AVAZU_40M,\n",
    "    label_col=\"click\",\n",
    "    time_info={\"time_cols\":[\"hour\"], \"order_key\":\"hour\"},\n",
    "    base_num_cols=[\"hour\"],\n",
    "    drop_cols=[\"id\"],\n",
    "    single_freq_cats=[\"C14\",\"C17\",\"site_id\",\"app_id\",\"device_model\"],\n",
    "    pair_freq_cats=[(\"site_id\",\"app_id\"),(\"device_model\",\"hour\")],\n",
    "    te_targets=[\"C14\",\"site_id\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS - Avazu 40M FT-AFM\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test AUC:     {test_auc:.6f}\")\n",
    "print(f\"Test LogLoss: {test_ll:.6f}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nResults saved to: {OUT_AVAZU_40M}/\")\n",
    "print(\"✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0f728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 4\n",
      "Using DataParallel with 4 GPUs\n",
      "\n",
      "================================================================================\n",
      "Starting IMPROVED FT-AFM training on Avazu 40M dataset\n",
      "IMPROVEMENTS:\n",
      "  • Larger model: d=192, layers=3, ff=512\n",
      "  • Time features: hour_of_day, day_of_week, is_weekend\n",
      "  • More target encoding: C14, C17, site_id, app_id, device_model\n",
      "  • Enhanced AFM: attn_dim=64 with dropout\n",
      "  • Learning rate schedule: warmup + cosine annealing\n",
      "  • Longer training: 20 epochs, patience=5\n",
      "  • Better regularization: dropout=0.15, weight_decay=5e-5\n",
      "  • More cross features\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "IMPROVED FT-AFM on 40M Samples\n",
      "================================================================================\n",
      "Loading data...\n",
      "Loaded 40,428,967 rows\n",
      "Extracting time features...\n",
      "Added time features: ['hour_of_day', 'day_of_week', 'day_of_month', 'is_weekend']\n",
      "Splitting data (8:1:1)...\n",
      "Train: 32,343,173, Val: 4,042,896, Test: 4,042,898\n",
      "Encoding categoricals with rare category handling...\n",
      "  C1: 7 frequent categories, 8 total (with UNK)\n",
      "  banner_pos: 7 frequent categories, 8 total (with UNK)\n",
      "  site_id: 3005 frequent categories, 3006 total (with UNK)\n",
      "  site_domain: 3294 frequent categories, 3295 total (with UNK)\n",
      "  site_category: 22 frequent categories, 23 total (with UNK)\n",
      "  app_id: 3726 frequent categories, 3727 total (with UNK)\n",
      "  app_domain: 235 frequent categories, 236 total (with UNK)\n",
      "  app_category: 27 frequent categories, 28 total (with UNK)\n",
      "  device_id: 78121 frequent categories, 78122 total (with UNK)\n",
      "  device_ip: 414180 frequent categories, 414181 total (with UNK)\n",
      "  device_model: 5719 frequent categories, 5720 total (with UNK)\n",
      "  device_type: 5 frequent categories, 6 total (with UNK)\n",
      "  device_conn_type: 4 frequent categories, 5 total (with UNK)\n",
      "  C14: 2072 frequent categories, 2073 total (with UNK)\n",
      "  C15: 8 frequent categories, 9 total (with UNK)\n",
      "  C16: 9 frequent categories, 10 total (with UNK)\n",
      "  C17: 366 frequent categories, 367 total (with UNK)\n",
      "  C18: 4 frequent categories, 5 total (with UNK)\n",
      "  C19: 61 frequent categories, 62 total (with UNK)\n",
      "  C20: 165 frequent categories, 166 total (with UNK)\n",
      "  C21: 52 frequent categories, 53 total (with UNK)\n",
      "Engineering frequency features...\n",
      "Creating 6 pairwise frequency features...\n",
      "Target encoding 5 features: ['C14', 'C17', 'site_id', 'app_id', 'device_model']\n",
      "Standardizing numerics...\n",
      "Total features: 21 categorical + 25 numeric = 46\n",
      "Preparing data loaders...\n",
      "Saving preprocessed data for ablation studies...\n",
      "✅ Preprocessed data saved to runs_avazu_40m_improved_ft_afm/\n",
      "Train batch size: 8192 (reduced for memory)\n",
      "Eval batch size: 16384 (reduced for memory)\n",
      "\n",
      "Model Configuration:\n",
      "  d_model: 192, n_layers: 3, ff: 512\n",
      "  dropout: 0.15, lr: 0.001, weight_decay: 5e-05\n",
      "  max_epochs: 20, patience: 5, warmup: 2\n",
      "\n",
      "Building Improved FT-AFM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 99,325,761\n",
      "Trainable parameters: 99,325,761\n",
      "⏭️ Skipping FT-AFM training (preprocessing only).\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS - Avazu 40M IMPROVED FT-AFM\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 607\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINAL RESULTS - Avazu 40M IMPROVED FT-AFM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest AUC:     \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest LogLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_ll\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPROVED FT-AFM Training on 40M Samples\n",
    "# Enhancements:\n",
    "# - Larger model capacity (d=192, layers=3, ff=512)\n",
    "# - Enhanced feature engineering (time features, more TE, more crosses)\n",
    "# - Multi-head AFM with dropout\n",
    "# - Learning rate scheduling (cosine annealing + warmup)\n",
    "# - Better regularization\n",
    "# - Longer training with patience\n",
    "# ============================================================\n",
    "import os, json, math, random, numpy as np, pandas as pd, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# ============== Repro/Device ==============\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "USE_DDP = NUM_GPUS > 1\n",
    "\n",
    "print(f\"Available GPUs: {NUM_GPUS}\")\n",
    "if USE_DDP:\n",
    "    print(f\"Using DataParallel with {NUM_GPUS} GPUs\")\n",
    "else:\n",
    "    print(f\"Using single device: {DEVICE}\")\n",
    "    \n",
    "RUN_FT_AFM = False\n",
    "\n",
    "\n",
    "# ============== Utils ==============\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def kfold_target_encode(df_tr, df_va, col, yname, n_splits=5, min_samples=50, prior=None, seed=42):\n",
    "    if prior is None: prior = float(df_tr[yname].mean())\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    te_tr = pd.Series(np.zeros(len(df_tr), dtype=\"float32\"), index=df_tr.index)\n",
    "    for tr_idx, hold_idx in kf.split(df_tr):\n",
    "        cur = df_tr.iloc[tr_idx]\n",
    "        means = cur.groupby(col)[yname].mean()\n",
    "        cnts  = cur.groupby(col)[yname].size()\n",
    "        m = (means*cnts + prior*min_samples) / (cnts + min_samples)\n",
    "        te_tr.iloc[hold_idx] = df_tr.iloc[hold_idx][col].map(m).fillna(prior).astype(\"float32\")\n",
    "    means = df_tr.groupby(col)[yname].mean()\n",
    "    cnts  = df_tr.groupby(col)[yname].size()\n",
    "    mfull = (means*cnts + prior*min_samples) / (cnts + min_samples)\n",
    "    te_va = df_va[col].map(mfull).fillna(prior).astype(\"float32\")\n",
    "    return te_tr, te_va\n",
    "\n",
    "def temporal_or_stratified_split(df, label, time_cols=None, order_key=None, train=0.8, val=0.1, test=0.1):\n",
    "    assert abs(train + val + test - 1.0) < 1e-8\n",
    "    if time_cols and all(c in df.columns for c in time_cols):\n",
    "        key = order_key if order_key is not None else time_cols\n",
    "        df_sorted = df.sort_values(key).reset_index(drop=True)\n",
    "        n = len(df_sorted); n_tr = int(train*n); n_va = int(val*n)\n",
    "        df_tr = df_sorted.iloc[:n_tr].copy()\n",
    "        df_va = df_sorted.iloc[n_tr:n_tr+n_va].copy()\n",
    "        df_te = df_sorted.iloc[n_tr+n_va:].copy()\n",
    "    else:\n",
    "        df_tr, df_tmp = train_test_split(df, test_size=(1-train), stratify=df[label], random_state=42)\n",
    "        df_va, df_te = train_test_split(df_tmp, test_size=(test/(test+val)), stratify=df_tmp[label], random_state=42)\n",
    "        df_tr, df_va, df_te = df_tr.copy(), df_va.copy(), df_te.copy()\n",
    "    return df_tr, df_va, df_te\n",
    "\n",
    "# ============== IMPROVED Models ==============\n",
    "class ImprovedAFM(nn.Module):\n",
    "    \"\"\"AFM with larger attention dimension and dropout\"\"\"\n",
    "    def __init__(self, d, attn_dim=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(d, attn_dim, bias=False)\n",
    "        self.h = nn.Linear(attn_dim, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, E):\n",
    "        B,F,d = E.shape; pairs=[]\n",
    "        for i in range(F):\n",
    "            for j in range(i+1, F):\n",
    "                pairs.append(E[:,i]*E[:,j])\n",
    "        P = torch.stack(pairs, dim=1)\n",
    "        P = self.dropout(P)\n",
    "        A = torch.softmax(self.h(torch.tanh(self.W(P))), dim=1)\n",
    "        return (A * P).sum(dim=1)\n",
    "\n",
    "class FeatureTokenizer(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, n_num, d_model):\n",
    "        super().__init__()\n",
    "        self.cat_embs = nn.ModuleList([nn.Embedding(card, d_model) for card in cat_cardinalities])\n",
    "        self.num_proj = nn.ModuleList([nn.Linear(1, d_model) for _ in range(n_num)])\n",
    "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)); nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "    def forward(self, x_cat, x_num):\n",
    "        B = x_cat.size(0)\n",
    "        cat_tokens = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embs)]\n",
    "        num_tokens = [proj(x_num[:, i:i+1]) for i, proj in enumerate(self.num_proj)]\n",
    "        field_embs = torch.stack(cat_tokens + num_tokens, dim=1)\n",
    "        cls = self.cls.expand(B, -1, -1)\n",
    "        tokens = torch.cat([cls, field_embs], dim=1)\n",
    "        return tokens, field_embs\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15):\n",
    "        super().__init__()\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=ff,\n",
    "                                         dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "    def forward(self, tokens): return self.encoder(tokens)\n",
    "\n",
    "class ImprovedFTAFM(nn.Module):\n",
    "    \"\"\"Improved FT+AFM with larger capacity and better fusion\"\"\"\n",
    "    def __init__(self, cat_cards, n_num, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15, afm_attn_dim=64):\n",
    "        super().__init__()\n",
    "        self.tok = FeatureTokenizer(cat_cards, n_num, d_model)\n",
    "        self.backbone = FTTransformer(d_model, nhead, ff, n_layers, dropout)\n",
    "        self.afm = ImprovedAFM(d_model, afm_attn_dim, dropout=dropout)\n",
    "        \n",
    "        # Larger head\n",
    "        fusion_dim = d_model + d_model\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_cat, x_num):\n",
    "        tokens, field_embs = self.tok(x_cat, x_num)\n",
    "        H = self.backbone(tokens); h_cls = H[:,0,:]\n",
    "        v_afm = self.afm(field_embs)\n",
    "        z = torch.cat([h_cls, v_afm], dim=1)\n",
    "        return self.head(z).squeeze(1)\n",
    "\n",
    "# ============== Train/Eval ==============\n",
    "def init_final_bias_to_ctr(module_last_linear, base_ctr):\n",
    "    with torch.no_grad():\n",
    "        module_last_linear.bias.fill_(math.log(base_ctr/(1.0-base_ctr)))\n",
    "\n",
    "def evaluate_model(model, dl, use_smaller_batches=False):\n",
    "    \"\"\"Evaluate with optional smaller batches to avoid OOM\"\"\"\n",
    "    model.eval(); ys, ps = [], []\n",
    "    \n",
    "    # Unwrap DataParallel if present to avoid issues\n",
    "    model_eval = model.module if isinstance(model, nn.DataParallel) else model\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for Xc, Xn, yb in dl:\n",
    "            # If OOM during eval, process in smaller chunks\n",
    "            if use_smaller_batches and Xc.size(0) > 2048:\n",
    "                chunk_size = 2048\n",
    "                batch_probs = []\n",
    "                for i in range(0, Xc.size(0), chunk_size):\n",
    "                    Xc_chunk = Xc[i:i+chunk_size].to(DEVICE)\n",
    "                    Xn_chunk = Xn[i:i+chunk_size].to(DEVICE)\n",
    "                    logits = model_eval(Xc_chunk, Xn_chunk)\n",
    "                    if logits.dim()==1: logits = logits.unsqueeze(1)\n",
    "                    probs = torch.sigmoid(logits).squeeze(-1)\n",
    "                    batch_probs.append(probs.cpu())\n",
    "                    # Clear cache after each chunk\n",
    "                    del Xc_chunk, Xn_chunk, logits, probs\n",
    "                    torch.cuda.empty_cache()\n",
    "                probs = torch.cat(batch_probs)\n",
    "            else:\n",
    "                Xc, Xn = Xc.to(DEVICE), Xn.to(DEVICE)\n",
    "                logits = model_eval(Xc, Xn)\n",
    "                if logits.dim()==1: logits = logits.unsqueeze(1)\n",
    "                probs = torch.sigmoid(logits).squeeze(-1).cpu()\n",
    "            \n",
    "            ys.append(yb.squeeze(-1).numpy())\n",
    "            ps.append(probs.numpy())\n",
    "            \n",
    "            # Clear GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    y_true = np.concatenate(ys); y_prob = np.clip(np.concatenate(ps), 1e-7, 1-1e-7)\n",
    "    return roc_auc_score(y_true, y_prob), log_loss(y_true, y_prob)\n",
    "\n",
    "class TrainCfg:\n",
    "    def __init__(self, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15,\n",
    "                 lr=1e-3, weight_decay=5e-5, max_epochs=20, patience=5, clip=1.0, warmup_epochs=2):\n",
    "        self.d_model=d_model; self.nhead=nhead; self.ff=ff; self.n_layers=n_layers\n",
    "        self.dropout=dropout; self.lr=lr; self.weight_decay=weight_decay\n",
    "        self.max_epochs=max_epochs; self.patience=patience; self.clip=clip\n",
    "        self.warmup_epochs=warmup_epochs\n",
    "        self.use_amp=torch.cuda.is_available()\n",
    "\n",
    "def get_warmup_cosine_scheduler(optimizer, warmup_epochs, max_epochs):\n",
    "    \"\"\"Warmup for first few epochs, then cosine decay\"\"\"\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            progress = (epoch - warmup_epochs) / (max_epochs - warmup_epochs)\n",
    "            return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "def train_improved_ft_afm(model, tr_dl, va_dl, cfg, outdir):\n",
    "    ensure_dir(outdir)\n",
    "    \n",
    "    # Wrap model with DataParallel if multiple GPUs available\n",
    "    if USE_DDP and NUM_GPUS > 1:\n",
    "        print(f\"Wrapping model with DataParallel across {NUM_GPUS} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = get_warmup_cosine_scheduler(opt, cfg.warmup_epochs, cfg.max_epochs)\n",
    "    scaler = GradScaler(device='cuda', enabled=cfg.use_amp)\n",
    "    best_ll, best_state, stale = float(\"inf\"), None, 0\n",
    "    history = []\n",
    "    \n",
    "    for ep in range(1, cfg.max_epochs+1):\n",
    "        model.train(); run = 0.0\n",
    "        batch_count = 0\n",
    "        for Xc, Xn, yb in tr_dl:\n",
    "            Xc, Xn, yb = Xc.to(DEVICE), Xn.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type='cuda', enabled=cfg.use_amp):\n",
    "                logits = model(Xc, Xn)\n",
    "                if logits.dim()==1: logits = logits.unsqueeze(1)\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            model_to_clip = model.module if isinstance(model, nn.DataParallel) else model\n",
    "            torch.nn.utils.clip_grad_norm_(model_to_clip.parameters(), cfg.clip)\n",
    "            \n",
    "            scaler.step(opt); scaler.update()\n",
    "            run += loss.item() * yb.size(0)\n",
    "            batch_count += 1\n",
    "            \n",
    "            if batch_count % 100 == 0:\n",
    "                print(f\"  Epoch {ep}, Batch {batch_count}/{len(tr_dl)}, Loss: {loss.item():.4f}, LR: {opt.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        val_auc, val_ll = evaluate_model(model, va_dl, use_smaller_batches=True)  # Changed to True\n",
    "        train_ll = run / len(tr_dl.dataset)\n",
    "        improved = val_ll + 1e-9 < best_ll\n",
    "        if improved:\n",
    "            best_ll = val_ll; stale = 0\n",
    "            model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n",
    "            best_state = {k: v.detach().cpu().clone() for k,v in model_to_save.state_dict().items()}\n",
    "        else:\n",
    "            stale += 1\n",
    "\n",
    "        history.append({\"epoch\": ep, \"train_ll\": float(train_ll), \"val_ll\": float(val_ll), \n",
    "                       \"val_auc\": float(val_auc), \"lr\": float(opt.param_groups[0]['lr'])})\n",
    "        print(f\"[IMPROVED FT-AFM] ep{ep:02d} train_ll={train_ll:.4f} val_ll={val_ll:.4f} val_auc={val_auc:.4f} \"\n",
    "              f\"lr={opt.param_groups[0]['lr']:.6f} {'*BEST*' if improved else f'stale {stale}/{cfg.patience}'}\")\n",
    "        if stale >= cfg.patience:\n",
    "            print(f\"[IMPROVED FT-AFM] early-stopped.\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model_to_load = model.module if isinstance(model, nn.DataParallel) else model\n",
    "        model_to_load.load_state_dict(best_state)\n",
    "        torch.save(best_state, os.path.join(outdir, \"improved_ft_afm_best.pth\"))\n",
    "    pd.DataFrame(history).to_csv(os.path.join(outdir, \"improved_ft_afm_history.csv\"), index=False)\n",
    "    \n",
    "    val_auc, val_ll = evaluate_model(model, va_dl, use_smaller_batches=True)  # Changed to True\n",
    "    save_json({\"val_auc\": float(val_auc), \"val_logloss\": float(val_ll)}, \n",
    "              os.path.join(outdir, \"improved_ft_afm_val_metrics.json\"))\n",
    "    \n",
    "    return model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "# ============== Dataset ==============\n",
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, Xc, Xn, y):\n",
    "        self.Xc = torch.as_tensor(Xc, dtype=torch.long)\n",
    "        self.Xn = torch.as_tensor(Xn, dtype=torch.float32)\n",
    "        self.y  = torch.as_tensor(y,  dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.Xc[i], self.Xn[i], self.y[i].unsqueeze(-1)\n",
    "\n",
    "# ============== Enhanced Feature Engineering ==============\n",
    "def extract_time_features(df, hour_col='hour'):\n",
    "    \"\"\"Extract time-based features from YYYYMMDDHH format\"\"\"\n",
    "    print(\"Extracting time features...\")\n",
    "    df[hour_col] = pd.to_numeric(df[hour_col], errors='coerce').fillna(0).astype('int64')\n",
    "    \n",
    "    # Hour of day (0-23)\n",
    "    df['hour_of_day'] = (df[hour_col] % 100).astype('int32')\n",
    "    \n",
    "    # Convert to datetime for more features\n",
    "    df['_temp_datetime'] = pd.to_datetime(df[hour_col], format='%y%m%d%H', errors='coerce')\n",
    "    df['day_of_week'] = df['_temp_datetime'].dt.dayofweek.fillna(0).astype('int32')\n",
    "    df['day_of_month'] = df['_temp_datetime'].dt.day.fillna(1).astype('int32')\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5,6]).astype('int32')\n",
    "    \n",
    "    # Drop temp column\n",
    "    df.drop('_temp_datetime', axis=1, inplace=True)\n",
    "    \n",
    "    return ['hour_of_day', 'day_of_week', 'day_of_month', 'is_weekend']\n",
    "\n",
    "# ============== Main Runner ==============\n",
    "def run_improved_ft_afm_40m(csv_path, outdir, label_col, time_info, base_num_cols, drop_cols, \n",
    "                            single_freq_cats, pair_freq_cats, te_targets):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"IMPROVED FT-AFM on 40M Samples\")\n",
    "    print(f\"{'='*80}\")\n",
    "    ensure_dir(outdir)\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    if csv_path.endswith('.parquet'):\n",
    "        df = pd.read_parquet(csv_path)\n",
    "    else:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {len(df):,} rows\")\n",
    "    assert label_col in df.columns\n",
    "\n",
    "    # Save schema before\n",
    "    schema_before = {\n",
    "        \"n_rows\": int(len(df)),\n",
    "        \"n_cols\": int(len(df.columns)),\n",
    "        \"positive_rate\": float(df[label_col].mean())\n",
    "    }\n",
    "    save_json(schema_before, os.path.join(outdir, \"schema_before.json\"))\n",
    "\n",
    "    # Cast base numeric columns\n",
    "    for c in base_num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # Extract time features BEFORE splitting\n",
    "    if 'hour' in df.columns:\n",
    "        time_features = extract_time_features(df, 'hour')\n",
    "        base_num_cols = base_num_cols + time_features\n",
    "        print(f\"Added time features: {time_features}\")\n",
    "\n",
    "    # 8:1:1 split\n",
    "    print(\"Splitting data (8:1:1)...\")\n",
    "    if time_info is not None:\n",
    "        df_tr, df_va, df_te = temporal_or_stratified_split(\n",
    "            df, label_col, time_cols=time_info.get(\"time_cols\"), \n",
    "            order_key=time_info.get(\"order_key\"), train=0.8, val=0.1, test=0.1\n",
    "        )\n",
    "    else:\n",
    "        df_tr, df_va, df_te = temporal_or_stratified_split(\n",
    "            df, label_col, time_cols=None, order_key=None, train=0.8, val=0.1, test=0.1\n",
    "        )\n",
    "    print(f\"Train: {len(df_tr):,}, Val: {len(df_va):,}, Test: {len(df_te):,}\")\n",
    "\n",
    "    # Feature lists\n",
    "    all_cols_tr = df_tr.columns.tolist()\n",
    "    drop_cols_eff = [c for c in drop_cols if c in all_cols_tr]\n",
    "    base_num_eff = [c for c in base_num_cols if c in all_cols_tr]\n",
    "    cat_cols = [c for c in all_cols_tr if c not in ([label_col] + drop_cols_eff + base_num_eff)]\n",
    "    num_cols = base_num_eff[:]\n",
    "\n",
    "    # Categorical encoding with rare category handling\n",
    "    print(\"Encoding categoricals with rare category handling...\")\n",
    "    cat_cards, cat_maps = [], {}\n",
    "    MIN_FREQ = 10  # Categories with < 10 occurrences -> UNK\n",
    "    \n",
    "    for c in cat_cols:\n",
    "        vc = df_tr[c].value_counts()\n",
    "        frequent = vc[vc >= MIN_FREQ].index\n",
    "        uniq = pd.Index(frequent)\n",
    "        mapping = {v:i for i,v in enumerate(uniq)}\n",
    "        unk_id = len(mapping)\n",
    "        cat_maps[c] = (mapping, unk_id)\n",
    "        \n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            d[c] = d[c].map(mapping).fillna(unk_id).astype(\"int64\")\n",
    "        cat_cards.append(unk_id + 1)\n",
    "        print(f\"  {c}: {len(mapping)} frequent categories, {unk_id+1} total (with UNK)\")\n",
    "\n",
    "    # Numeric base features\n",
    "    for c in num_cols:\n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\").fillna(0).astype(\"float32\")\n",
    "\n",
    "    # Single frequency features\n",
    "    print(\"Engineering frequency features...\")\n",
    "    cand_freq = [c for c in single_freq_cats if c in df_tr.columns]\n",
    "    for c in cand_freq:\n",
    "        vc = df_tr[c].value_counts()\n",
    "        df_tr[f\"{c}_freq\"] = df_tr[c].map(vc).astype(\"float32\")\n",
    "        df_va[f\"{c}_freq\"] = df_va[c].map(vc).fillna(0).astype(\"float32\")\n",
    "        df_te[f\"{c}_freq\"] = df_te[c].map(vc).fillna(0).astype(\"float32\")\n",
    "\n",
    "    # Pairwise frequency features\n",
    "    pairs_eff = [(a,b) for (a,b) in pair_freq_cats if set([a,b]).issubset(df_tr.columns)]\n",
    "    print(f\"Creating {len(pairs_eff)} pairwise frequency features...\")\n",
    "    for a,b in pairs_eff:\n",
    "        key_tr = df_tr[a].astype(\"int64\")*10_000_000 + df_tr[b].astype(\"int64\")\n",
    "        key_va = df_va[a].astype(\"int64\")*10_000_000 + df_va[b].astype(\"int64\")\n",
    "        key_te = df_te[a].astype(\"int64\")*10_000_000 + df_te[b].astype(\"int64\")\n",
    "        vc = key_tr.value_counts()\n",
    "        name = f\"{a}__{b}__freq\"\n",
    "        df_tr[name] = key_tr.map(vc).astype(\"float32\")\n",
    "        df_va[name] = key_va.map(vc).fillna(0).astype(\"float32\")\n",
    "        df_te[name] = key_te.map(vc).fillna(0).astype(\"float32\")\n",
    "\n",
    "    # Target encoding\n",
    "    te_eff = [c for c in te_targets if c in df_tr.columns]\n",
    "    print(f\"Target encoding {len(te_eff)} features: {te_eff}\")\n",
    "    for c in te_eff:\n",
    "        te_tr, te_va = kfold_target_encode(df_tr, df_va, c, yname=label_col, n_splits=5, min_samples=50)\n",
    "        prior = float(df_tr[label_col].mean())\n",
    "        means = df_tr.groupby(c)[label_col].mean()\n",
    "        cnts  = df_tr.groupby(c)[label_col].size()\n",
    "        mfull = (means*cnts + prior*50) / (cnts + 50)\n",
    "        te_te = df_te[c].map(mfull).fillna(prior).astype(\"float32\")\n",
    "        df_tr[f\"{c}_te\"] = te_tr.astype(\"float32\")\n",
    "        df_va[f\"{c}_te\"] = te_va.astype(\"float32\")\n",
    "        df_te[f\"{c}_te\"] = te_te.astype(\"float32\")\n",
    "\n",
    "    # Register engineered numerics\n",
    "    new_num = [f\"{c}_freq\" for c in cand_freq] + [f\"{a}__{b}__freq\" for (a,b) in pairs_eff] + [f\"{c}_te\" for c in te_eff]\n",
    "    for c in new_num:\n",
    "        if c not in num_cols: num_cols.append(c)\n",
    "\n",
    "    # Log1p freq features\n",
    "    freq_like = [c for c in num_cols if c.endswith(\"_freq\")]\n",
    "    for c in freq_like:\n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            d[c] = np.log1p(pd.to_numeric(d[c], errors=\"coerce\").fillna(0).clip(lower=0).astype(\"float64\"))\n",
    "\n",
    "    # Standardize all numerics\n",
    "    print(\"Standardizing numerics...\")\n",
    "    num_means = {c: float(pd.to_numeric(df_tr[c], errors=\"coerce\").fillna(0).mean()) for c in num_cols}\n",
    "    num_stds  = {c: float(pd.to_numeric(df_tr[c], errors=\"coerce\").fillna(0).std(ddof=0)) for c in num_cols}\n",
    "    for c in num_cols:\n",
    "        mu, sd = num_means[c], (num_stds[c] if num_stds[c] > 1e-8 else 1.0)\n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            v = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "            d[c] = ((v - mu)/sd).replace([np.inf,-np.inf], np.nan).fillna(0).astype(\"float32\")\n",
    "\n",
    "    # Save schema after\n",
    "    schema_after = {\n",
    "        \"cat_cols\": cat_cols,\n",
    "        \"num_cols\": num_cols,\n",
    "        \"cat_cards\": cat_cards,\n",
    "        \"n_features\": len(cat_cols) + len(num_cols),\n",
    "        \"splits\": {\"train\": len(df_tr), \"val\": len(df_va), \"test\": len(df_te)},\n",
    "        \"train_ctr\": float(df_tr[label_col].mean()),\n",
    "        \"val_ctr\": float(df_va[label_col].mean()),\n",
    "        \"test_ctr\": float(df_te[label_col].mean())\n",
    "    }\n",
    "    save_json(schema_after, os.path.join(outdir, \"schema_after.json\"))\n",
    "    print(f\"Total features: {len(cat_cols)} categorical + {len(num_cols)} numeric = {len(cat_cols)+len(num_cols)}\")\n",
    "\n",
    "    # Prepare tensors\n",
    "    print(\"Preparing data loaders...\")\n",
    "    Xc_tr = df_tr[cat_cols].to_numpy()\n",
    "    Xn_tr = df_tr[num_cols].to_numpy().astype(\"float32\")\n",
    "    y_tr = df_tr[label_col].to_numpy().astype(\"float32\")\n",
    "    \n",
    "    Xc_va = df_va[cat_cols].to_numpy()\n",
    "    Xn_va = df_va[num_cols].to_numpy().astype(\"float32\")\n",
    "    y_va = df_va[label_col].to_numpy().astype(\"float32\")\n",
    "    \n",
    "    Xc_te = df_te[cat_cols].to_numpy()\n",
    "    Xn_te = df_te[num_cols].to_numpy().astype(\"float32\")\n",
    "    y_te = df_te[label_col].to_numpy().astype(\"float32\")\n",
    "    \n",
    "    # Right after you have Xc_tr, Xn_tr, y_tr, etc.\n",
    "    # and BEFORE you create DataLoaders\n",
    "\n",
    "    print(\"Saving preprocessed data for ablation studies...\")\n",
    "    save_dir = \"runs_avazu_40m_improved_ft_afm\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save arrays\n",
    "    np.save(os.path.join(save_dir, 'Xc_train.npy'), Xc_tr)\n",
    "    np.save(os.path.join(save_dir, 'Xn_train.npy'), Xn_tr)\n",
    "    np.save(os.path.join(save_dir, 'y_train.npy'), y_tr)\n",
    "\n",
    "    np.save(os.path.join(save_dir, 'Xc_val.npy'), Xc_va)\n",
    "    np.save(os.path.join(save_dir, 'Xn_val.npy'), Xn_va)\n",
    "    np.save(os.path.join(save_dir, 'y_val.npy'), y_va)\n",
    "\n",
    "    np.save(os.path.join(save_dir, 'Xc_test.npy'), Xc_te)\n",
    "    np.save(os.path.join(save_dir, 'Xn_test.npy'), Xn_te)\n",
    "    np.save(os.path.join(save_dir, 'y_test.npy'), y_te)\n",
    "\n",
    "    # Save schema info\n",
    "    schema = {\n",
    "       'cat_cards': cat_cards,\n",
    "       'num_cols': num_cols\n",
    "    }\n",
    "    with open(os.path.join(save_dir, 'schema.json'), 'w') as f:\n",
    "        json.dump(schema, f)\n",
    "\n",
    "    print(f\"✅ Preprocessed data saved to {save_dir}/\")\n",
    "\n",
    "\n",
    "    # REDUCED batch sizes to avoid OOM with many features\n",
    "    batch_size_train = 2048 * NUM_GPUS if USE_DDP else 2048  # Reduced from 4096\n",
    "    batch_size_eval = 4096 * NUM_GPUS if USE_DDP else 4096   # Reduced from 8192\n",
    "    \n",
    "    print(f\"Train batch size: {batch_size_train} (reduced for memory)\")\n",
    "    print(f\"Eval batch size: {batch_size_eval} (reduced for memory)\")\n",
    "    \n",
    "    tr_dl = DataLoader(CTRDataset(Xc_tr, Xn_tr, y_tr), batch_size=batch_size_train, \n",
    "                       shuffle=True, num_workers=4, pin_memory=True)\n",
    "    va_dl = DataLoader(CTRDataset(Xc_va, Xn_va, y_va), batch_size=batch_size_eval, \n",
    "                       shuffle=False, num_workers=4, pin_memory=True)\n",
    "    te_dl = DataLoader(CTRDataset(Xc_te, Xn_te, y_te), batch_size=batch_size_eval, \n",
    "                       shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Build improved model\n",
    "    base_ctr = float(df_tr[label_col].mean())\n",
    "    cfg = TrainCfg(\n",
    "        d_model=192,        # Increased from 128\n",
    "        nhead=8, \n",
    "        ff=512,             # Increased from 256\n",
    "        n_layers=3,         # Increased from 2\n",
    "        dropout=0.15,       # Increased from 0.10\n",
    "        lr=1e-3, \n",
    "        weight_decay=5e-5,  # Decreased from 1e-4\n",
    "        max_epochs=20,      # Increased from 12\n",
    "        patience=5,         # Increased from 3\n",
    "        warmup_epochs=2\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel Configuration:\")\n",
    "    print(f\"  d_model: {cfg.d_model}, n_layers: {cfg.n_layers}, ff: {cfg.ff}\")\n",
    "    print(f\"  dropout: {cfg.dropout}, lr: {cfg.lr}, weight_decay: {cfg.weight_decay}\")\n",
    "    print(f\"  max_epochs: {cfg.max_epochs}, patience: {cfg.patience}, warmup: {cfg.warmup_epochs}\")\n",
    "    \n",
    "    print(\"\\nBuilding Improved FT-AFM model...\")\n",
    "    model = ImprovedFTAFM(cat_cards, len(num_cols), cfg.d_model, cfg.nhead, \n",
    "                          cfg.ff, cfg.n_layers, cfg.dropout, afm_attn_dim=64).to(DEVICE)\n",
    "    init_final_bias_to_ctr(model.head[-1], base_ctr)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    if RUN_FT_AFM:\n",
    "        print(f\"\\nTraining Improved FT-AFM on {len(df_tr):,} samples...\")\n",
    "        model = train_improved_ft_afm(model, tr_dl, va_dl, cfg, outdir)\n",
    "    \n",
    "        # Final test evaluation with smaller batches to avoid OOM\n",
    "        print(\"\\nEvaluating on test set (using memory-efficient batching)...\")\n",
    "        torch.cuda.empty_cache()  # Clear cache before final eval\n",
    "        test_auc, test_ll = evaluate_model(model, te_dl, use_smaller_batches=True)\n",
    "        save_json({\"test_auc\": float(test_auc), \"test_logloss\": float(test_ll)},\n",
    "              os.path.join(outdir, \"improved_ft_afm_test_metrics.json\"))\n",
    "    \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"IMPROVED FT-AFM Results on 40M Samples:\")\n",
    "        print(f\"Test AUC:     {test_auc:.6f}\")\n",
    "        print(f\"Test LogLoss: {test_ll:.6f}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "        return test_auc, test_ll\n",
    "    else:\n",
    "        print(\"⏭️ Skipping FT-AFM training (preprocessing only).\")\n",
    "        return None, None\n",
    "\n",
    "# ================== RUN: AVAZU 40M IMPROVED ==================\n",
    "AVAZU_40M_PATH = \"/home/elicer/ctr_project/data/avazu_full/avazu_full.parquet\"\n",
    "OUT_AVAZU_IMPROVED = \"runs_avazu_40m_improved_ft_afm\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting IMPROVED FT-AFM training on Avazu 40M dataset\")\n",
    "print(\"IMPROVEMENTS:\")\n",
    "print(\"  • Larger model: d=192, layers=3, ff=512\")\n",
    "print(\"  • Time features: hour_of_day, day_of_week, is_weekend\")\n",
    "print(\"  • More target encoding: C14, C17, site_id, app_id, device_model\")\n",
    "print(\"  • Enhanced AFM: attn_dim=64 with dropout\")\n",
    "print(\"  • Learning rate schedule: warmup + cosine annealing\")\n",
    "print(\"  • Longer training: 20 epochs, patience=5\")\n",
    "print(\"  • Better regularization: dropout=0.15, weight_decay=5e-5\")\n",
    "print(\"  • More cross features\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_auc, test_ll = run_improved_ft_afm_40m(\n",
    "    csv_path=AVAZU_40M_PATH,\n",
    "    outdir=OUT_AVAZU_IMPROVED,\n",
    "    label_col=\"click\",\n",
    "    time_info={\"time_cols\":[\"hour\"], \"order_key\":\"hour\"},\n",
    "    base_num_cols=[\"hour\"],\n",
    "    drop_cols=[\"id\"],\n",
    "    single_freq_cats=[\"C14\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"site_id\",\"app_id\",\"device_model\"],  # Added more C fields\n",
    "    pair_freq_cats=[\n",
    "        (\"site_id\",\"app_id\"),\n",
    "        (\"device_model\",\"hour\"),\n",
    "        (\"C14\",\"site_id\"),          # NEW\n",
    "        (\"C17\",\"app_id\"),           # NEW\n",
    "        (\"site_id\",\"C14\"),          # NEW\n",
    "        (\"app_id\",\"device_model\"),  # NEW\n",
    "    ],\n",
    "    te_targets=[\"C14\",\"C17\",\"site_id\",\"app_id\",\"device_model\"]  # Added C17, app_id, device_model\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS - Avazu 40M IMPROVED FT-AFM\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test AUC:     {test_auc:.6f}\")\n",
    "print(f\"Test LogLoss: {test_ll:.6f}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nResults saved to: {OUT_AVAZU_IMPROVED}/\")\n",
    "print(\"\\n✅ IMPROVED Training complete!\")\n",
    "print(\"\\nIMPROVEMENTS SUMMARY:\")\n",
    "print(\"  ✓ Model capacity increased (128→192 dim, 2→3 layers, 256→512 FF)\")\n",
    "print(\"  ✓ Time-based features extracted (hour_of_day, day_of_week, weekend)\")\n",
    "print(\"  ✓ More target encoding (5 features vs 2)\")\n",
    "print(\"  ✓ More cross features (6 pairs vs 2)\")\n",
    "print(\"  ✓ Larger AFM attention (64 vs 32)\")\n",
    "print(\"  ✓ Learning rate scheduling (warmup + cosine decay)\")\n",
    "print(\"  ✓ Longer training (20 epochs vs 12)\")\n",
    "print(\"  ✓ Better regularization tuning\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c9933c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING ABLATION STUDIES - SELECTED VARIANTS\n",
      "================================================================================\n",
      "\n",
      "Loading preprocessed data...\n",
      "Loaded: 32,343,173 train, 4,042,896 val, 4,042,898 test\n",
      "Features: 21 categorical, 25 numerical\n",
      "Schema: runs_avazu_40m_improved_ft_afm/schema.json\n",
      "\n",
      "Found existing summary with 4 rows: ablation_results/ablation_summary.csv\n",
      "\n",
      "⏭️ Skipping FT-only (already done)\n",
      "⏭️ Skipping AFM-only (already done)\n",
      "⏭️ Skipping FT+FM (already done)\n",
      "\n",
      "================================================================================\n",
      "VARIANT 4: FT+AFM (GATED)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ft_afm_gated] ep01 train_ll=0.3936 val_ll=0.3878 val_auc=0.7372 *\n",
      "[ft_afm_gated] ep02 train_ll=0.3833 val_ll=0.3878 val_auc=0.7384 *\n",
      "[ft_afm_gated] ep03 train_ll=0.3762 val_ll=0.3892 val_auc=0.7352 stale 1/5\n",
      "[ft_afm_gated] ep04 train_ll=0.3712 val_ll=0.3906 val_auc=0.7351 stale 2/5\n",
      "[ft_afm_gated] ep05 train_ll=0.3674 val_ll=0.3941 val_auc=0.7294 stale 3/5\n",
      "[ft_afm_gated] ep06 train_ll=0.3641 val_ll=0.3924 val_auc=0.7341 stale 4/5\n",
      "[ft_afm_gated] ep07 train_ll=0.3612 val_ll=0.3986 val_auc=0.7308 stale 5/5\n",
      "[ft_afm_gated] early-stopped.\n",
      "FT+AFM (gated) Test: AUC=0.7283, LogLoss=0.4081\n",
      "\n",
      "================================================================================\n",
      "ABLATION RESULTS SUMMARY (UPDATED)\n",
      "================================================================================\n",
      "         Model  Test AUC  Test LogLoss\n",
      "       FT-only  0.742573      0.398276\n",
      "FT+AFM (yours)  0.739000      0.398800\n",
      "         FT+FM  0.735403      0.410631\n",
      "FT+AFM (gated)  0.728305      0.408130\n",
      "      AFM-only  0.720308      0.409382\n",
      "\n",
      "✅ Saved updated summary to: ablation_results/ablation_summary.csv\n",
      "================================================================================\n",
      "ABLATION STUDY SCRIPT READY\n",
      "================================================================================\n",
      "\n",
      "Before running, you need to save preprocessed data as numpy arrays.\n",
      "Add this to the end of your FT-AFM training script:\n",
      "\n",
      "# After preprocessing, before training:\n",
      "np.save('Xc_train.npy', Xc_tr)\n",
      "np.save('Xn_train.npy', Xn_tr)\n",
      "np.save('y_train.npy', y_tr)\n",
      "np.save('Xc_val.npy', Xc_va)\n",
      "np.save('Xn_val.npy', Xn_va)  \n",
      "np.save('y_val.npy', y_va)\n",
      "np.save('Xc_test.npy', Xc_te)\n",
      "np.save('Xn_test.npy', Xn_te)\n",
      "np.save('y_test.npy', y_te)\n",
      "with open('schema.json', 'w') as f:\n",
      "    json.dump({'cat_cards': cat_cards, 'num_cols': num_cols}, f)\n",
      "\n",
      "\n",
      "Then run: run_ablations('path/to/saved/data', 'ablation_results')\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# QUICK ABLATION STUDY - Run 3 variants in 3 days\n",
    "# You already have FT-AFM (0.739), need: FT-only, AFM-only, FT+FM\n",
    "# ============================================================\n",
    "import os, json, math, random, numpy as np, pandas as pd, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# ===================== ADD THESE FLAGS (near the top, after imports) =====================\n",
    "RUN_FT_ONLY = False\n",
    "RUN_AFM_ONLY = False\n",
    "RUN_FT_FM = False\n",
    "RUN_FT_AFM_GATED = True     # <-- run ONLY this now\n",
    "# =======================================================================================\n",
    "\n",
    "# ============== Setup ==============\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "\n",
    "# ============== Models ==============\n",
    "class AFM(nn.Module):\n",
    "    def __init__(self, d, attn_dim=32):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(d, attn_dim, bias=False)\n",
    "        self.h = nn.Linear(attn_dim, 1, bias=False)\n",
    "    def forward(self, E):\n",
    "        B,F,d = E.shape; pairs=[]\n",
    "        for i in range(F):\n",
    "            for j in range(i+1, F):\n",
    "                pairs.append(E[:,i]*E[:,j])\n",
    "        P = torch.stack(pairs, dim=1)\n",
    "        A = torch.softmax(self.h(torch.tanh(self.W(P))), dim=1)\n",
    "        return (A * P).sum(dim=1)\n",
    "\n",
    "def fm_interaction(E):\n",
    "    sum_embed = E.sum(dim=1)\n",
    "    square_sum = sum_embed.pow(2)\n",
    "    sum_square = (E.pow(2)).sum(dim=1)\n",
    "    return (0.5 * (square_sum - sum_square)).sum(dim=1, keepdim=True)\n",
    "\n",
    "class FeatureTokenizer(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, n_num, d_model):\n",
    "        super().__init__()\n",
    "        self.cat_embs = nn.ModuleList([nn.Embedding(card, d_model) for card in cat_cardinalities])\n",
    "        self.num_proj = nn.ModuleList([nn.Linear(1, d_model) for _ in range(n_num)])\n",
    "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)); nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "    def forward(self, x_cat, x_num):\n",
    "        B = x_cat.size(0)\n",
    "        cat_tokens = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embs)]\n",
    "        num_tokens = [proj(x_num[:, i:i+1]) for i, proj in enumerate(self.num_proj)]\n",
    "        field_embs = torch.stack(cat_tokens + num_tokens, dim=1)\n",
    "        cls = self.cls.expand(B, -1, -1)\n",
    "        tokens = torch.cat([cls, field_embs], dim=1)\n",
    "        return tokens, field_embs\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, d_model=128, nhead=8, ff=256, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=ff,\n",
    "                                         dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "    def forward(self, tokens): return self.encoder(tokens)\n",
    "    \n",
    "# ===================== ADD THIS GATED MODEL (paste with your other model classes) =======\n",
    "class ImprovedFTAFM_Gated(nn.Module):\n",
    "    \"\"\"FT + AFM with gated fusion (adaptive AFM contribution).\"\"\"\n",
    "    def __init__(self, cat_cards, n_num, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15, afm_attn_dim=64):\n",
    "        super().__init__()\n",
    "        self.tok = FeatureTokenizer(cat_cards, n_num, d_model)\n",
    "        self.backbone = FTTransformer(d_model, nhead, ff, n_layers, dropout)\n",
    "        self.afm = AFM(d_model, attn_dim=afm_attn_dim)  # uses your AFM class above\n",
    "\n",
    "        # gate: conditioned on CLS, outputs per-dimension (0..1)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        fusion_dim = d_model + d_model\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        tokens, field_embs = self.tok(x_cat, x_num)\n",
    "        H = self.backbone(tokens)\n",
    "        h_cls = H[:, 0, :]                 # [B, d_model]\n",
    "\n",
    "        v_afm = self.afm(field_embs)       # [B, d_model]\n",
    "\n",
    "        g = self.gate(h_cls)               # [B, d_model] in (0,1)\n",
    "        v_afm = g * v_afm                  # gated AFM signal\n",
    "\n",
    "        z = torch.cat([h_cls, v_afm], dim=1)\n",
    "        return self.head(z).squeeze(1)\n",
    "\n",
    "# ============== Variant 1: FT-only ==============\n",
    "class FTOnly(nn.Module):\n",
    "    def __init__(self, cat_cards, n_num, d_model=128, nhead=8, ff=256, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tok = FeatureTokenizer(cat_cards, n_num, d_model)\n",
    "        self.backbone = FTTransformer(d_model, nhead, ff, n_layers, dropout)\n",
    "        self.head = nn.Sequential(nn.Linear(d_model, 128), nn.ReLU(), nn.Dropout(dropout), nn.Linear(128, 1))\n",
    "    def forward(self, x_cat, x_num):\n",
    "        tokens, _ = self.tok(x_cat, x_num)\n",
    "        H = self.backbone(tokens)\n",
    "        return self.head(H[:,0,:]).squeeze(1)\n",
    "\n",
    "# ============== Variant 2: AFM-only ==============\n",
    "class AFMOnly(nn.Module):\n",
    "    def __init__(self, cat_cards, n_num, d_model=64):\n",
    "        super().__init__()\n",
    "        self.cat_embs = nn.ModuleList([nn.Embedding(card, d_model) for card in cat_cards])\n",
    "        self.num_proj = nn.ModuleList([nn.Linear(1, d_model) for _ in range(n_num)])\n",
    "        self.afm = AFM(d_model, attn_dim=32)\n",
    "        self.head = nn.Linear(d_model, 1)\n",
    "    def forward(self, x_cat, x_num):\n",
    "        cat_embs = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embs)]\n",
    "        num_embs = [proj(x_num[:, i:i+1]) for i, proj in enumerate(self.num_proj)]\n",
    "        field_embs = torch.stack(cat_embs + num_embs, dim=1)\n",
    "        afm_out = self.afm(field_embs)\n",
    "        return self.head(afm_out).squeeze(1)\n",
    "\n",
    "# ============== Variant 3: FT+FM ==============\n",
    "class FTFM(nn.Module):\n",
    "    def __init__(self, cat_cards, n_num, d_model=128, nhead=8, ff=256, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tok = FeatureTokenizer(cat_cards, n_num, d_model)\n",
    "        self.backbone = FTTransformer(d_model, nhead, ff, n_layers, dropout)\n",
    "        self.head = nn.Sequential(nn.Linear(d_model + 1, 128), nn.ReLU(), nn.Dropout(dropout), nn.Linear(128, 1))\n",
    "    def forward(self, x_cat, x_num):\n",
    "        tokens, field_embs = self.tok(x_cat, x_num)\n",
    "        H = self.backbone(tokens)\n",
    "        fm_out = fm_interaction(field_embs)\n",
    "        z = torch.cat([H[:,0,:], fm_out], dim=1)\n",
    "        return self.head(z).squeeze(1)\n",
    "\n",
    "# ============== Training ==============\n",
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, Xc, Xn, y):\n",
    "        self.Xc = torch.as_tensor(Xc, dtype=torch.long)\n",
    "        self.Xn = torch.as_tensor(Xn, dtype=torch.float32)\n",
    "        self.y  = torch.as_tensor(y,  dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.Xc[i], self.Xn[i], self.y[i].unsqueeze(-1)\n",
    "\n",
    "def evaluate(model, dl):\n",
    "    model.eval(); ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xc, Xn, yb in dl:\n",
    "            # Process in chunks to avoid OOM\n",
    "            batch_probs = []\n",
    "            for i in range(0, Xc.size(0), 2048):\n",
    "                Xc_chunk = Xc[i:i+2048].to(DEVICE)\n",
    "                Xn_chunk = Xn[i:i+2048].to(DEVICE)\n",
    "                logits = model(Xc_chunk, Xn_chunk)\n",
    "                if logits.dim()==1: logits = logits.unsqueeze(1)\n",
    "                probs = torch.sigmoid(logits).squeeze(-1).cpu()\n",
    "                batch_probs.append(probs)\n",
    "                del Xc_chunk, Xn_chunk, logits, probs\n",
    "                torch.cuda.empty_cache()\n",
    "            probs = torch.cat(batch_probs)\n",
    "            ys.append(yb.squeeze(-1).numpy())\n",
    "            ps.append(probs.numpy())\n",
    "    y_true = np.concatenate(ys); y_prob = np.clip(np.concatenate(ps), 1e-7, 1-1e-7)\n",
    "    return roc_auc_score(y_true, y_prob), log_loss(y_true, y_prob)\n",
    "\n",
    "def train_variant(model, tr_dl, va_dl, name, outdir, max_epochs=12, patience=3):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    if NUM_GPUS > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scaler = GradScaler(device='cuda', enabled=torch.cuda.is_available())\n",
    "    best_ll, best_state, stale = float(\"inf\"), None, 0\n",
    "    \n",
    "    for ep in range(1, max_epochs+1):\n",
    "        model.train(); run = 0.0\n",
    "        for Xc, Xn, yb in tr_dl:\n",
    "            Xc, Xn, yb = Xc.to(DEVICE), Xn.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "                logits = model(Xc, Xn)\n",
    "                if logits.dim()==1: logits = logits.unsqueeze(1)\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "            run += loss.item() * yb.size(0)\n",
    "        \n",
    "        val_auc, val_ll = evaluate(model, va_dl)\n",
    "        train_ll = run / len(tr_dl.dataset)\n",
    "        improved = val_ll < best_ll\n",
    "        if improved:\n",
    "            best_ll = val_ll; stale = 0\n",
    "            model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n",
    "            best_state = {k: v.cpu().clone() for k,v in model_to_save.state_dict().items()}\n",
    "        else:\n",
    "            stale += 1\n",
    "        \n",
    "        print(f\"[{name}] ep{ep:02d} train_ll={train_ll:.4f} val_ll={val_ll:.4f} val_auc={val_auc:.4f} {'*' if improved else f'stale {stale}/{patience}'}\")\n",
    "        if stale >= patience:\n",
    "            print(f\"[{name}] early-stopped.\")\n",
    "            break\n",
    "    \n",
    "    if best_state:\n",
    "        model_to_load = model.module if isinstance(model, nn.DataParallel) else model\n",
    "        model_to_load.load_state_dict(best_state)\n",
    "        torch.save(best_state, os.path.join(outdir, f\"{name}_best.pth\"))\n",
    "    \n",
    "    return model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "# ============== Main Runner ==============\n",
    "# ===================== REPLACE YOUR run_ablations() WITH THIS PATCHED VERSION ===========\n",
    "def run_ablations(preprocessed_data_dir=\"runs_avazu_40m_improved_ft_afm\", output_dir=\"ablation_results\"):\n",
    "    \"\"\"\n",
    "    Loads saved preprocessed splits and runs selected ablation variants.\n",
    "    You can skip already-finished variants using the RUN_* flags.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RUNNING ABLATION STUDIES - SELECTED VARIANTS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # ---- Load preprocessed data ----\n",
    "    print(\"\\nLoading preprocessed data...\")\n",
    "    Xc_tr = np.load(os.path.join(preprocessed_data_dir, \"Xc_train.npy\"))\n",
    "    Xn_tr = np.load(os.path.join(preprocessed_data_dir, \"Xn_train.npy\"))\n",
    "    y_tr  = np.load(os.path.join(preprocessed_data_dir, \"y_train.npy\"))\n",
    "\n",
    "    Xc_va = np.load(os.path.join(preprocessed_data_dir, \"Xc_val.npy\"))\n",
    "    Xn_va = np.load(os.path.join(preprocessed_data_dir, \"Xn_val.npy\"))\n",
    "    y_va  = np.load(os.path.join(preprocessed_data_dir, \"y_val.npy\"))\n",
    "\n",
    "    Xc_te = np.load(os.path.join(preprocessed_data_dir, \"Xc_test.npy\"))\n",
    "    Xn_te = np.load(os.path.join(preprocessed_data_dir, \"Xn_test.npy\"))\n",
    "    y_te  = np.load(os.path.join(preprocessed_data_dir, \"y_test.npy\"))\n",
    "\n",
    "    # ---- Schema file name: adjust ONLY if your file is named differently ----\n",
    "    schema_path = os.path.join(preprocessed_data_dir, \"schema.json\")\n",
    "    if not os.path.exists(schema_path):\n",
    "        alt = os.path.join(preprocessed_data_dir, \"preprocessed_schema.json\")\n",
    "        if os.path.exists(alt):\n",
    "            schema_path = alt\n",
    "\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "\n",
    "    cat_cards = schema[\"cat_cards\"]\n",
    "    n_num = len(schema[\"num_cols\"])\n",
    "\n",
    "    print(f\"Loaded: {len(y_tr):,} train, {len(y_va):,} val, {len(y_te):,} test\")\n",
    "    print(f\"Features: {len(cat_cards)} categorical, {n_num} numerical\")\n",
    "    print(f\"Schema: {schema_path}\")\n",
    "\n",
    "    # ---- Dataloaders ----\n",
    "    batch_size = 2048 * NUM_GPUS if NUM_GPUS > 1 else 2048\n",
    "    tr_dl = DataLoader(CTRDataset(Xc_tr, Xn_tr, y_tr), batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "    va_dl = DataLoader(CTRDataset(Xc_va, Xn_va, y_va), batch_size=batch_size*2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    te_dl = DataLoader(CTRDataset(Xc_te, Xn_te, y_te), batch_size=batch_size*2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ---- Load existing summary if present so we don't lose old results ----\n",
    "    summary_path = os.path.join(output_dir, \"ablation_summary.csv\")\n",
    "    results = []\n",
    "    if os.path.exists(summary_path):\n",
    "        try:\n",
    "            prev = pd.read_csv(summary_path)\n",
    "            results = [tuple(x) for x in prev[[\"Model\", \"Test AUC\", \"Test LogLoss\"]].values.tolist()]\n",
    "            print(f\"\\nFound existing summary with {len(results)} rows: {summary_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ Could not read existing summary ({summary_path}): {e}\")\n",
    "            results = []\n",
    "\n",
    "    # Helper to avoid duplicate rows\n",
    "    def upsert_result(model_name, auc, ll):\n",
    "        nonlocal results\n",
    "        results = [r for r in results if r[0] != model_name]\n",
    "        results.append((model_name, float(auc), float(ll)))\n",
    "\n",
    "    # =================== 1) FT-only (skip if already done) ===================\n",
    "    if RUN_FT_ONLY:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"VARIANT 1: FT-only\")\n",
    "        print(\"=\"*80)\n",
    "        model = FTOnly(cat_cards, n_num, d_model=128, nhead=8, ff=256, n_layers=2, dropout=0.1).to(DEVICE)\n",
    "        model = train_variant(model, tr_dl, va_dl, \"ft_only\", output_dir)\n",
    "        test_auc, test_ll = evaluate(model, te_dl)\n",
    "        upsert_result(\"FT-only\", test_auc, test_ll)\n",
    "        print(f\"FT-only Test: AUC={test_auc:.4f}, LogLoss={test_ll:.4f}\")\n",
    "    else:\n",
    "        print(\"\\n⏭️ Skipping FT-only (already done)\")\n",
    "\n",
    "    # =================== 2) AFM-only (skip if already done) ===================\n",
    "    if RUN_AFM_ONLY:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"VARIANT 2: AFM-only\")\n",
    "        print(\"=\"*80)\n",
    "        model = AFMOnly(cat_cards, n_num, d_model=64).to(DEVICE)\n",
    "        model = train_variant(model, tr_dl, va_dl, \"afm_only\", output_dir)\n",
    "        test_auc, test_ll = evaluate(model, te_dl)\n",
    "        upsert_result(\"AFM-only\", test_auc, test_ll)\n",
    "        print(f\"AFM-only Test: AUC={test_auc:.4f}, LogLoss={test_ll:.4f}\")\n",
    "    else:\n",
    "        print(\"⏭️ Skipping AFM-only (already done)\")\n",
    "\n",
    "    # =================== 3) FT+FM (skip if already done) ===================\n",
    "    if RUN_FT_FM:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"VARIANT 3: FT+FM\")\n",
    "        print(\"=\"*80)\n",
    "        model = FTFM(cat_cards, n_num, d_model=128, nhead=8, ff=256, n_layers=2, dropout=0.1).to(DEVICE)\n",
    "        model = train_variant(model, tr_dl, va_dl, \"ft_fm\", output_dir)\n",
    "        test_auc, test_ll = evaluate(model, te_dl)\n",
    "        upsert_result(\"FT+FM\", test_auc, test_ll)\n",
    "        print(f\"FT+FM Test: AUC={test_auc:.4f}, LogLoss={test_ll:.4f}\")\n",
    "    else:\n",
    "        print(\"⏭️ Skipping FT+FM (already done)\")\n",
    "\n",
    "    # =================== 4) FT+AFM (GATED) (run this now) ===================\n",
    "    if RUN_FT_AFM_GATED:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"VARIANT 4: FT+AFM (GATED)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Use same capacity as your main model for fairness\n",
    "        model = ImprovedFTAFM_Gated(\n",
    "            cat_cards, n_num,\n",
    "            d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15, afm_attn_dim=64\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        model = train_variant(model, tr_dl, va_dl, \"ft_afm_gated\", output_dir, max_epochs=20, patience=5)\n",
    "        test_auc, test_ll = evaluate(model, te_dl)\n",
    "        upsert_result(\"FT+AFM (gated)\", test_auc, test_ll)\n",
    "        print(f\"FT+AFM (gated) Test: AUC={test_auc:.4f}, LogLoss={test_ll:.4f}\")\n",
    "    else:\n",
    "        print(\"⏭️ Skipping FT+AFM (gated)\")\n",
    "\n",
    "    # =================== Summary ===================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ABLATION RESULTS SUMMARY (UPDATED)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\"Model\", \"Test AUC\", \"Test LogLoss\"])\n",
    "    df = df.sort_values(by=\"Test AUC\", ascending=False)\n",
    "    df.to_csv(summary_path, index=False)\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "    print(f\"\\n✅ Saved updated summary to: {summary_path}\")\n",
    "    return df\n",
    "\n",
    "# ============== USAGE ==============\n",
    "# First, save your preprocessed data from the FT-AFM run\n",
    "# Then run this:\n",
    "run_ablations(\"runs_avazu_40m_improved_ft_afm\", \"ablation_results\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ABLATION STUDY SCRIPT READY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBefore running, you need to save preprocessed data as numpy arrays.\")\n",
    "print(\"Add this to the end of your FT-AFM training script:\")\n",
    "print(\"\"\"\n",
    "# After preprocessing, before training:\n",
    "np.save('Xc_train.npy', Xc_tr)\n",
    "np.save('Xn_train.npy', Xn_tr)\n",
    "np.save('y_train.npy', y_tr)\n",
    "np.save('Xc_val.npy', Xc_va)\n",
    "np.save('Xn_val.npy', Xn_va)  \n",
    "np.save('y_val.npy', y_va)\n",
    "np.save('Xc_test.npy', Xc_te)\n",
    "np.save('Xn_test.npy', Xn_te)\n",
    "np.save('y_test.npy', y_te)\n",
    "with open('schema.json', 'w') as f:\n",
    "    json.dump({'cat_cards': cat_cards, 'num_cols': num_cols}, f)\n",
    "\"\"\")\n",
    "print(\"\\nThen run: run_ablations('path/to/saved/data', 'ablation_results')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3935e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FT-AFM Training Summary:\n",
      "============================================================\n",
      "Best val AUC: 0.7477\n",
      "Best val LL: 0.3823\n",
      "Final val AUC: 0.7467\n",
      "Stopped at epoch: 7\n",
      "\n",
      "Full history:\n",
      "   epoch   val_auc    val_ll\n",
      "0      1  0.746883  0.382866\n",
      "1      2  0.747685  0.382297\n",
      "2      3  0.746501  0.383027\n",
      "3      4  0.746505  0.383146\n",
      "4      5  0.746465  0.382589\n",
      "5      6  0.746195  0.382757\n",
      "6      7  0.746650  0.382608\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load FT-AFM history\n",
    "history = pd.read_csv(\"runs_avazu_40m_improved_ft_afm/improved_ft_afm_history.csv\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FT-AFM Training Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best val AUC: {history['val_auc'].max():.4f}\")\n",
    "print(f\"Best val LL: {history['val_ll'].min():.4f}\")\n",
    "print(f\"Final val AUC: {history['val_auc'].iloc[-1]:.4f}\")\n",
    "print(f\"Stopped at epoch: {len(history)}\")\n",
    "print(f\"\\nFull history:\")\n",
    "print(history[['epoch', 'val_auc', 'val_ll']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac30e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Find all history files\n",
    "history_files = glob.glob(\"ablation_results/*history.csv\")\n",
    "\n",
    "for file in history_files:\n",
    "    model_name = file.split('/')[-1].replace('_history.csv', '')\n",
    "    history = pd.read_csv(file)\n",
    "    best_val = history['val_auc'].max()\n",
    "    print(f\"{model_name:15} | Best Val AUC: {best_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ec5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "p = \"ctr_project/data/criteo/train.csv\"   # change to your actual split file\n",
    "df = pd.read_csv(p, nrows=5)\n",
    "print(df.columns.tolist())\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
