{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: 4\n",
      "STEP 1: Preprocessing Criteo...\n",
      "\n",
      "================================================================================\n",
      "CRITEO PREPROCESSING\n",
      "================================================================================\n",
      "Loading from: ctr_project/data/criteo/train.txt\n",
      "Loaded 45,840,617 rows, 40 columns\n",
      "CTR: 0.2562\n",
      "Handling missing values...\n",
      "Splitting data (8:1:1)...\n",
      "Train: 36,672,493, Val: 4,584,062, Test: 4,584,062\n",
      "Encoding categorical features...\n",
      "  C1: 1444 categories\n",
      "  C2: 553 categories\n",
      "  C3: 157547 categories\n",
      "  C4: 117728 categories\n",
      "  C5: 306 categories\n",
      "  C6: 19 categories\n",
      "  C7: 11880 categories\n",
      "  C8: 629 categories\n",
      "  C9: 4 categories\n",
      "  C10: 39495 categories\n",
      "  C11: 5129 categories\n",
      "  C12: 156812 categories\n",
      "  C13: 3174 categories\n",
      "  C14: 27 categories\n",
      "  C15: 11053 categories\n",
      "  C16: 149078 categories\n",
      "  C17: 11 categories\n",
      "  C18: 4546 categories\n",
      "  C19: 1998 categories\n",
      "  C20: 5 categories\n",
      "  C21: 154814 categories\n",
      "  C22: 18 categories\n",
      "  C23: 16 categories\n",
      "  C24: 53042 categories\n",
      "  C25: 82 categories\n",
      "  C26: 40957 categories\n",
      "Engineering frequency features...\n",
      "Target encoding...\n",
      "Standardizing numerics...\n",
      "Saving preprocessed data...\n",
      "✅ Preprocessing complete!\n",
      "   Categorical features: 26\n",
      "   Numerical features: 21\n",
      "   Saved to: criteo_preprocessed/\n",
      "\n",
      "STEP 2: Training FT-AFM...\n",
      "\n",
      "================================================================================\n",
      "TRAINING FT-AFM ON CRITEO\n",
      "================================================================================\n",
      "Loading preprocessed data...\n",
      "Train: 36,672,493, Val: 4,584,062, Test: 4,584,062\n",
      "Features: 26 cat + 21 num\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_ll=0.4579 val_ll=0.4500 val_auc=0.8029 *BEST*\n",
      "Epoch 02 | train_ll=0.4461 val_ll=0.4455 val_auc=0.8060 *BEST*\n",
      "Epoch 03 | train_ll=0.4415 val_ll=0.4446 val_auc=0.8076 *BEST*\n",
      "Epoch 04 | train_ll=0.4377 val_ll=0.4448 val_auc=0.8082 stale 1/5\n",
      "Epoch 05 | train_ll=0.4337 val_ll=0.4445 val_auc=0.8079 *BEST*\n",
      "Epoch 06 | train_ll=0.4293 val_ll=0.4467 val_auc=0.8071 stale 1/5\n",
      "Epoch 07 | train_ll=0.4240 val_ll=0.4547 val_auc=0.8042 stale 2/5\n",
      "Epoch 08 | train_ll=0.4178 val_ll=0.4574 val_auc=0.8018 stale 3/5\n",
      "Epoch 09 | train_ll=0.4109 val_ll=0.4665 val_auc=0.7982 stale 4/5\n",
      "Epoch 10 | train_ll=0.4033 val_ll=0.4804 val_auc=0.7934 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPLETE CRITEO PIPELINE\n",
    "# Preprocessing + FT-AFM Training\n",
    "# Based on your Avazu pipeline\n",
    "# ============================================================\n",
    "import os, json, math, random, numpy as np, pandas as pd, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# ============== Setup ==============\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "\n",
    "print(f\"GPUs available: {NUM_GPUS}\")\n",
    "\n",
    "# ============== Models (Copy from your FT-AFM script) ==============\n",
    "class ImprovedAFM(nn.Module):\n",
    "    \"\"\"AFM with larger attention dimension and dropout\"\"\"\n",
    "    def __init__(self, d, attn_dim=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(d, attn_dim, bias=False)\n",
    "        self.h = nn.Linear(attn_dim, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, E):\n",
    "        B,F,d = E.shape; pairs=[]\n",
    "        for i in range(F):\n",
    "            for j in range(i+1, F):\n",
    "                pairs.append(E[:,i]*E[:,j])\n",
    "        P = torch.stack(pairs, dim=1)\n",
    "        P = self.dropout(P)\n",
    "        A = torch.softmax(self.h(torch.tanh(self.W(P))), dim=1)\n",
    "        return (A * P).sum(dim=1)\n",
    "\n",
    "class FeatureTokenizer(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, n_num, d_model):\n",
    "        super().__init__()\n",
    "        self.cat_embs = nn.ModuleList([nn.Embedding(card, d_model) for card in cat_cardinalities])\n",
    "        self.num_proj = nn.ModuleList([nn.Linear(1, d_model) for _ in range(n_num)])\n",
    "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)); nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "    def forward(self, x_cat, x_num):\n",
    "        B = x_cat.size(0)\n",
    "        cat_tokens = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embs)]\n",
    "        num_tokens = [proj(x_num[:, i:i+1]) for i, proj in enumerate(self.num_proj)]\n",
    "        field_embs = torch.stack(cat_tokens + num_tokens, dim=1)\n",
    "        cls = self.cls.expand(B, -1, -1)\n",
    "        tokens = torch.cat([cls, field_embs], dim=1)\n",
    "        return tokens, field_embs\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15):\n",
    "        super().__init__()\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=ff,\n",
    "                                         dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "    def forward(self, tokens): return self.encoder(tokens)\n",
    "\n",
    "class ImprovedFTAFM(nn.Module):\n",
    "    \"\"\"Improved FT+AFM with larger capacity and better fusion\"\"\"\n",
    "    def __init__(self, cat_cards, n_num, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15, afm_attn_dim=64):\n",
    "        super().__init__()\n",
    "        self.tok = FeatureTokenizer(cat_cards, n_num, d_model)\n",
    "        self.backbone = FTTransformer(d_model, nhead, ff, n_layers, dropout)\n",
    "        self.afm = ImprovedAFM(d_model, afm_attn_dim, dropout=dropout)\n",
    "        \n",
    "        # Larger head\n",
    "        fusion_dim = d_model + d_model\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_cat, x_num):\n",
    "        tokens, field_embs = self.tok(x_cat, x_num)\n",
    "        H = self.backbone(tokens); h_cls = H[:,0,:]\n",
    "        v_afm = self.afm(field_embs)\n",
    "        z = torch.cat([h_cls, v_afm], dim=1)\n",
    "        return self.head(z).squeeze(1)\n",
    "\n",
    "\n",
    "# ============== Utils ==============\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def kfold_target_encode(df_tr, df_va, col, yname, n_splits=5, min_samples=50):\n",
    "    prior = float(df_tr[yname].mean())\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    te_tr = pd.Series(np.zeros(len(df_tr), dtype=\"float32\"), index=df_tr.index)\n",
    "    for tr_idx, hold_idx in kf.split(df_tr):\n",
    "        cur = df_tr.iloc[tr_idx]\n",
    "        means = cur.groupby(col)[yname].mean()\n",
    "        cnts  = cur.groupby(col)[yname].size()\n",
    "        m = (means*cnts + prior*min_samples) / (cnts + min_samples)\n",
    "        te_tr.iloc[hold_idx] = df_tr.iloc[hold_idx][col].map(m).fillna(prior).astype(\"float32\")\n",
    "    means = df_tr.groupby(col)[yname].mean()\n",
    "    cnts  = df_tr.groupby(col)[yname].size()\n",
    "    mfull = (means*cnts + prior*min_samples) / (cnts + min_samples)\n",
    "    te_va = df_va[col].map(mfull).fillna(prior).astype(\"float32\")\n",
    "    return te_tr, te_va\n",
    "\n",
    "# ============== Criteo Preprocessing ==============\n",
    "def preprocess_criteo(data_path, output_dir, sample_size=None):\n",
    "    \"\"\"\n",
    "    Preprocess Criteo dataset\n",
    "    \n",
    "    Criteo format:\n",
    "    - Column 0: Label (0/1)\n",
    "    - Columns 1-13: I1-I13 (numerical features)\n",
    "    - Columns 14-39: C1-C26 (categorical features)\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to Criteo train.txt file\n",
    "        output_dir: Where to save preprocessed data\n",
    "        sample_size: If set, use only this many rows (for testing)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CRITEO PREPROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ensure_dir(output_dir)\n",
    "    \n",
    "    # Load data\n",
    "    print(f\"Loading from: {data_path}\")\n",
    "    if sample_size:\n",
    "        df = pd.read_csv(data_path, sep='\\t', header=None, nrows=sample_size)\n",
    "        print(f\"Using {sample_size:,} samples for testing\")\n",
    "    else:\n",
    "        df = pd.read_csv(data_path, sep='\\t', header=None)\n",
    "    \n",
    "    print(f\"Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Name columns\n",
    "    df.columns = ['label'] + [f'I{i}' for i in range(1,14)] + [f'C{i}' for i in range(1,27)]\n",
    "    \n",
    "    print(f\"CTR: {df['label'].mean():.4f}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"Handling missing values...\")\n",
    "    for col in df.columns:\n",
    "        if col.startswith('I'):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('float32')\n",
    "        elif col.startswith('C'):\n",
    "            df[col] = df[col].fillna('missing').astype(str)\n",
    "    \n",
    "    # 8:1:1 split (random - Criteo has no temporal ordering)\n",
    "    print(\"Splitting data (8:1:1)...\")\n",
    "    df_tr, df_tmp = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "    df_va, df_te = train_test_split(df_tmp, test_size=0.5, stratify=df_tmp['label'], random_state=42)\n",
    "    df_tr, df_va, df_te = df_tr.copy(), df_va.copy(), df_te.copy()\n",
    "    \n",
    "    print(f\"Train: {len(df_tr):,}, Val: {len(df_va):,}, Test: {len(df_te):,}\")\n",
    "    \n",
    "    # Feature lists\n",
    "    num_cols = [f'I{i}' for i in range(1,14)]\n",
    "    cat_cols = [f'C{i}' for i in range(1,27)]\n",
    "    \n",
    "    # Categorical encoding with frequency filtering\n",
    "    print(\"Encoding categorical features...\")\n",
    "    cat_cards = []\n",
    "    MIN_FREQ = 10\n",
    "    \n",
    "    for c in cat_cols:\n",
    "        vc = df_tr[c].value_counts()\n",
    "        frequent = vc[vc >= MIN_FREQ].index\n",
    "        mapping = {v:i for i,v in enumerate(frequent)}\n",
    "        unk_id = len(mapping)\n",
    "        \n",
    "        df_tr[c] = df_tr[c].map(mapping).fillna(unk_id).astype('int64')\n",
    "        df_va[c] = df_va[c].map(mapping).fillna(unk_id).astype('int64')\n",
    "        df_te[c] = df_te[c].map(mapping).fillna(unk_id).astype('int64')\n",
    "        \n",
    "        cat_cards.append(unk_id + 1)\n",
    "        print(f\"  {c}: {unk_id+1} categories\")\n",
    "    \n",
    "    # Single frequency features\n",
    "    print(\"Engineering frequency features...\")\n",
    "    freq_candidates = ['C1', 'C14', 'C17', 'C20', 'C21']  # High-cardinality fields\n",
    "    for c in freq_candidates:\n",
    "        vc = df_tr[c].value_counts()\n",
    "        df_tr[f\"{c}_freq\"] = df_tr[c].map(vc).astype('float32')\n",
    "        df_va[f\"{c}_freq\"] = df_va[c].map(vc).fillna(0).astype('float32')\n",
    "        df_te[f\"{c}_freq\"] = df_te[c].map(vc).fillna(0).astype('float32')\n",
    "        num_cols.append(f\"{c}_freq\")\n",
    "    \n",
    "    # Target encoding\n",
    "    print(\"Target encoding...\")\n",
    "    te_candidates = ['C1', 'C14', 'C20']\n",
    "    for c in te_candidates:\n",
    "        te_tr, te_va = kfold_target_encode(df_tr, df_va, c, 'label', n_splits=5, min_samples=50)\n",
    "        prior = float(df_tr['label'].mean())\n",
    "        means = df_tr.groupby(c)['label'].mean()\n",
    "        cnts  = df_tr.groupby(c)['label'].size()\n",
    "        mfull = (means*cnts + prior*50) / (cnts + 50)\n",
    "        te_te = df_te[c].map(mfull).fillna(prior).astype('float32')\n",
    "        \n",
    "        df_tr[f\"{c}_te\"] = te_tr.astype('float32')\n",
    "        df_va[f\"{c}_te\"] = te_va.astype('float32')\n",
    "        df_te[f\"{c}_te\"] = te_te.astype('float32')\n",
    "        num_cols.append(f\"{c}_te\")\n",
    "    \n",
    "    # Log1p frequency features\n",
    "    freq_cols = [c for c in num_cols if c.endswith('_freq')]\n",
    "    for c in freq_cols:\n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            d[c] = np.log1p(d[c].clip(lower=0))\n",
    "    \n",
    "    # Standardize all numerics\n",
    "    print(\"Standardizing numerics...\")\n",
    "    num_means = {c: float(df_tr[c].mean()) for c in num_cols}\n",
    "    num_stds  = {c: float(df_tr[c].std()) for c in num_cols}\n",
    "    \n",
    "    for c in num_cols:\n",
    "        mu, sd = num_means[c], (num_stds[c] if num_stds[c] > 1e-8 else 1.0)\n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            d[c] = ((d[c] - mu)/sd).clip(-5, 5).astype('float32')  # Clip outliers\n",
    "    \n",
    "    # Save preprocessed arrays\n",
    "    print(\"Saving preprocessed data...\")\n",
    "    np.save(f'{output_dir}/Xc_train.npy', df_tr[cat_cols].to_numpy())\n",
    "    np.save(f'{output_dir}/Xn_train.npy', df_tr[num_cols].to_numpy().astype('float32'))\n",
    "    np.save(f'{output_dir}/y_train.npy', df_tr['label'].to_numpy().astype('float32'))\n",
    "    \n",
    "    np.save(f'{output_dir}/Xc_val.npy', df_va[cat_cols].to_numpy())\n",
    "    np.save(f'{output_dir}/Xn_val.npy', df_va[num_cols].to_numpy().astype('float32'))\n",
    "    np.save(f'{output_dir}/y_val.npy', df_va['label'].to_numpy().astype('float32'))\n",
    "    \n",
    "    np.save(f'{output_dir}/Xc_test.npy', df_te[cat_cols].to_numpy())\n",
    "    np.save(f'{output_dir}/Xn_test.npy', df_te[num_cols].to_numpy().astype('float32'))\n",
    "    np.save(f'{output_dir}/y_test.npy', df_te['label'].to_numpy().astype('float32'))\n",
    "    \n",
    "    schema = {\n",
    "        'cat_cards': cat_cards,\n",
    "        'num_cols': num_cols,\n",
    "        'cat_cols': cat_cols,\n",
    "        'train_ctr': float(df_tr['label'].mean()),\n",
    "        'val_ctr': float(df_va['label'].mean()),\n",
    "        'test_ctr': float(df_te['label'].mean())\n",
    "    }\n",
    "    save_json(schema, f'{output_dir}/schema.json')\n",
    "    \n",
    "    print(f\"✅ Preprocessing complete!\")\n",
    "    print(f\"   Categorical features: {len(cat_cols)}\")\n",
    "    print(f\"   Numerical features: {len(num_cols)}\")\n",
    "    print(f\"   Saved to: {output_dir}/\")\n",
    "    \n",
    "    return cat_cards, num_cols\n",
    "\n",
    "# ============== Training (Copy from your FT-AFM script) ==============\n",
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, Xc, Xn, y):\n",
    "        self.Xc = torch.as_tensor(Xc, dtype=torch.long)\n",
    "        self.Xn = torch.as_tensor(Xn, dtype=torch.float32)\n",
    "        self.y  = torch.as_tensor(y,  dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.Xc[i], self.Xn[i], self.y[i].unsqueeze(-1)\n",
    "\n",
    "def evaluate(model, dl):\n",
    "    model.eval(); ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xc, Xn, yb in dl:\n",
    "            batch_probs = []\n",
    "            for i in range(0, Xc.size(0), 2048):\n",
    "                Xc_chunk = Xc[i:i+2048].to(DEVICE)\n",
    "                Xn_chunk = Xn[i:i+2048].to(DEVICE)\n",
    "                logits = model(Xc_chunk, Xn_chunk)\n",
    "                if logits.dim()==1: logits = logits.unsqueeze(1)\n",
    "                probs = torch.sigmoid(logits).squeeze(-1).cpu()\n",
    "                batch_probs.append(probs)\n",
    "                del Xc_chunk, Xn_chunk, logits, probs\n",
    "                torch.cuda.empty_cache()\n",
    "            probs = torch.cat(batch_probs)\n",
    "            ys.append(yb.squeeze(-1).numpy())\n",
    "            ps.append(probs.numpy())\n",
    "    y_true = np.concatenate(ys); y_prob = np.clip(np.concatenate(ps), 1e-7, 1-1e-7)\n",
    "    return roc_auc_score(y_true, y_prob), log_loss(y_true, y_prob)\n",
    "\n",
    "def train_criteo_ft_afm(data_dir, output_dir, max_epochs=15, patience=5):\n",
    "    \"\"\"Train FT-AFM on preprocessed Criteo data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING FT-AFM ON CRITEO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ensure_dir(output_dir)\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    Xc_tr = np.load(f'{data_dir}/Xc_train.npy')\n",
    "    Xn_tr = np.load(f'{data_dir}/Xn_train.npy')\n",
    "    y_tr = np.load(f'{data_dir}/y_train.npy')\n",
    "    \n",
    "    Xc_va = np.load(f'{data_dir}/Xc_val.npy')\n",
    "    Xn_va = np.load(f'{data_dir}/Xn_val.npy')\n",
    "    y_va = np.load(f'{data_dir}/y_val.npy')\n",
    "    \n",
    "    Xc_te = np.load(f'{data_dir}/Xc_test.npy')\n",
    "    Xn_te = np.load(f'{data_dir}/Xn_test.npy')\n",
    "    y_te = np.load(f'{data_dir}/y_test.npy')\n",
    "    \n",
    "    with open(f'{data_dir}/schema.json') as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    cat_cards = schema['cat_cards']\n",
    "    n_num = len(schema['num_cols'])\n",
    "    \n",
    "    print(f\"Train: {len(y_tr):,}, Val: {len(y_va):,}, Test: {len(y_te):,}\")\n",
    "    print(f\"Features: {len(cat_cards)} cat + {n_num} num\")\n",
    "    \n",
    "    # Data loaders\n",
    "    batch_size = 2048 * NUM_GPUS if NUM_GPUS > 1 else 2048\n",
    "    tr_dl = DataLoader(CTRDataset(Xc_tr, Xn_tr, y_tr), batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    va_dl = DataLoader(CTRDataset(Xc_va, Xn_va, y_va), batch_size=batch_size*2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    te_dl = DataLoader(CTRDataset(Xc_te, Xn_te, y_te), batch_size=batch_size*2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Build model\n",
    "    model = ImprovedFTAFM(cat_cards, n_num, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15, afm_attn_dim=64).to(DEVICE)\n",
    "    \n",
    "    if NUM_GPUS > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    # Initialize bias\n",
    "    base_ctr = float(y_tr.mean())\n",
    "    last_linear = model.module.head[-1] if isinstance(model, nn.DataParallel) else model.head[-1]\n",
    "    with torch.no_grad():\n",
    "        last_linear.bias.fill_(math.log(base_ctr/(1.0-base_ctr)))\n",
    "    \n",
    "    # Training\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-5)\n",
    "    scaler = GradScaler(device='cuda', enabled=torch.cuda.is_available())\n",
    "    best_ll, best_state, stale = float(\"inf\"), None, 0\n",
    "    history = []\n",
    "    \n",
    "    for ep in range(1, max_epochs+1):\n",
    "        model.train(); run = 0.0\n",
    "        for Xc, Xn, yb in tr_dl:\n",
    "            Xc, Xn, yb = Xc.to(DEVICE), Xn.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "                logits = model(Xc, Xn)\n",
    "                if logits.dim()==1: logits = logits.unsqueeze(1)\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "            run += loss.item() * yb.size(0)\n",
    "        \n",
    "        val_auc, val_ll = evaluate(model, va_dl)\n",
    "        train_ll = run / len(tr_dl.dataset)\n",
    "        improved = val_ll < best_ll\n",
    "        \n",
    "        if improved:\n",
    "            best_ll = val_ll; stale = 0\n",
    "            model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n",
    "            best_state = {k: v.cpu().clone() for k,v in model_to_save.state_dict().items()}\n",
    "        else:\n",
    "            stale += 1\n",
    "        \n",
    "        history.append({\"epoch\": ep, \"train_ll\": train_ll, \"val_ll\": val_ll, \"val_auc\": val_auc})\n",
    "        print(f\"Epoch {ep:02d} | train_ll={train_ll:.4f} val_ll={val_ll:.4f} val_auc={val_auc:.4f} {'*BEST*' if improved else f'stale {stale}/{patience}'}\")\n",
    "        \n",
    "        if stale >= patience:\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_state:\n",
    "        model_to_load = model.module if isinstance(model, nn.DataParallel) else model\n",
    "        model_to_load.load_state_dict(best_state)\n",
    "        torch.save(best_state, f'{output_dir}/criteo_ft_afm_best.pth')\n",
    "    \n",
    "    pd.DataFrame(history).to_csv(f'{output_dir}/criteo_history.csv', index=False)\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_auc, test_ll = evaluate(model, te_dl)\n",
    "    \n",
    "    results = {\n",
    "        \"test_auc\": float(test_auc),\n",
    "        \"test_logloss\": float(test_ll),\n",
    "        \"val_auc\": float(max([h['val_auc'] for h in history])),\n",
    "        \"val_logloss\": float(min([h['val_ll'] for h in history]))\n",
    "    }\n",
    "    save_json(results, f'{output_dir}/criteo_results.json')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CRITEO FT-AFM RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Test AUC:     {test_auc:.4f}\")\n",
    "    print(f\"Test LogLoss: {test_ll:.4f}\")\n",
    "    print(f\"Saved to: {output_dir}/\")\n",
    "    \n",
    "    return test_auc, test_ll\n",
    "\n",
    "# ============== MAIN RUNNER ==============\n",
    "def run_criteo_complete(\n",
    "    data_path=\"/ctr_project/data/criteo/train.txt\",\n",
    "    preprocess_dir=\"criteo_preprocessed\",\n",
    "    output_dir=\"criteo_ft_afm_results\",\n",
    "    sample_size=None  # Set to e.g. 1000000 for testing\n",
    "):\n",
    "    \"\"\"Complete Criteo pipeline\"\"\"\n",
    "    \n",
    "    # Step 1: Preprocess\n",
    "    print(\"STEP 1: Preprocessing Criteo...\")\n",
    "    cat_cards, num_cols = preprocess_criteo(data_path, preprocess_dir, sample_size)\n",
    "    \n",
    "    # Step 2: Train FT-AFM\n",
    "    print(\"\\nSTEP 2: Training FT-AFM...\")\n",
    "    test_auc, test_ll = train_criteo_ft_afm(preprocess_dir, output_dir)\n",
    "    \n",
    "    print(\"\\n✅ CRITEO PIPELINE COMPLETE!\")\n",
    "    return test_auc, test_ll\n",
    "\n",
    "# ============== USAGE ==============\n",
    "\n",
    "# Full dataset (45M samples, ~8-10 hours on 4×A100)\n",
    "run_criteo_complete(\n",
    "    data_path=\"ctr_project/data/criteo/train.txt\",\n",
    "    preprocess_dir=\"criteo_preprocessed\",\n",
    "    output_dir=\"criteo_ft_afm_results\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "# Test on 1M samples first (~1 hour)\n",
    "run_criteo_complete(\n",
    "    data_path=\"ctr_project/data/criteo/train.txt\",\n",
    "    preprocess_dir=\"criteo_preprocessed_1m\",\n",
    "    output_dir=\"criteo_ft_afm_results_1m\",\n",
    "    sample_size=1000000\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CRITEO PIPELINE READY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDownload Criteo from:\")\n",
    "print(\"  https://www.kaggle.com/datasets/mrkmakr/criteo-dataset\")\n",
    "print(\"  or: http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/\")\n",
    "print(\"\\nPlace train.txt in: /ctr_project/data/criteo/\")\n",
    "print(\"\\nThen run: run_criteo_complete()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50af1a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists? True\n",
      "✅ Unzipped successfully!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"ctr_project/data/criteo/train.txt.zip\"\n",
    "extract_dir = \"ctr_project/data/criteo\"\n",
    "\n",
    "print(\"Exists?\", os.path.exists(zip_path))\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"✅ Unzipped successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc63283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
