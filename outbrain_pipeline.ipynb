{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df266717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 zip files:\n",
      "  - documents_topics.csv.zip\n",
      "  - documents_categories.csv.zip\n",
      "  - events.csv.zip\n",
      "  - promoted_content.csv.zip\n",
      "  - clicks_train.csv.zip\n",
      "  - documents_meta.csv.zip\n",
      "\n",
      "Unzipping documents_topics.csv.zip...\n",
      "  ✓ Extracted documents_topics.csv.zip\n",
      "\n",
      "Unzipping documents_categories.csv.zip...\n",
      "  ✓ Extracted documents_categories.csv.zip\n",
      "\n",
      "Unzipping events.csv.zip...\n",
      "  ✓ Extracted events.csv.zip\n",
      "\n",
      "Unzipping promoted_content.csv.zip...\n",
      "  ✓ Extracted promoted_content.csv.zip\n",
      "\n",
      "Unzipping clicks_train.csv.zip...\n",
      "  ✓ Extracted clicks_train.csv.zip\n",
      "\n",
      "Unzipping documents_meta.csv.zip...\n",
      "  ✓ Extracted documents_meta.csv.zip\n",
      "\n",
      "============================================================\n",
      "Extracted files:\n",
      "  - documents_topics.csv (339.5 MB)\n",
      "  - documents_categories.csv (118.0 MB)\n",
      "  - events.csv (1208.5 MB)\n",
      "  - promoted_content.csv (13.9 MB)\n",
      "  - clicks_train.csv (1486.7 MB)\n",
      "  - documents_meta.csv (89.4 MB)\n",
      "\n",
      "✅ All files unzipped!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Path to Outbrain data\n",
    "data_dir = \"ctr_project/data/outbrain\"\n",
    "\n",
    "# Find all zip files\n",
    "zip_files = [f for f in os.listdir(data_dir) if f.endswith('.zip')]\n",
    "print(f\"Found {len(zip_files)} zip files:\")\n",
    "for f in zip_files:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Unzip each file\n",
    "for zip_file in zip_files:\n",
    "    zip_path = os.path.join(data_dir, zip_file)\n",
    "    print(f\"\\nUnzipping {zip_file}...\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    \n",
    "    print(f\"  ✓ Extracted {zip_file}\")\n",
    "\n",
    "# List all extracted CSV files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Extracted files:\")\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "for f in csv_files:\n",
    "    size_mb = os.path.getsize(os.path.join(data_dir, f)) / 1e6\n",
    "    print(f\"  - {f} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(\"\\n✅ All files unzipped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acc16c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pandas 2.0.3\n",
      "Uninstalling pandas-2.0.3:\n",
      "  Successfully uninstalled pandas-2.0.3\n",
      "Found existing installation: numpy 2.2.6\n",
      "Uninstalling numpy-2.2.6:\n",
      "  Successfully uninstalled numpy-2.2.6\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy<2.0\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m152.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas==2.1.4\n",
      "  Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m144.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tzdata>=2022.1 in ./.local/lib/python3.10/site-packages (from pandas==2.1.4) (2025.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas==2.1.4) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas==2.1.4) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.17.0)\n",
      "Installing collected packages: numpy, pandas\n",
      "Successfully installed numpy-1.26.4 pandas-2.1.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y pandas numpy\n",
    "!pip install --no-cache-dir -U \"numpy<2.0\" \"pandas==2.1.4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94e2879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4 2.1.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "print(np.__version__, pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246d5e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Preprocessing Outbrain...\n",
      "\n",
      "================================================================================\n",
      "OUTBRAIN PREPROCESSING\n",
      "================================================================================\n",
      "Loading clicks_train.csv...\n",
      "  Loaded 87,141,731 clicks\n",
      "  Sampling 20,000,000 rows...\n",
      "Loading events.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_254/2310797792.py:151: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  events = pd.read_csv(f\"{data_dir}/events.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 23,120,126 events\n",
      "Loading promoted_content.csv...\n",
      "  Loaded 559,583 promoted content\n",
      "Merging datasets...\n",
      "  Merged dataset: 20,000,000 rows\n",
      "  CTR: 0.1937\n",
      "Extracting temporal features...\n",
      "Handling missing values...\n",
      "Parsing geo_location...\n",
      "Splitting data temporally (8:1:1)...\n",
      "Train: 16,000,000, Val: 2,000,000, Test: 2,000,000\n",
      "Train CTR: 0.1938\n",
      "Val CTR: 0.1923\n",
      "Test CTR: 0.1945\n",
      "Encoding categorical features...\n",
      "  uuid: 11930 categories\n",
      "  platform: 4 categories\n",
      "  geo_location: 2145 categories\n",
      "  ad_id: 62260 categories\n",
      "  document_id_y: 33741 categories\n",
      "  campaign_id: 20441 categories\n",
      "  advertiser_id: 3351 categories\n",
      "  country: 220 categories\n",
      "  state: 394 categories\n",
      "Engineering frequency features...\n",
      "Target encoding...\n",
      "Standardizing numerics...\n",
      "Saving preprocessed data...\n",
      "✅ Preprocessing complete!\n",
      "   Categorical features: 9\n",
      "   Numerical features: 8\n",
      "   Saved to: outbrain_preprocessed/\n",
      "\n",
      "STEP 2: Training FT-AFM...\n",
      "\n",
      "================================================================================\n",
      "TRAINING FT-AFM ON OUTBRAIN\n",
      "================================================================================\n",
      "Loading preprocessed data...\n",
      "Train: 16,000,000, Val: 2,000,000, Test: 2,000,000\n",
      "Features: 9 cat + 8 num\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_ll=0.4430 val_ll=0.4453 val_auc=0.7092 *BEST*\n",
      "Epoch 02 | train_ll=0.4415 val_ll=0.4445 val_auc=0.7112 *BEST*\n",
      "Epoch 03 | train_ll=0.4402 val_ll=0.4447 val_auc=0.7116 stale 1/5\n",
      "Epoch 04 | train_ll=0.4389 val_ll=0.4439 val_auc=0.7124 *BEST*\n",
      "Epoch 05 | train_ll=0.4375 val_ll=0.4440 val_auc=0.7118 stale 1/5\n",
      "Epoch 06 | train_ll=0.4359 val_ll=0.4456 val_auc=0.7116 stale 2/5\n",
      "Epoch 07 | train_ll=0.4340 val_ll=0.4461 val_auc=0.7104 stale 3/5\n",
      "Epoch 08 | train_ll=0.4320 val_ll=0.4477 val_auc=0.7090 stale 4/5\n",
      "Epoch 09 | train_ll=0.4300 val_ll=0.4494 val_auc=0.7072 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "OUTBRAIN FT-AFM RESULTS\n",
      "================================================================================\n",
      "Test AUC:     0.6986\n",
      "Test LogLoss: 0.4530\n",
      "Saved to: outbrain_ft_afm_results/\n",
      "\n",
      "✅ OUTBRAIN PIPELINE COMPLETE!\n",
      "================================================================================\n",
      "OUTBRAIN PIPELINE READY\n",
      "================================================================================\n",
      "\n",
      "Download Outbrain from:\n",
      "  https://www.kaggle.com/c/outbrain-click-prediction/data\n",
      "\n",
      "Files needed:\n",
      "  - clicks_train.csv\n",
      "  - events.csv\n",
      "  - promoted_content.csv\n",
      "\n",
      "Place in: /ctr_project/data/outbrain/\n",
      "\n",
      "Then run: run_outbrain_complete()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# OUTBRAIN FT-AFM PIPELINE\n",
    "# Content recommendation CTR prediction\n",
    "# ============================================================\n",
    "import os, json, math, random, numpy as np, pandas as pd, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torch.amp import GradScaler, autocast\n",
    "from datetime import datetime\n",
    "\n",
    "# ============== Setup ==============\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "\n",
    "# ============== Models (Copy from previous scripts) ==============\n",
    "class ImprovedAFM(nn.Module):\n",
    "    \"\"\"AFM with larger attention dimension and dropout\"\"\"\n",
    "    def __init__(self, d, attn_dim=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(d, attn_dim, bias=False)\n",
    "        self.h = nn.Linear(attn_dim, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, E):\n",
    "        B,F,d = E.shape; pairs=[]\n",
    "        for i in range(F):\n",
    "            for j in range(i+1, F):\n",
    "                pairs.append(E[:,i]*E[:,j])\n",
    "        P = torch.stack(pairs, dim=1)\n",
    "        P = self.dropout(P)\n",
    "        A = torch.softmax(self.h(torch.tanh(self.W(P))), dim=1)\n",
    "        return (A * P).sum(dim=1)\n",
    "\n",
    "class FeatureTokenizer(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, n_num, d_model):\n",
    "        super().__init__()\n",
    "        self.cat_embs = nn.ModuleList([nn.Embedding(card, d_model) for card in cat_cardinalities])\n",
    "        self.num_proj = nn.ModuleList([nn.Linear(1, d_model) for _ in range(n_num)])\n",
    "        self.cls = nn.Parameter(torch.zeros(1,1,d_model)); nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "    def forward(self, x_cat, x_num):\n",
    "        B = x_cat.size(0)\n",
    "        cat_tokens = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embs)]\n",
    "        num_tokens = [proj(x_num[:, i:i+1]) for i, proj in enumerate(self.num_proj)]\n",
    "        field_embs = torch.stack(cat_tokens + num_tokens, dim=1)\n",
    "        cls = self.cls.expand(B, -1, -1)\n",
    "        tokens = torch.cat([cls, field_embs], dim=1)\n",
    "        return tokens, field_embs\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15):\n",
    "        super().__init__()\n",
    "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=ff,\n",
    "                                         dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc, num_layers=n_layers)\n",
    "    def forward(self, tokens): return self.encoder(tokens)\n",
    "\n",
    "class ImprovedFTAFM(nn.Module):\n",
    "    \"\"\"Improved FT+AFM with larger capacity and better fusion\"\"\"\n",
    "    def __init__(self, cat_cards, n_num, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15, afm_attn_dim=64):\n",
    "        super().__init__()\n",
    "        self.tok = FeatureTokenizer(cat_cards, n_num, d_model)\n",
    "        self.backbone = FTTransformer(d_model, nhead, ff, n_layers, dropout)\n",
    "        self.afm = ImprovedAFM(d_model, afm_attn_dim, dropout=dropout)\n",
    "        \n",
    "        # Larger head\n",
    "        fusion_dim = d_model + d_model\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_cat, x_num):\n",
    "        tokens, field_embs = self.tok(x_cat, x_num)\n",
    "        H = self.backbone(tokens); h_cls = H[:,0,:]\n",
    "        v_afm = self.afm(field_embs)\n",
    "        z = torch.cat([h_cls, v_afm], dim=1)\n",
    "        return self.head(z).squeeze(1)\n",
    "\n",
    "# ============== Utils ==============\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def kfold_target_encode(df_tr, df_va, col, yname, n_splits=5, min_samples=50):\n",
    "    prior = float(df_tr[yname].mean())\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    te_tr = pd.Series(np.zeros(len(df_tr), dtype=\"float32\"), index=df_tr.index)\n",
    "    for tr_idx, hold_idx in kf.split(df_tr):\n",
    "        cur = df_tr.iloc[tr_idx]\n",
    "        means = cur.groupby(col)[yname].mean()\n",
    "        cnts  = cur.groupby(col)[yname].size()\n",
    "        m = (means*cnts + prior*min_samples) / (cnts + min_samples)\n",
    "        te_tr.iloc[hold_idx] = df_tr.iloc[hold_idx][col].map(m).fillna(prior).astype(\"float32\")\n",
    "    means = df_tr.groupby(col)[yname].mean()\n",
    "    cnts  = df_tr.groupby(col)[yname].size()\n",
    "    mfull = (means*cnts + prior*min_samples) / (cnts + min_samples)\n",
    "    te_va = df_va[col].map(mfull).fillna(prior).astype(\"float32\")\n",
    "    return te_tr, te_va\n",
    "\n",
    "# ============== Outbrain Preprocessing ==============\n",
    "def preprocess_outbrain(\n",
    "    data_dir=\"/ctr_project/data/outbrain\",\n",
    "    output_dir=\"outbrain_preprocessed\",\n",
    "    sample_size=None  # Set to e.g., 10M for faster processing\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess Outbrain click prediction data\n",
    "    \n",
    "    Files needed in data_dir:\n",
    "    - clicks_train.csv: (display_id, ad_id, clicked)\n",
    "    - events.csv: (display_id, uuid, timestamp, document_id, platform, geo_location)\n",
    "    - promoted_content.csv: (ad_id, document_id, campaign_id, advertiser_id)\n",
    "    \n",
    "    Optional (for richer features):\n",
    "    - documents_meta.csv\n",
    "    - documents_categories.csv\n",
    "    - documents_topics.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OUTBRAIN PREPROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ensure_dir(output_dir)\n",
    "    \n",
    "    # Load clicks\n",
    "    print(\"Loading clicks_train.csv...\")\n",
    "    clicks = pd.read_csv(f\"{data_dir}/clicks_train.csv\")\n",
    "    print(f\"  Loaded {len(clicks):,} clicks\")\n",
    "    \n",
    "    if sample_size:\n",
    "        print(f\"  Sampling {sample_size:,} rows...\")\n",
    "        clicks = clicks.sample(n=min(sample_size, len(clicks)), random_state=42)\n",
    "    \n",
    "    # Load events\n",
    "    print(\"Loading events.csv...\")\n",
    "    events = pd.read_csv(f\"{data_dir}/events.csv\")\n",
    "    print(f\"  Loaded {len(events):,} events\")\n",
    "    \n",
    "    # Load promoted content\n",
    "    print(\"Loading promoted_content.csv...\")\n",
    "    promoted = pd.read_csv(f\"{data_dir}/promoted_content.csv\")\n",
    "    print(f\"  Loaded {len(promoted):,} promoted content\")\n",
    "    \n",
    "    # Merge data\n",
    "    print(\"Merging datasets...\")\n",
    "    df = clicks.merge(events, on='display_id', how='left')\n",
    "    df = df.merge(promoted, on='ad_id', how='left')\n",
    "    \n",
    "    print(f\"  Merged dataset: {len(df):,} rows\")\n",
    "    print(f\"  CTR: {df['clicked'].mean():.4f}\")\n",
    "    \n",
    "    # Extract temporal features\n",
    "    print(\"Extracting temporal features...\")\n",
    "    df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce')\n",
    "    df['hour'] = ((df['timestamp'] / 1000 / 3600) % 24).astype('int32')\n",
    "    df['day_of_week'] = ((df['timestamp'] / 1000 / 86400) % 7).astype('int32')\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"Handling missing values...\")\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna('missing').astype(str)\n",
    "        else:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Select features\n",
    "    label_col = 'clicked'\n",
    "    \n",
    "    # Categorical features\n",
    "    cat_cols = ['uuid', 'document_id', 'platform', 'geo_location', \n",
    "                'ad_id', 'document_id_y', 'campaign_id', 'advertiser_id']\n",
    "    cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "    \n",
    "    # Numerical features\n",
    "    num_cols = ['hour', 'day_of_week', 'timestamp']\n",
    "    \n",
    "    # Parse geo_location into country and state (format: country>state)\n",
    "    if 'geo_location' in df.columns:\n",
    "        print(\"Parsing geo_location...\")\n",
    "        df['country'] = df['geo_location'].str.split('>').str[0]\n",
    "        df['state'] = df['geo_location'].str.split('>').str[1].fillna('unknown')\n",
    "        cat_cols.extend(['country', 'state'])\n",
    "    \n",
    "    # 8:1:1 temporal split (Outbrain has temporal ordering)\n",
    "    print(\"Splitting data temporally (8:1:1)...\")\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    n = len(df)\n",
    "    n_tr = int(0.8 * n)\n",
    "    n_va = int(0.1 * n)\n",
    "    \n",
    "    df_tr = df.iloc[:n_tr].copy()\n",
    "    df_va = df.iloc[n_tr:n_tr+n_va].copy()\n",
    "    df_te = df.iloc[n_tr+n_va:].copy()\n",
    "    \n",
    "    print(f\"Train: {len(df_tr):,}, Val: {len(df_va):,}, Test: {len(df_te):,}\")\n",
    "    print(f\"Train CTR: {df_tr[label_col].mean():.4f}\")\n",
    "    print(f\"Val CTR: {df_va[label_col].mean():.4f}\")\n",
    "    print(f\"Test CTR: {df_te[label_col].mean():.4f}\")\n",
    "    \n",
    "    # Categorical encoding\n",
    "    print(\"Encoding categorical features...\")\n",
    "    cat_cards = []\n",
    "    MIN_FREQ = 10\n",
    "    \n",
    "    for c in cat_cols:\n",
    "        vc = df_tr[c].value_counts()\n",
    "        frequent = vc[vc >= MIN_FREQ].index\n",
    "        mapping = {v:i for i,v in enumerate(frequent)}\n",
    "        unk_id = len(mapping)\n",
    "        \n",
    "        df_tr[c] = df_tr[c].map(mapping).fillna(unk_id).astype('int64')\n",
    "        df_va[c] = df_va[c].map(mapping).fillna(unk_id).astype('int64')\n",
    "        df_te[c] = df_te[c].map(mapping).fillna(unk_id).astype('int64')\n",
    "        \n",
    "        cat_cards.append(unk_id + 1)\n",
    "        print(f\"  {c}: {unk_id+1} categories\")\n",
    "    \n",
    "    # Frequency features\n",
    "    print(\"Engineering frequency features...\")\n",
    "    freq_candidates = ['ad_id', 'document_id', 'campaign_id', 'advertiser_id']\n",
    "    freq_candidates = [c for c in freq_candidates if c in cat_cols]\n",
    "    \n",
    "    for c in freq_candidates:\n",
    "        vc = df_tr[c].value_counts()\n",
    "        df_tr[f\"{c}_freq\"] = df_tr[c].map(vc).astype('float32')\n",
    "        df_va[f\"{c}_freq\"] = df_va[c].map(vc).fillna(0).astype('float32')\n",
    "        df_te[f\"{c}_freq\"] = df_te[c].map(vc).fillna(0).astype('float32')\n",
    "        num_cols.append(f\"{c}_freq\")\n",
    "    \n",
    "    # Target encoding\n",
    "    print(\"Target encoding...\")\n",
    "    te_candidates = ['ad_id', 'document_id', 'campaign_id']\n",
    "    te_candidates = [c for c in te_candidates if c in cat_cols]\n",
    "    \n",
    "    for c in te_candidates:\n",
    "        te_tr, te_va = kfold_target_encode(df_tr, df_va, c, label_col, n_splits=5, min_samples=50)\n",
    "        prior = float(df_tr[label_col].mean())\n",
    "        means = df_tr.groupby(c)[label_col].mean()\n",
    "        cnts  = df_tr.groupby(c)[label_col].size()\n",
    "        mfull = (means*cnts + prior*50) / (cnts + 50)\n",
    "        te_te = df_te[c].map(mfull).fillna(prior).astype('float32')\n",
    "        \n",
    "        df_tr[f\"{c}_te\"] = te_tr.astype('float32')\n",
    "        df_va[f\"{c}_te\"] = te_va.astype('float32')\n",
    "        df_te[f\"{c}_te\"] = te_te.astype('float32')\n",
    "        num_cols.append(f\"{c}_te\")\n",
    "    \n",
    "    # Log1p frequency features\n",
    "    freq_cols = [c for c in num_cols if c.endswith('_freq')]\n",
    "    for c in freq_cols:\n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            d[c] = np.log1p(d[c].clip(lower=0))\n",
    "    \n",
    "    # Standardize numerics\n",
    "    print(\"Standardizing numerics...\")\n",
    "    num_means = {c: float(df_tr[c].mean()) for c in num_cols}\n",
    "    num_stds  = {c: float(df_tr[c].std()) for c in num_cols}\n",
    "    \n",
    "    for c in num_cols:\n",
    "        mu, sd = num_means[c], (num_stds[c] if num_stds[c] > 1e-8 else 1.0)\n",
    "        for d in (df_tr, df_va, df_te):\n",
    "            d[c] = ((d[c] - mu)/sd).clip(-5, 5).astype('float32')\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    print(\"Saving preprocessed data...\")\n",
    "    np.save(f'{output_dir}/Xc_train.npy', df_tr[cat_cols].to_numpy())\n",
    "    np.save(f'{output_dir}/Xn_train.npy', df_tr[num_cols].to_numpy().astype('float32'))\n",
    "    np.save(f'{output_dir}/y_train.npy', df_tr[label_col].to_numpy().astype('float32'))\n",
    "    \n",
    "    np.save(f'{output_dir}/Xc_val.npy', df_va[cat_cols].to_numpy())\n",
    "    np.save(f'{output_dir}/Xn_val.npy', df_va[num_cols].to_numpy().astype('float32'))\n",
    "    np.save(f'{output_dir}/y_val.npy', df_va[label_col].to_numpy().astype('float32'))\n",
    "    \n",
    "    np.save(f'{output_dir}/Xc_test.npy', df_te[cat_cols].to_numpy())\n",
    "    np.save(f'{output_dir}/Xn_test.npy', df_te[num_cols].to_numpy().astype('float32'))\n",
    "    np.save(f'{output_dir}/y_test.npy', df_te[label_col].to_numpy().astype('float32'))\n",
    "    \n",
    "    schema = {\n",
    "        'cat_cards': cat_cards,\n",
    "        'num_cols': num_cols,\n",
    "        'cat_cols': cat_cols,\n",
    "        'train_ctr': float(df_tr[label_col].mean()),\n",
    "        'val_ctr': float(df_va[label_col].mean()),\n",
    "        'test_ctr': float(df_te[label_col].mean())\n",
    "    }\n",
    "    save_json(schema, f'{output_dir}/schema.json')\n",
    "    \n",
    "    print(f\"✅ Preprocessing complete!\")\n",
    "    print(f\"   Categorical features: {len(cat_cols)}\")\n",
    "    print(f\"   Numerical features: {len(num_cols)}\")\n",
    "    print(f\"   Saved to: {output_dir}/\")\n",
    "    \n",
    "    return cat_cards, num_cols\n",
    "\n",
    "# ============== Training ==============\n",
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, Xc, Xn, y):\n",
    "        self.Xc = torch.as_tensor(Xc, dtype=torch.long)\n",
    "        self.Xn = torch.as_tensor(Xn, dtype=torch.float32)\n",
    "        self.y  = torch.as_tensor(y,  dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.Xc[i], self.Xn[i], self.y[i].unsqueeze(-1)\n",
    "\n",
    "def evaluate(model, dl):\n",
    "    model.eval(); ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for Xc, Xn, yb in dl:\n",
    "            batch_probs = []\n",
    "            for i in range(0, Xc.size(0), 2048):\n",
    "                Xc_chunk = Xc[i:i+2048].to(DEVICE)\n",
    "                Xn_chunk = Xn[i:i+2048].to(DEVICE)\n",
    "                logits = model(Xc_chunk, Xn_chunk)\n",
    "                if logits.dim()==1: logits = logits.unsqueeze(1)\n",
    "                probs = torch.sigmoid(logits).squeeze(-1).cpu()\n",
    "                batch_probs.append(probs)\n",
    "                del Xc_chunk, Xn_chunk, logits, probs\n",
    "                torch.cuda.empty_cache()\n",
    "            probs = torch.cat(batch_probs)\n",
    "            ys.append(yb.squeeze(-1).numpy())\n",
    "            ps.append(probs.numpy())\n",
    "    y_true = np.concatenate(ys); y_prob = np.clip(np.concatenate(ps), 1e-7, 1-1e-7)\n",
    "    return roc_auc_score(y_true, y_prob), log_loss(y_true, y_prob)\n",
    "\n",
    "def train_outbrain_ft_afm(data_dir, output_dir, max_epochs=15, patience=5):\n",
    "    \"\"\"Train FT-AFM on preprocessed Outbrain data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING FT-AFM ON OUTBRAIN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    ensure_dir(output_dir)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    Xc_tr = np.load(f'{data_dir}/Xc_train.npy')\n",
    "    Xn_tr = np.load(f'{data_dir}/Xn_train.npy')\n",
    "    y_tr = np.load(f'{data_dir}/y_train.npy')\n",
    "    \n",
    "    Xc_va = np.load(f'{data_dir}/Xc_val.npy')\n",
    "    Xn_va = np.load(f'{data_dir}/Xn_val.npy')\n",
    "    y_va = np.load(f'{data_dir}/y_val.npy')\n",
    "    \n",
    "    Xc_te = np.load(f'{data_dir}/Xc_test.npy')\n",
    "    Xn_te = np.load(f'{data_dir}/Xn_test.npy')\n",
    "    y_te = np.load(f'{data_dir}/y_test.npy')\n",
    "    \n",
    "    with open(f'{data_dir}/schema.json') as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    cat_cards = schema['cat_cards']\n",
    "    n_num = len(schema['num_cols'])\n",
    "    \n",
    "    print(f\"Train: {len(y_tr):,}, Val: {len(y_va):,}, Test: {len(y_te):,}\")\n",
    "    print(f\"Features: {len(cat_cards)} cat + {n_num} num\")\n",
    "    \n",
    "    # Data loaders\n",
    "    batch_size = 2048 * NUM_GPUS if NUM_GPUS > 1 else 2048\n",
    "    tr_dl = DataLoader(CTRDataset(Xc_tr, Xn_tr, y_tr), batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    va_dl = DataLoader(CTRDataset(Xc_va, Xn_va, y_va), batch_size=batch_size*2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    te_dl = DataLoader(CTRDataset(Xc_te, Xn_te, y_te), batch_size=batch_size*2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Build model\n",
    "    model = ImprovedFTAFM(cat_cards, n_num, d_model=192, nhead=8, ff=512, n_layers=3, dropout=0.15, afm_attn_dim=64).to(DEVICE)\n",
    "    \n",
    "    if NUM_GPUS > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    # Initialize bias\n",
    "    base_ctr = float(y_tr.mean())\n",
    "    last_linear = model.module.head[-1] if isinstance(model, nn.DataParallel) else model.head[-1]\n",
    "    with torch.no_grad():\n",
    "        last_linear.bias.fill_(math.log(base_ctr/(1.0-base_ctr)))\n",
    "    \n",
    "    # Training\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-5)\n",
    "    scaler = GradScaler(device='cuda', enabled=torch.cuda.is_available())\n",
    "    best_ll, best_state, stale = float(\"inf\"), None, 0\n",
    "    history = []\n",
    "    \n",
    "    for ep in range(1, max_epochs+1):\n",
    "        model.train(); run = 0.0\n",
    "        for Xc, Xn, yb in tr_dl:\n",
    "            Xc, Xn, yb = Xc.to(DEVICE), Xn.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "                logits = model(Xc, Xn)\n",
    "                if logits.dim()==1: logits = logits.unsqueeze(1)\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "            run += loss.item() * yb.size(0)\n",
    "        \n",
    "        val_auc, val_ll = evaluate(model, va_dl)\n",
    "        train_ll = run / len(tr_dl.dataset)\n",
    "        improved = val_ll < best_ll\n",
    "        \n",
    "        if improved:\n",
    "            best_ll = val_ll; stale = 0\n",
    "            model_to_save = model.module if isinstance(model, nn.DataParallel) else model\n",
    "            best_state = {k: v.cpu().clone() for k,v in model_to_save.state_dict().items()}\n",
    "        else:\n",
    "            stale += 1\n",
    "        \n",
    "        history.append({\"epoch\": ep, \"train_ll\": train_ll, \"val_ll\": val_ll, \"val_auc\": val_auc})\n",
    "        print(f\"Epoch {ep:02d} | train_ll={train_ll:.4f} val_ll={val_ll:.4f} val_auc={val_auc:.4f} {'*BEST*' if improved else f'stale {stale}/{patience}'}\")\n",
    "        \n",
    "        if stale >= patience:\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "    \n",
    "    # Restore best\n",
    "    if best_state:\n",
    "        model_to_load = model.module if isinstance(model, nn.DataParallel) else model\n",
    "        model_to_load.load_state_dict(best_state)\n",
    "        torch.save(best_state, f'{output_dir}/outbrain_ft_afm_best.pth')\n",
    "    \n",
    "    pd.DataFrame(history).to_csv(f'{output_dir}/outbrain_history.csv', index=False)\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_auc, test_ll = evaluate(model, te_dl)\n",
    "    \n",
    "    results = {\n",
    "        \"test_auc\": float(test_auc),\n",
    "        \"test_logloss\": float(test_ll),\n",
    "        \"val_auc\": float(max([h['val_auc'] for h in history])),\n",
    "        \"val_logloss\": float(min([h['val_ll'] for h in history]))\n",
    "    }\n",
    "    save_json(results, f'{output_dir}/outbrain_results.json')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OUTBRAIN FT-AFM RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Test AUC:     {test_auc:.4f}\")\n",
    "    print(f\"Test LogLoss: {test_ll:.4f}\")\n",
    "    print(f\"Saved to: {output_dir}/\")\n",
    "    \n",
    "    return test_auc, test_ll\n",
    "\n",
    "# ============== MAIN RUNNER ==============\n",
    "def run_outbrain_complete(\n",
    "    data_dir=\"/ctr_project/data/outbrain\",\n",
    "    preprocess_dir=\"outbrain_preprocessed\",\n",
    "    output_dir=\"outbrain_ft_afm_results\",\n",
    "    sample_size=None  # Set to e.g., 10000000 for 10M samples\n",
    "):\n",
    "    \"\"\"Complete Outbrain pipeline\"\"\"\n",
    "    \n",
    "    # Step 1: Preprocess\n",
    "    print(\"STEP 1: Preprocessing Outbrain...\")\n",
    "    cat_cards, num_cols = preprocess_outbrain(data_dir, preprocess_dir, sample_size)\n",
    "    \n",
    "    # Step 2: Train FT-AFM\n",
    "    print(\"\\nSTEP 2: Training FT-AFM...\")\n",
    "    test_auc, test_ll = train_outbrain_ft_afm(preprocess_dir, output_dir)\n",
    "    \n",
    "    print(\"\\n✅ OUTBRAIN PIPELINE COMPLETE!\")\n",
    "    return test_auc, test_ll\n",
    "\n",
    "# ============== USAGE ==============\n",
    "\n",
    "# Full dataset (adjust sample_size based on your needs)\n",
    "run_outbrain_complete(\n",
    "    data_dir=\"ctr_project/data/outbrain\",\n",
    "    preprocess_dir=\"outbrain_preprocessed\",\n",
    "    output_dir=\"outbrain_ft_afm_results\",\n",
    "    sample_size=20000000  # Use 20M samples for reasonable training time\n",
    ")\n",
    "\"\"\"\n",
    "# Test on smaller sample first\n",
    "run_outbrain_complete(\n",
    "    data_dir=\"ctr_project/data/outbrain\",\n",
    "    preprocess_dir=\"outbrain_preprocessed_1m\",\n",
    "    output_dir=\"outbrain_ft_afm_results_1m\",\n",
    "    sample_size=1000000\n",
    ")\n",
    "\"\"\"\n",
    "print(\"=\"*80)\n",
    "print(\"OUTBRAIN PIPELINE READY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDownload Outbrain from:\")\n",
    "print(\"  https://www.kaggle.com/c/outbrain-click-prediction/data\")\n",
    "print(\"\\nFiles needed:\")\n",
    "print(\"  - clicks_train.csv\")\n",
    "print(\"  - events.csv\")\n",
    "print(\"  - promoted_content.csv\")\n",
    "print(\"\\nPlace in: /ctr_project/data/outbrain/\")\n",
    "print(\"\\nThen run: run_outbrain_complete()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cebd58bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Preprocessing Outbrain...\n",
      "\n",
      "================================================================================\n",
      "OUTBRAIN PREPROCESSING\n",
      "================================================================================\n",
      "Loading clicks_train.csv...\n",
      "  Loaded 87,141,731 clicks\n",
      "  Sampling 40,000,000 rows...\n",
      "Loading events.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_254/2310797792.py:151: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  events = pd.read_csv(f\"{data_dir}/events.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 23,120,126 events\n",
      "Loading promoted_content.csv...\n",
      "  Loaded 559,583 promoted content\n",
      "Merging datasets...\n",
      "  Merged dataset: 40,000,000 rows\n",
      "  CTR: 0.1937\n",
      "Extracting temporal features...\n",
      "Handling missing values...\n",
      "Parsing geo_location...\n",
      "Splitting data temporally (8:1:1)...\n",
      "Train: 32,000,000, Val: 4,000,000, Test: 4,000,000\n",
      "Train CTR: 0.1938\n",
      "Val CTR: 0.1922\n",
      "Test CTR: 0.1945\n",
      "Encoding categorical features...\n",
      "  uuid: 118343 categories\n",
      "  platform: 5 categories\n",
      "  geo_location: 2400 categories\n",
      "  ad_id: 97742 categories\n",
      "  document_id_y: 46594 categories\n",
      "  campaign_id: 23087 categories\n",
      "  advertiser_id: 3495 categories\n",
      "  country: 226 categories\n",
      "  state: 397 categories\n",
      "Engineering frequency features...\n",
      "Target encoding...\n",
      "Standardizing numerics...\n",
      "Saving preprocessed data...\n",
      "✅ Preprocessing complete!\n",
      "   Categorical features: 9\n",
      "   Numerical features: 8\n",
      "   Saved to: outbrain_preprocessed_40m/\n",
      "\n",
      "STEP 2: Training FT-AFM...\n",
      "\n",
      "================================================================================\n",
      "TRAINING FT-AFM ON OUTBRAIN\n",
      "================================================================================\n",
      "Loading preprocessed data...\n",
      "Train: 32,000,000, Val: 4,000,000, Test: 4,000,000\n",
      "Features: 9 cat + 8 num\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_ll=0.4418 val_ll=0.4450 val_auc=0.7103 *BEST*\n",
      "Epoch 02 | train_ll=0.4399 val_ll=0.4444 val_auc=0.7133 *BEST*\n",
      "Epoch 03 | train_ll=0.4386 val_ll=0.4428 val_auc=0.7146 *BEST*\n",
      "Epoch 04 | train_ll=0.4373 val_ll=0.4425 val_auc=0.7148 *BEST*\n",
      "Epoch 05 | train_ll=0.4358 val_ll=0.4427 val_auc=0.7144 stale 1/5\n",
      "Epoch 06 | train_ll=0.4339 val_ll=0.4431 val_auc=0.7141 stale 2/5\n",
      "Epoch 07 | train_ll=0.4316 val_ll=0.4442 val_auc=0.7127 stale 3/5\n",
      "Epoch 08 | train_ll=0.4293 val_ll=0.4461 val_auc=0.7128 stale 4/5\n",
      "Epoch 09 | train_ll=0.4268 val_ll=0.4467 val_auc=0.7114 stale 5/5\n",
      "Early stopped.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "================================================================================\n",
      "OUTBRAIN FT-AFM RESULTS\n",
      "================================================================================\n",
      "Test AUC:     0.7003\n",
      "Test LogLoss: 0.4516\n",
      "Saved to: outbrain_ft_afm_results_40m/\n",
      "\n",
      "✅ OUTBRAIN PIPELINE COMPLETE!\n",
      "0.7003085858210122 0.45155954123671616\n"
     ]
    }
   ],
   "source": [
    "test_auc, test_ll = run_outbrain_complete(\n",
    "    data_dir=\"ctr_project/data/outbrain\",\n",
    "    preprocess_dir=\"outbrain_preprocessed_40m\",\n",
    "    output_dir=\"outbrain_ft_afm_results_40m\",\n",
    "    sample_size=40_000_000\n",
    ")\n",
    "\n",
    "print(test_auc, test_ll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc661ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
